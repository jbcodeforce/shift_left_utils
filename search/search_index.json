{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Shift Left tools and practices","text":"Versions <ul> <li>Created 12/2024</li> <li>Updated 02/23: new pipeline helper functions to support sink to source pipeline metadata creation or source to sink.</li> <li>Update 06/08: New features, and explanation in recipes for better manage Large Flink project.</li> <li>Update 07/24: v0.1.28: blue/green doc see CHANGELOG file.</li> </ul> <p>Recognizing the increasing need for real-time data processing, driven by demands for immediate insights and responsive applications (ultimately benefiting the end customer), this initiative tackles the challenging task of migrating traditional batch pipelines to modern, stream-based architectures using Apache Flink. </p> <p>The approach, supported that the tools in this repository, centers around the data engineer, and Site Reliability Engineers as the \"users\". By providing intelligent tooling, leveraging the power of Large Language Models and AI Agents, we aim to significantly reduce time associated with this complex refactoring. The utilities within this repository, focusing on SQL translation and Flink project organization aligned with Kimball's dimensional modeling principles, to directly address the pain points of migrating and managing real-time data pipelines. </p> <p>The list of important features supported:</p> <ul> <li> Migrate SQL / dbt sql scripts to Apache Flink SQL, using LLM compatible with OpenAI SDK. It supports understanding the static relationship between source SQL tables</li> <li> Define code structure to manage Flink statements with DDL, DML, Makefile (wrapper on top of Confluent Cli).</li> <li> Build table inventory for a project with metadata to help automate CI/CD work or supporting the shift_left tool itself</li> <li> Create metadata about a pipeline for a given Flink Statement: those metadata includes, name, type of Flink statement (stateless, stateful), what are the direct ancestors of the flink statement, what are the descendants, users of the current Flink statement.</li> <li> Assessing statement complexity in term of number of different type of joins: number_of_regular_joins, number_of_left_joins, number_of_right_joins, number_of_inner_joins, number_of_outer_joins</li> <li> Build an execution plan for each pipeline to understand what needs to be started and redeployed to avoid brute force deployment. Execution plan is a topological sorted graph which helps to start Flink statements that are needed before other statements</li> <li> Deploy execution plan, with constraints on forcing restart of ancestors (or not), update descendants or not.</li> <li> Support grouping Flink statements per directory (reflecting the medallion structure of the project: sources, intermediates, dimensions, facts and views), or as a product (data as a product) as an orthogonal view of the medallion view), or as a list of table / Flink statements or a unique table/Flink statement.</li> <li> Create Confluent Cloud Flink compute pool via REST API when needed during the execution plan assessment. This helps to get less SRE involvement and dynamic assignement of compute pool to flink.</li> <li> Select compute pool from existing running Flink statement to reuse resources.</li> <li> Support reports of running statements using the execution plan semantic</li> <li> Verify naming convention is respected and other best practices as topic configuration, schema parameters.</li> <li> Support aggregating report on statements, like running Flink explain and reporting deployment errors for all statements within a Folder or for a product.</li> <li> Support adding custom table worker to do SQL content update during deployment: this is needed to support multi-tenancy into the same Kafka Cluster and Schema Registry, or apply some table changes names. Some default transformations are available:</li> <li> Unit tests creation from DML with test definition metadata to be able to run unit tests with mock data on Confluent Cloud, by using REST API to deploy statement.</li> <li> Report on cross-product tables as sensitive tables: Some tables are becoming common and shared between data product, adding risk to change them over time, this function helps assessing the risk.</li> <li> Getting the list of modified files from a specific date to assess the difference of SQL semantic between the running statement and the one on disk. Uses <code>git log</code>.</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>Shift Left means taking bach-processing jobs and try to refactor them to real-time processing using product such as Apache Flink. In batch processing a lot of projects use SQL and dbt (Data build tool) to define the logic of the data pipeline. In real-time processing, Apache Kafka is a de-facto middleware to persist immutable records, and for SQL, Python and Java based real-time processing, Apache Flink is also the preferred platform.</p> <p>To organize batch processing and data pipelines to Datawarehouse, some compagnies are adopting the Kimball guidelines and best practices.</p> <p>Working on this kind of refactoring projects is taking  time and is challenging. Large Language Model and AI Agentic solution should help data engineers to shift their data pipelines to the real-time processing by offering flexible SQL translation tools.</p> <p>Also organizing Flink project using the Kimball guidelines is possible and even recommended. </p> <p>This repository is a tentative to develop and share some of the tools and practices needed for running such shift-leet projects.</p> <p>As of now the utilities are oriented to use Confluent Cloud for Kafka and for Flink, but running local Flink and Kafka should be easy to support.</p> <p>To avoid calling remote LLM, the current repository uses Ollama, running locally, or potentially in a remote server on-premises or inside a private network.</p> <p>Two important concepts of this practice:</p> <ul> <li>Dimensions provide the \u201cwho, what, where, when, why, and how\u201d context surrounding a business process event. Dimension tables contain the descriptive attributes used by BI applications for \ufb01ltering and grouping the facts. </li> <li>Facts are the measurements that result from a business process event and are almost always numeric. The design of a fact table is entirely based on a physical activity, and not by the reports to produce from those facts. A fact table always contains foreign keys for each of its associated dimensions, as well as optional degenerate dimension keys and date/time stamps</li> </ul>"},{"location":"#the-star-schema","title":"The star schema","text":"<p>The star schema, was defined at the end of the 80s, as a multi-dimensional data model to organize data in Date warehouse, to maintain history and by reducing the data duplication. A star schema is used to denormalize business data into dimensions and facts. The fact table connects to multiple other dimension tables along \"dimensions\" like time, or product.</p> <p></p>"},{"location":"#context","title":"Context","text":"<p>The target environment will be Apache Flink running within the Confluent Cloud as a managed service or in th future running in standalone cluster. The source of the batch processing is defined within a dbt (Data build tool) project or within a SQL project and the refactored SQL are produced under the <code>pipelines</code> folder.</p> <p>At the system context level, for the tools of this repository, we can see the following high level components:</p> Shift Left project system context <ol> <li>The source project to migrate: could be dbt or SQL</li> <li>The new Flink project repository, where the project structure is built with a specific tool</li> <li>An automatic migration tool, using Agentic App, used by developers to migrate one pipeline at a time. A pipeline is from a fact or dimension table up to the sources. The landing zone is a <code>staging folder</code> in the repository.</li> <li>Tested and finalized pipelines are saved under the <code>pipelines folder</code>.</li> <li>Test harness helps to validate complex Flink statement by isolation, using mockup data and test topics.</li> <li>Pipeline dependencies is a tool to get understanding of the sink to source pipeline and from the source to sink too. There is another tool that help assess which tables use a given table.</li> <li>Finally pipeline deployment help to automate, and pace the deployment of a given pipeline.</li> </ol>"},{"location":"#shift_left-tool","title":"Shift_left tool","text":"<p>The following diagram illustrates the development environment which, mainly, uses two docker containers, or when not using docker, the ollama cli and a python virtual environment with needed modules.</p> shift_left tool context <p>The <code>shift_left</code> cli groups a set of Python tools, Python 3.13.1 and the needed libraries to integrate with Ollama, and Confluent Cloud via REST APIs . The ollama image is used to run qwen2.5-coder:32b LLM model locally on the developer's computer.</p> <p>Follow the setup instructions to get started with the migration project.</p> <p>If the developer laptop does not have enough capacity, there is an option to run Ollama on an EC2 machine with GPUs.</p>"},{"location":"#a-migration-path","title":"A migration path","text":"<p>Any batch pipelines that create tables or files in a Lakehouse platform can be refactored using Apache Flink pipelines, as illustrated in the following figure:</p> Shifting left the data pipeline processing <p>In the diagram above, the architecture employs a sink configured as a PostgreSQL database, which will support business intelligence dashboards. It can also be a set of Iceberg tables persisted in an object storage. The Flink tables are mapped to Kafka topics, and Kafka connectors are used to transfer data from these topics to the PostgreSQL database. On the left side of the diagram source topics content is coming from Change Data Capture.</p> <p>From the perspective of a Confluent Cloud Flink pipeline, the last topic serves as the sink.</p> <p>To facilitate the refactoring, the approach begins with the sink table and works backward to identify the sources. Once the sources are determined, the process may involve implementing a set of deduplication statements and intermediate steps to apply some business logic and data transformations.</p> <p>The dbt project contains all the SQL statements necessary to do a classical bronze to gold landing zone transformation. The goal of the shift_left migration tools is to process these files and replicate the same organizational structure for the Flink SQL statements, which includes sources, intermediates, dimensions, and facts, but adapting the SQL structure due to the power fo Apache Flinkc. </p> <p>The target Flink project structure will look like in the following example:</p> <pre><code>pipelines\n\u251c\u2500\u2500 dimensions\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 {data_product_name}\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 {dimension_name}\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 Makefile\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 sql-scripts\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.{dim_name}.sql\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.{dim_trainee}.sql\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 tests\n\u251c\u2500\u2500 facts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 {data_product_name}\n\u2502\u00a0\u00a0     \u2514\u2500\u2500\u2500 {fact_name}\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 Makefile\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 sql-scripts\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.{fact_name}.sql\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.{fact_name}.sql\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 tests\n\u251c\u2500\u2500 intermediates\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 {data_product_name}\n\u2502\u00a0\u00a0     \u2514\u2500\u2500\u2500 {fact_name}\n\u2502\u00a0\u00a0 \u00a0\u00a0     \u251c\u2500\u2500 Makefile\n\u2502\u00a0\u00a0 \u00a0\u00a0     \u251c\u2500\u2500 sql-scripts\n\u2502\u00a0\u00a0 \u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.{intermediate_name}.sql\n\u2502\u00a0\u00a0 \u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.{intermediate_name}.sql\n\u2502\u00a0\u00a0 \u00a0\u00a0     \u2514\u2500\u2500 tests\n\u251c\u2500\u2500 sources\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 {data_product_name}\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 {src_name}\n\u2502\u00a0\u00a0  \u00a0\u00a0     \u251c\u2500\u2500 Makefile\n\u2502\u00a0\u00a0     \u00a0\u00a0  \u251c\u2500\u2500 dedups\n\u2502\u00a0\u00a0      \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.{src_name}.sql\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0  \u00a0\u00a0         \u2514\u2500\u2500 ddl.{src_name}.sql\n</code></pre>"},{"location":"#an-illustrative-example","title":"An illustrative example","text":"<p>The <code>src/shift_left/tests/data</code> folder includes a dbt project used to demonstrate the power of LLM for automatic migration. The dbt is an example of ELT batch processing including a set of SQL scripts organized by sources, intermediates and facts scripts. </p> <ol> <li>The source file to migrate is data/dbt-project/facts/p7/fct_user_role.sql. This file includes references to dbt table names, macros, and then templated SQL statements.</li> <li> <p>When starting a new project, Data engineers, may create the target Flink project, using the Kimball architecture, with the following command:     <pre><code>shift_left project init demo-flink-project ~/Code\n</code></pre></p> </li> <li> <p>Modify the config.yaml file to reflect the Confluent Cloud environment. See dedicated note on the config.yaml, but for migration the only important parameters are:     <pre><code>confluent_cloud:\nflink:\napp:\n</code></pre></p> </li> <li> <p>Modify the set of mandatory environment variables for the Confluent Cloud environment in the <code>.env</code> file under the project, then:     <pre><code>source ~/Code/demo-flink-project/.env\n</code></pre></p> </li> <li> <p>Understand the dependencies of the table to migrate:     <pre><code>shift_left table search-source-dependencies $SRC_FOLDER/facts/p7/fct_user_role.sql\n</code></pre></p> </li> <li> <p>Then for each table to migrate, run the command:     <pre><code>shift_left table migrate user_role  $SRC_FOLDER/facts/p7/fct_user_role.sql $STAGING --recursive\n</code></pre></p> </li> </ol> <p>This command can take some time as it involves multiple calls to LLM.</p>"},{"location":"#source-topic-management","title":"Source Topic management","text":"<p>In some ETL or ELT pipelines, Kafka topics may be the source of the pipeline. A classical example is when a change data capture mechanism is deployed and get records from existing SQL database to Kafka Topics. One topic per table. The architecture looks like in the following diagram:</p> <p></p> <p>When migrating to real-time processing, using Confluent Cloud and Flink compute pool, the source topics may be reused and the Flink Statement will do mostly the same processing as the ELT. Confluent Cloud for Flink creates tables from topic and get the schema from the schema registry.</p> <p>The name of the table matches the name of the topic and the table schema maps to the topic-value schema. </p> <p>Adopt the naming convention from the change data capture, like Debezium for the topic. Any naming convention based on the environment like dev, staging, or production, will impact any DML statements. In Confluent Cloud the environment groups one to many Kafka clusters and multiple Flink compute pools. The topic name can be the same between Kafka cluster so the same DML statement can be used in different conpute pool.</p>"},{"location":"#navigate-the-documentation","title":"Navigate the documentation","text":"<p>To get hands-on lab:</p> <p> guided tutorials</p> <p>As a developer of this project read:</p> <p> How to contribute article  Code structure and development practices</p> <p>As a Data engineer read:</p> <p> The recipe for data engineers and the corresponding referenced practices</p> <p>As Site Reliable Engineer read:</p> <p> The recipe for SRE and the corresponding referenced practices</p>"},{"location":"blue_green_deploy/","title":"Blue/Green Deployment Testing","text":"Version <p>Created Sept/2025 Updated 12/06/2025</p> <p>The goals of the process presented in this note are:</p> <ol> <li>Reduce the impact on continuously running Flink statements: do not redeploy statements when not necessary</li> <li>Reduce the cost of dual environments (Kafka clusters, connectors), and running parallel logic</li> <li>Simplify the overall deployment of Flink statements, and authorize quicker deployment time</li> <li>Avoid redeploying stateful processing with big state to construct when not necessary</li> <li>Take into account the modified file in git/branch as candidates for deployment</li> </ol>"},{"location":"blue_green_deploy/#context","title":"Context","text":"<p>In a classical blue-green deployment, for ETL jobs, the CI/CD process updates everything and once the batch is done, the consumer of the data products, switches to the new content. The following figure illustrates this approach at a high level:</p> Figure 1: Blue-Green for batch processing <p>Blue is the production, Green is the new logic to deploy</p> <p>The processing includes:</p> <ul> <li>reloading the data from the CDC output topics, </li> <li>create a new S3 Sink Connector to write to new bucket folders</li> <li>re-run the batch processing to create the bronze, silver and gold records for consumption by the query engine to serve data to the business intelligence dashboard. </li> </ul> <p>When the green data set is ready the query engine switches to the new object storage location.</p> <p>While in real-time processing the concept of blue-green deployment should only be limited to the Flink pipeline impacted, as presented in the pipeline management chapter.</p> <p>The following figure illustrates the Flink statements are processing data across source, intermediate, and fact tables. </p> Figure 2: Real-time processing with Apache Flink within a Data Stream Plarform <p>On the left side, Raw data originates from Change Data Capture of a transactional database or from event-driven microservices utilizing the transactional outbox pattern. Given the volume of data injected into these raw topics and the need to retain historical data for extended periods, these topics should be rarely re-created.</p> <p>To simplify the diagram above the sink Kafka connectors to the object storage buckets with Iceberg or Delta Lake format are not presented, but it is assumed that those connectors support upsert semantic. </p> <p>On right side, Iceberg or Delta Lake tables, stored in Apache Parquet format, are directly queried by the query engine.</p>"},{"location":"blue_green_deploy/#git-flow-process","title":"Git flow process","text":"<p>As Flink SQLs are managed in a git repository, the process starts by identifying the files modified from a certain time, on a given branch.</p> <p>As an example, the code release goal is to modify only the green statements and redeploy them as part of a blue-green deployment. </p> <p>The general strategy for query evolution involves replacing the existing statement and its corresponding tables with a new statement and new tables. A straightforward approach is to use a release branch, for a short time period, modify the Flink statements, and then deploy those statements to the staging environments. Once validated, these statements can be merged into the <code>main</code> branch where production deploymment may be done. </p> <p>The gitflow process may look like:</p> <ul> <li>main branch: This branch always reflects the production-ready, stable code. Only thoroughly tested and finalized code is merged into <code>main</code>. Commits are tagged in <code>main</code> with version numbers for easy tracking of releases.</li> <li>develop branch: This branch serves as the integration point for all new features and ongoing development. * Feature branches are created from the <code>develop</code> branch and merged back into it after completion and PR review.</li> <li>Creating a Release Branch: When a set of features in develop is deemed ready for release, a new release branch is created from <code>develop</code>. This branch allows for final testing, bug fixes, and release-specific tasks without interrupting the ongoing development work in <code>develop</code>.</li> <li>Finalizing the Release: Only bug fixes and necessary adjustments are made on the <code>release</code> branch. New feature development is strictly avoided.</li> <li> <p>Merging and Tagging: Once the release branch is stable and ready for production deployment, it's merged into two places:</p> <ul> <li><code>main</code>: The release branch is merged into main, effectively updating the production-ready code with the new release.</li> <li><code>develop</code>: The release branch is also merged back into develop to ensure that any bug fixes or changes made during the release preparation are incorporated into the ongoing development work.</li> </ul> </li> <li> <p>Tagging: After merging into main, the merge commit is tagged with a version number (e.g., v1.0.0) to mark the specific release point in the repository's history.</p> </li> <li>Cleanup: After the release is finalized and merged, the release branch can be safely deleted</li> </ul> Figure 3:GitFlow branching for Flink Statement updates <p>An alternate approach is to work directly to the <code>main</code> branch:</p> Figure 3-bis: Branching from main, for Flink Statement updates"},{"location":"blue_green_deploy/#flink-pipelines-deployment","title":"Flink pipelines deployment","text":"<p>To illustrate the process, we will start by this flink pipeline topology, running in production:</p> Figure 4: Current Flink Statements in production <p>The process starts by getting the list of changed flink statements from a given tag or date, on a given git branch. The shift left tool can get the list of statements modified, in a given branch, from a given date:</p> <pre><code># At the project folder level do:\nshift_left project list-modified-files --project-path . --file-filter sql --since 2025-09-10 main\n</code></pre> <p>The above command may list that the tables: <code>int 3</code>, <code>fact 3</code> were modified and <code>view 1</code> was added. Looking at the impact of those changes, the tool needs to redeploy the following tables with a new version:</p> Figure 5: Flink logic update and impacted statements <p>The above figure illustrates those new tables:</p> Blue Table name Green Table name Statement name Triggered Change int_3 int_3_v2 dml.int_3 User modified content, tool adds _v2 fact_3 fact_3_v2 dml.fact_3 User modified content, tool adds _v2 fact_2 fact_2_v2 dml.fact_2 tool adds _v2 for output as it modified input(s). fact_2 was not modified in the git, this is a side effect of the relationship view32 dml.view32 User created this new content - no extension <p>Also as a side effect the sink connectors configuration need to be modified to go to _v2 topics and even add one new connector because of the new view32, table.</p> <p>The command creates two files under the $HOME/.shift_left folder: </p> File name Type Content modified_flink_files.txt json contains a filelist with element like: <code>{\"table_name\": \"p1_dim_c2\",\"file_modified_url\": \"...pipelines/dimensions/p1/dim_c2/sql-scripts/ddl.dim_c2.sql\",\"same_sql_content\": false,\"running\": false }</code> modified_flink_files_short.txt txt list of table name only <p>With this, it will be possible to assess the execution plan with:</p> <pre><code>shift_left pipeline build-execution-plan --table-list-file-name  ~/.shift_left/modified_flink_files_short.txt\n</code></pre> <p>The DDL Flink statements need to have a new table name with the next version postfix (e.g. int_3_v2). </p> <p><pre><code>--- DDL intermediate table\ncreate table int_3_v2 (\n    --- all columns, new columns, ...\n)\n</code></pre> The DML with the <code>insert into</code> table name also needs to be modified.</p> <pre><code>-- DML intermediate table\ninsert into int_3_v2 \nselect \n...\nfrom src_a ...\njoin src_b ... \njoin src_c  ...\n</code></pre> <p>Any children of the modified statement needs to take into account the new table name of it input tables. For example the fact table needs to use the new versioned intermediate table:</p> <pre><code>--- DML Fact table\ninsert into fact_3_v2\nselect \n...\nfrom int_3_v2  -- ATTENTION \njoin int_1\n</code></pre> <p>For the 'view' creation, the Flink statement may be impacted as one of its source table is modified. So the same renaming logic applies.</p> <p>During the tuning on the impacted statements, the pipeline dependencies can help assessing which statements to change. (e.g. <code>shift_left pipeline build-execution-plan --table-name &lt;flink-intermediate&gt; --may-start-descendants</code>).</p> <p>The list of impacted table can be specified in a text file and specified as parameter to the deployment:</p> <pre><code>shift_left pipeline deploy --table-list-file-name statement_list.txt --may-start-descendants\n</code></pre>"},{"location":"blue_green_deploy/#different-deployment-scenarios","title":"Different deployment scenarios","text":"<p>To demonstrate blue/green deployment we will take the flink_project_demos git repo as a source of a real-time processing using a Kimball structure.</p> Access to the the flink demos repository <p>Clone the https://github.com/jbcodeforce/flink_project_demos. Set environment variables like: <pre><code>export FLINK_PROJECT=$HOME/Code/flink_project_demos/customer_360/c360_flink_processing\nexport PIPELINES=$FLINK_PROJECT/pipelines\n</code></pre></p> <p>The current pipeline is presented in this figure:</p> <p></p>"},{"location":"blue_green_deploy/#version-migration-rules","title":"Version migration rules","text":"<ul> <li>For table with non existant <code>_v[0-9]+</code> as postfix, then use the default postfix of <code>_v2</code></li> <li>For table with existing <code>_v[0-9]+</code>, then extract the numerical value, and add one: <code>_v2</code> -&gt; <code>_v3</code>, <code>_v9</code> -&gt; <code>_v10</code></li> <li>Statement name does not need to have this name update. </li> </ul>"},{"location":"blue_green_deploy/#sink-tables","title":"Sink tables","text":"<p>Some Facts and Views have no child, in this case the version management is to modify the DDL and DML to change the current table with a new version. No need to walk to descendants</p>"},{"location":"blue_green_deploy/#intermediate-tables","title":"Intermediate tables","text":"<p>For any table with descendants, need to modify DDL and DML of current, and for each direct descedant modify DML for new input version of current table, and then create new version for DDL and DML. Continue recursively until a table has no descendant.</p> <p>The list of table modified will increase because of those changes.</p>"},{"location":"blue_green_deploy/#source-schema-evolution","title":"Source schema evolution","text":"<p>For source schema evolution, it is assumed the modifications are schema compatible with Full Transitive semantic. </p> Figure 6: Transactional data change: schema evolution <p>The CDC raw topic will contain records with both old and new schema definitions. The initial Flink statement, responsible for creating the source topic, is affected as it must now process new columns. This statement, which may handle deduplication, filtering, primary key redefinition, and field encryption, is designed to process both the previous and new schema versions. When Flink Statements are executed they load the last version of a schema and keep it in their state. Restarting the first level of Flink statements (to create src_), will get the last schema version, the new version. As it may reprocess records earliest offset, it will generate new records with default value or new values. This is a new version too.</p> <p>So it may make sense to take into account schema modification in the registry as part of the CI/CD process to be able to modify the list of modified file, as Flink statements to create those src_ may not be aware to the change done into the transactional table schema.</p>"},{"location":"blue_green_deploy/#a-bluegreen-deployment-process","title":"A blue/green deployment process","text":""},{"location":"blue_green_deploy/#pre-deployment-activities","title":"Pre-deployment activities","text":"<ul> <li>Get the list of Flink statements modified in <code>main</code> branch since a given date:     <pre><code>shift_left project list-modified-files --project-path . --file-filter sql --since 2025-09-10 main\n</code></pre>     This will create <code>$HOME/.shift_left/modified_flink_files.txt</code> and <code>$HOME/.shift_left/modified_flink_files.json</code></li> <li>Create git release branch (<code>git checkout -b v1.0.1</code>)</li> <li>Potentially modify the <code>modified_flink_files_short.txt</code> to include table SREs know they have to change.</li> <li> <p>Modify each Flink statement for the modified tables so the DDLs and <code>insert into</code> of the DLMs use the new version postfix, taking into consideration their descendants.     <pre><code>shift_left project update-table-version $HOME/.shift_left/modified_flink_files.json --default_version _v2\n</code></pre></p> </li> <li> <p>From the list of tables impacted, review execution plan     <pre><code> shift_left pipeline build-execution-plan --table-list-file-name .shift_left/modified_flink_files_short.txt --version _v2\n</code></pre></p> </li> <li>Verify resource (compute pool and CFU usage) availability</li> <li>Deploy to the <code>stage</code> environment (use a specific config.yaml file): an environment with existing Flink statements are already running     <pre><code>export CONFIG_FILE=staging_config.yaml\nshift_left pipeline deploy --table-list-file-name .shift_left/modified_flink_files_short.txt\n</code></pre></li> </ul>"},{"location":"blue_green_deploy/#data-quality-validation","title":"Data Quality Validation","text":"<p>The tools and practices need to address:</p> <ul> <li>Schema Compatibility Checks: Validate that new table versions maintain backward compatibility</li> <li>Data Lineage Validation: Ensure data flows correctly through the new pipeline versions</li> <li>Record Count Validation: Compare record counts between blue/green versions</li> <li>Data Freshness Checks: Validate that data processing latency hasn't increased</li> </ul>"},{"location":"blue_green_deploy/#integration-tests","title":"Integration tests","text":"<p>Generate comprehensive test data for blue-green validation</p> <ul> <li>Send a sample of synthetic test data to source topics, validate they reach sink tables</li> <li>Verify no duplicate records are created from the new deployment in the output tables</li> <li>Validate all source records are processed</li> <li>Compare aggregations between blue/green versions</li> <li>Ensure event time processing remains consistent</li> </ul>"},{"location":"blue_green_deploy/#test-rollback-procedures","title":"Test rollback procedures","text":""},{"location":"command/","title":"CLI","text":"<p>Usage:</p> <pre><code>$ [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>version</code>: Display the current version of shift-left...</li> <li><code>project</code></li> <li><code>table</code></li> <li><code>pipeline</code></li> </ul>"},{"location":"command/#version","title":"<code>version</code>","text":"<p>Display the current version of shift-left CLI.</p> <p>Usage:</p> <pre><code>$ version [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project","title":"<code>project</code>","text":"<p>Usage:</p> <pre><code>$ project [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>init</code>: Create a project structure with a...</li> <li><code>list-topics</code>: Get the list of topics for the Kafka...</li> <li><code>list-compute-pools</code>: Get the complete list and detail of the...</li> <li><code>delete-all-compute-pools</code>: Delete all compute pools for the given...</li> <li><code>housekeep-statements</code>: Delete statements in FAILED or COMPLETED...</li> <li><code>validate-config</code>: Validate the config.yaml file</li> <li><code>report-table-cross-products</code>: Report the list of tables that are...</li> <li><code>list-tables-with-one-child</code>: Report the list of tables that have...</li> <li><code>list-modified-files</code>: Get the list of files modified in the...</li> <li><code>update-tables-version</code>: Update the table version for the given file</li> <li><code>init-integration-tests</code>: Initialize integration test structure for...</li> <li><code>run-integration-tests</code>: Run integration tests for a given sink table.</li> <li><code>delete-integration-tests</code>: Delete all integration test artifacts...</li> <li><code>isolate-data-product</code>: Isolate the data product from the project</li> <li><code>get-statement-list</code>: Get the list of statements</li> <li><code>assess-unused-tables</code>: Assess Flink SQL tables that are not used...</li> <li><code>delete-unused-tables</code>: Delete unused tables</li> </ul>"},{"location":"command/#project-init","title":"<code>project init</code>","text":"<p>Create a project structure with a specified name, target path, and optional project type. The project type can be one of <code>kimball</code> or <code>data_product</code>. Kimball will use a structure like pipelines/sources pipelines/facts pipelines/dimensions ...</p> <p>Usage:</p> <pre><code>$ project init [OPTIONS] [PROJECT_NAME] [PROJECT_PATH]\n</code></pre> <p>Arguments:</p> <ul> <li><code>[PROJECT_NAME]</code>: Name of project to create  [default: default_data_project]</li> <li><code>[PROJECT_PATH]</code>: [default: ./tmp]</li> </ul> <p>Options:</p> <ul> <li><code>--project-type TEXT</code>: [default: kimball]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-list-topics","title":"<code>project list-topics</code>","text":"<p>Get the list of topics for the Kafka Cluster define in <code>config.yaml</code> and save the list in the <code>topic_list.txt</code> file under the given folder. Be sure to have a <code>conflig.yaml</code> file setup.</p> <p>Usage:</p> <pre><code>$ project list-topics [OPTIONS] PROJECT_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>PROJECT_PATH</code>: Project path to save the topic list text file.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-list-compute-pools","title":"<code>project list-compute-pools</code>","text":"<p>Get the complete list and detail of the compute pools of the given environment_id. If the environment_id is not specified, it will use the conflig.yaml with the ['confluent_cloud']['environment_id']</p> <p>Usage:</p> <pre><code>$ project list-compute-pools [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--environment-id TEXT</code>: Environment_id to return all compute pool</li> <li><code>--region TEXT</code>: Region_id to return all compute pool</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-delete-all-compute-pools","title":"<code>project delete-all-compute-pools</code>","text":"<p>Delete all compute pools for the given product name</p> <p>Usage:</p> <pre><code>$ project delete-all-compute-pools [OPTIONS] PRODUCT_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>PRODUCT_NAME</code>: The product name to delete all compute pools for  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-housekeep-statements","title":"<code>project housekeep-statements</code>","text":"<p>Delete statements in FAILED or COMPLETED state that starts with string 'workspace' in it ( default ). Applies optional starts-with, age, status filters when provided. Note:  and  are mutually exclusive.</p> <p>Usage:</p> <pre><code>$ project housekeep-statements [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID.</li> <li><code>--action TEXT</code>: Action to take on the statements.</li> <li><code>--statement-name TEXT</code>: Action to take on specific statement.</li> <li><code>--starts-with TEXT</code>: Statements names starting with this string.</li> <li><code>--status TEXT</code>: Statements with this status.</li> <li><code>--age INTEGER</code>: Statements with created_date &gt;= age (days).</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-validate-config","title":"<code>project validate-config</code>","text":"<p>Validate the config.yaml file</p> <p>Usage:</p> <pre><code>$ project validate-config [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-report-table-cross-products","title":"<code>project report-table-cross-products</code>","text":"<p>Report the list of tables that are referenced in other products</p> <p>Usage:</p> <pre><code>$ project report-table-cross-products [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-list-tables-with-one-child","title":"<code>project list-tables-with-one-child</code>","text":"<p>Report the list of tables that have exactly one child table</p> <p>Usage:</p> <pre><code>$ project list-tables-with-one-child [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-list-modified-files","title":"<code>project list-modified-files</code>","text":"<p>Get the list of files modified in the current git branch compared to the specified branch. Filters for Flink-related files (by default SQL files) and saves the list to a text file. This is useful for identifying which Flink statements need to be redeployed in a blue-green deployment.</p> <p>Usage:</p> <pre><code>$ project list-modified-files [OPTIONS] BRANCH_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>BRANCH_NAME</code>: Git branch name to compare against (e.g., 'main', 'origin/main')  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--project-path TEXT</code>: Project path where git repository is located  [default: .]</li> <li><code>--file-filter TEXT</code>: File extension filter (e.g., '.sql', '.py')  [default: .sql]</li> <li><code>--since TEXT</code>: Date from which the files were modified (e.g., 'YYYY-MM-DD')  [default: 2025-12-01]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-update-tables-version","title":"<code>project update-tables-version</code>","text":"<p>Update the table version for the given file</p> <p>Usage:</p> <pre><code>$ project update-tables-version [OPTIONS] TABLE_LIST_FILE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_LIST_FILE_NAME</code>: File name containing json object with table names and file modified url to update the table version for  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--default-version TEXT</code>: Default version to use if not provided in the file  [default: _v2]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-init-integration-tests","title":"<code>project init-integration-tests</code>","text":"<p>Initialize integration test structure for a given sink table. Creates test scaffolding including synthetic data templates and validation queries. Integration tests validate end-to-end data flow from source tables to the specified sink table.</p> <p>Usage:</p> <pre><code>$ project init-integration-tests [OPTIONS] SINK_TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>SINK_TABLE_NAME</code>: Name of the sink table to create integration tests for  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--project-path TEXT</code>: Project path where pipelines are located. If not provided, uses $PIPELINES environment variable  [env var: PIPELINES]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-run-integration-tests","title":"<code>project run-integration-tests</code>","text":"<p>Run integration tests for a given sink table. Executes end-to-end pipeline testing by injecting synthetic data into source tables, waiting for data to flow through the pipeline, and validating results in the sink table. Optionally measures latency from data injection to sink table arrival.</p> <p>Usage:</p> <pre><code>$ project run-integration-tests [OPTIONS] SINK_TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>SINK_TABLE_NAME</code>: Name of the sink table to run integration tests for  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--scenario-name TEXT</code>: Specific test scenario to run. If not specified, runs all scenarios</li> <li><code>--project-path TEXT</code>: Project path where pipelines are located  [env var: PIPELINES]</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID. Uses config.yaml value if not provided  [env var: CPOOL_ID]</li> <li><code>--measure-latency / --no-measure-latency</code>: Whether to measure end-to-end latency from source to sink  [default: measure-latency]</li> <li><code>--output-file TEXT</code>: File path to save detailed test results</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-delete-integration-tests","title":"<code>project delete-integration-tests</code>","text":"<p>Delete all integration test artifacts (Flink statements and Kafka topics) for a given sink table. This cleanup command removes all test-related resources created during integration test execution.</p> <p>Usage:</p> <pre><code>$ project delete-integration-tests [OPTIONS] SINK_TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>SINK_TABLE_NAME</code>: Name of the sink table to delete integration test artifacts for  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--project-path TEXT</code>: Project path where pipelines are located  [env var: PIPELINES]</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID where test artifacts were created  [env var: CPOOL_ID]</li> <li><code>--no-confirm</code>: Skip confirmation prompt and delete immediately</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-isolate-data-product","title":"<code>project isolate-data-product</code>","text":"<p>Isolate the data product from the project</p> <p>Usage:</p> <pre><code>$ project isolate-data-product [OPTIONS] PRODUCT_NAME SOURCE_FOLDER TARGET_FOLDER\n</code></pre> <p>Arguments:</p> <ul> <li><code>PRODUCT_NAME</code>: Product name to isolate  [required]</li> <li><code>SOURCE_FOLDER</code>: Source folder to isolate the data product  [required]</li> <li><code>TARGET_FOLDER</code>: Target folder to isolate the data product  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-get-statement-list","title":"<code>project get-statement-list</code>","text":"<p>Get the list of statements</p> <p>Usage:</p> <pre><code>$ project get-statement-list [OPTIONS] COMPUTE_POOL_ID\n</code></pre> <p>Arguments:</p> <ul> <li><code>COMPUTE_POOL_ID</code>: Compute pool id to get the statement list for  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-assess-unused-tables","title":"<code>project assess-unused-tables</code>","text":"<p>Assess Flink SQL tables that are not used by any running DML statements.</p> <p>This command: - Gets all tables from the inventory - Checks which tables are referenced by running Flink statements - Identifies tables that are not referenced (potentially unused) - Optionally compares Kafka topics to find unused topics</p> <p>The analysis uses existing pipeline.json files which contain parent/child relationships, so no SQL parsing is required.</p> <p>Usage:</p> <pre><code>$ project assess-unused-tables [OPTIONS] INVENTORY_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>INVENTORY_PATH</code>: Pipeline path where tables are defined  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--include-topics / --no-topics</code>: Also check for unused Kafka topics  [default: include-topics]</li> <li><code>--output-file TEXT</code>: File path to save results (default: unused_tables_&lt;timestamp&gt;.txt)</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#project-delete-unused-tables","title":"<code>project delete-unused-tables</code>","text":"<p>Delete unused tables</p> <p>Usage:</p> <pre><code>$ project delete-unused-tables [OPTIONS] TABLE_LIST_FILE INVENTORY_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_LIST_FILE</code>: File path to table list to delete  [required]</li> <li><code>INVENTORY_PATH</code>: Pipeline path where tables are defined  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table","title":"<code>table</code>","text":"<p>Usage:</p> <pre><code>$ table [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>init</code>: Build a new table structure under the...</li> <li><code>build-inventory</code>: Build the table inventory from the...</li> <li><code>search-source-dependencies</code>: Search the parent for a given table from...</li> <li><code>migrate</code>: Migrate a source SQL Table defined in a...</li> <li><code>update-makefile</code>: Update existing Makefile for a given table...</li> <li><code>update-all-makefiles</code>: Update all the Makefiles for all the...</li> <li><code>validate-table-names</code>: Go over the pipeline folder to assess if...</li> <li><code>update-tables</code>: Update the tables with SQL code changes...</li> <li><code>init-unit-tests</code>: Initialize the unit test folder and...</li> <li><code>run-unit-tests</code>: Run all the unit tests or a specified test...</li> <li><code>run-validation-tests</code>: Run only the validation tests (1 to n...</li> <li><code>validate-unit-tests</code>: just a synonym for run-validation-tests</li> <li><code>delete-unit-tests</code>: Delete the Flink statements and kafka...</li> <li><code>explain</code>: Get the Flink execution plan explanations...</li> </ul>"},{"location":"command/#table-init","title":"<code>table init</code>","text":"<p>Build a new table structure under the specified path. For example to add a source table structure use for example the command: <code>shift_left table init src_table_1 $PIPELINES/sources/p1</code></p> <p>Usage:</p> <pre><code>$ table init [OPTIONS] TABLE_NAME TABLE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Table name to build  [required]</li> <li><code>TABLE_PATH</code>: Folder Path in which the table folder structure will be created.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--product-name TEXT</code>: Product name to use for the table. If not provided, it will use the table_path last folder as product name</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-build-inventory","title":"<code>table build-inventory</code>","text":"<p>Build the table inventory from the PIPELINES path.</p> <p>Usage:</p> <pre><code>$ table build-inventory [OPTIONS] PIPELINE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>PIPELINE_PATH</code>: Pipeline folder where all the tables are defined, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-search-source-dependencies","title":"<code>table search-source-dependencies</code>","text":"<p>Search the parent for a given table from the source project (dbt, sql or ksql folders). Example: shift_left table search-source-dependencies $SRC_FOLDER/</p> <p>Usage:</p> <pre><code>$ table search-source-dependencies [OPTIONS] TABLE_SQL_FILE_NAME SRC_PROJECT_FOLDER\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_SQL_FILE_NAME</code>: Full path to the file name of the dbt sql file  [required]</li> <li><code>SRC_PROJECT_FOLDER</code>: Folder name for all the dbt sources (e.g. models)  [env var: SRC_FOLDER; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-migrate","title":"<code>table migrate</code>","text":"<p>Migrate a source SQL Table defined in a sql file with AI Agent to a Staging area to complete the work. The command uses the SRC_FOLDER to access to src_path folder.</p> <p>Usage:</p> <pre><code>$ table migrate [OPTIONS] TABLE_NAME SQL_SRC_FILE_NAME TARGET_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: the name of the table once migrated.  [required]</li> <li><code>SQL_SRC_FILE_NAME</code>: the source file name for the sql script to migrate.  [required]</li> <li><code>TARGET_PATH</code>: the target path where to store the migrated content (default is $STAGING)  [env var: STAGING; required]</li> </ul> <p>Options:</p> <ul> <li><code>--source-type TEXT</code>: the type of the SQL source file to migrate. It can be ksql, dbt, spark, etc.  [default: spark]</li> <li><code>--validate</code>: Validate the migrated sql using Confluent Cloud for Flink.</li> <li><code>--product-name TEXT</code>: Product name to use for the table. If not provided, it will use the table_path last folder as product name  [default: default]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-update-makefile","title":"<code>table update-makefile</code>","text":"<p>Update existing Makefile for a given table or build a new one</p> <p>Usage:</p> <pre><code>$ table update-makefile [OPTIONS] TABLE_NAME PIPELINE_FOLDER_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Name of the table to process and update the Makefile from.  [required]</li> <li><code>PIPELINE_FOLDER_NAME</code>: Pipeline folder where all the tables are defined, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-update-all-makefiles","title":"<code>table update-all-makefiles</code>","text":"<p>Update all the Makefiles for all the tables in the given folder. Example: shift_left table update-all-makefiles $PIPELINES/dimensions/product_1</p> <p>Usage:</p> <pre><code>$ table update-all-makefiles [OPTIONS] FOLDER_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>FOLDER_NAME</code>: Folder from where all the Makefile will be updated. If not provided, it will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-validate-table-names","title":"<code>table validate-table-names</code>","text":"<p>Go over the pipeline folder to assess if table name,  naming convention, and other development best practices are respected.</p> <p>Usage:</p> <pre><code>$ table validate-table-names [OPTIONS] PIPELINE_FOLDER_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>PIPELINE_FOLDER_NAME</code>: Pipeline folder where all the tables are defined, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-update-tables","title":"<code>table update-tables</code>","text":"<p>Update the tables with SQL code changes defined in external python callback. It will read dml or ddl and apply the updates.</p> <p>Usage:</p> <pre><code>$ table update-tables [OPTIONS] FOLDER_TO_WORK_FROM\n</code></pre> <p>Arguments:</p> <ul> <li><code>FOLDER_TO_WORK_FROM</code>: Folder from where to do the table update. It could be the all pipelines or subfolders.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--ddl</code>: Focus on DDL processing. Default is only DML</li> <li><code>--both-ddl-dml</code>: Run both DDL and DML sql files</li> <li><code>--string-to-change-from TEXT</code>: String to change in the SQL content</li> <li><code>--string-to-change-to TEXT</code>: String to change in the SQL content</li> <li><code>--class-to-use TEXT</code>: [default: typing.Annotated[str, &lt;typer.models.ArgumentInfo object at 0x106cfd040&gt;]]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-init-unit-tests","title":"<code>table init-unit-tests</code>","text":"<p>Initialize the unit test folder and template files for a given table. It will parse the SQL statements to create the insert statements for the unit tests. It is using the table inventory to find the table folder for the given table name. Optionally, it can also create a CSV file for the unit test data if --create-csv is set.</p> <p>Usage:</p> <pre><code>$ table init-unit-tests [OPTIONS] TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Name of the table to unit tests.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--create-csv</code>: If set, also create a CSV file for the unit test data.</li> <li><code>--nb-test-cases INTEGER</code>: Number of test cases to create. Default is 2.  [default: 2]</li> <li><code>--ai</code>: Use AI to generate test data and validate with tool calling.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-run-unit-tests","title":"<code>table run-unit-tests</code>","text":"<p>Run all the unit tests or a specified test case by sending data to <code>_ut</code> topics and validating the results</p> <p>Usage:</p> <pre><code>$ table run-unit-tests [OPTIONS] TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Name of the table to unit tests.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--test-case-name TEXT</code>: Name of the individual unit test to run. By default it will run all the tests</li> <li><code>--run-all</code>: By default run insert sqls and foundations, with this flag it will also run validation sql too.</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID. If not provided, it will use config.yaml one.  [env var: CPOOL_ID]</li> <li><code>--post-fix-unit-test TEXT</code>: Provide a unique post fix (e.g _foo) to avoid conflicts with other UT runs. If not provided will use config.yaml, if that doesnt exist, use default _ut.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-run-validation-tests","title":"<code>table run-validation-tests</code>","text":"<p>Run only the validation tests (1 to n validation tests) for a given table.</p> <p>Usage:</p> <pre><code>$ table run-validation-tests [OPTIONS] TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Name of the table to unit tests.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--test-case-name TEXT</code>: Name of the individual unit test to run. By default it will run all the tests</li> <li><code>--run-all</code>: With this flag, and not test case name provided, it will run all the validation sqls.</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID. If not provided, it will use config.yaml one.  [env var: CPOOL_ID]</li> <li><code>--post-fix-unit-test TEXT</code>: By default it is _ut. A Unique post fix to avoid conflict between multiple UT runs. If not provided, it will use config.yaml one.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-validate-unit-tests","title":"<code>table validate-unit-tests</code>","text":"<p>just a synonym for run-validation-tests</p> <p>Usage:</p> <pre><code>$ table validate-unit-tests [OPTIONS] TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Name of the table to unit tests.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--test-case-name TEXT</code>: Name of the individual unit test to run. By default it will run all the tests</li> <li><code>--run-all</code>: With this flag, and not test case name provided, it will run all the validation sqls.</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID. If not provided, it will use config.yaml one.  [env var: CPOOL_ID]</li> <li><code>--post-fix-unit-test TEXT</code>: By default it is _ut. A Unique post fix to avoid conflict between multiple UT runs. If not provided, it will use config.yaml one.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-delete-unit-tests","title":"<code>table delete-unit-tests</code>","text":"<p>Delete the Flink statements and kafka topics used for unit tests for a given table.</p> <p>Usage:</p> <pre><code>$ table delete-unit-tests [OPTIONS] TABLE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Name of the table to unit tests.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID. If not provided, it will use config.yaml one.  [env var: CPOOL_ID]</li> <li><code>--post-fix-unit-test TEXT</code>: By default it is _ut. A Unique post fix to avoid conflict between multiple UT runs. If not provided, it will use config.yaml one.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#table-explain","title":"<code>table explain</code>","text":"<p>Get the Flink execution plan explanations for a given table or a group of tables using the product name or a list of tables from a file.</p> <p>Usage:</p> <pre><code>$ table explain [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--table-name TEXT</code>: Name of the table to get Flink execution plan explanations from.</li> <li><code>--product-name TEXT</code>: The directory to run the explain on each tables found within this directory. table or dir needs to be provided.</li> <li><code>--table-list-file-name TEXT</code>: The file containing the list of tables to deploy.</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID. If not provided, it will use config.yaml one.  [env var: CPOOL_ID]</li> <li><code>--persist-report</code>: Persist the report in the shift_left_dir folder.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline","title":"<code>pipeline</code>","text":"<p>Usage:</p> <pre><code>$ pipeline [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>field-lineage</code>: Compute field-level lineage for a table up...</li> <li><code>build-metadata</code>: Build a pipeline definition metadata by...</li> <li><code>delete-all-metadata</code>: Delete all pipeline definition json files...</li> <li><code>build-all-metadata</code>: Go to the hierarchy of folders for...</li> <li><code>report</code>: Generate a report showing the static...</li> <li><code>healthcheck</code>: Generate a healthcheck report of a given...</li> <li><code>deploy</code>: Deploy a pipeline from a given table name...</li> <li><code>build-execution-plan</code>: From a given table, this command goes all...</li> <li><code>report-running-statements</code>: Assess for a given table, what are the...</li> <li><code>undeploy</code>: From a given sink table, this command goes...</li> <li><code>prepare</code>: Execute the content of the sql file, line...</li> <li><code>analyze-pool-usage</code>: Analyze compute pool usage and assess...</li> </ul>"},{"location":"command/#pipeline-field-lineage","title":"<code>pipeline field-lineage</code>","text":"<p>Compute field-level lineage for a table up to sources and save metadata plus graph under $HOME/.shift_left/field_lineage.</p> <p>Usage:</p> <pre><code>$ pipeline field-lineage [OPTIONS] TABLE_NAME PIPELINE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: Table name (must exist in inventory).  [required]</li> <li><code>PIPELINE_PATH</code>: Pipeline path; uses $PIPELINES if not set.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>-o, --output-dir TEXT</code>: Output directory for lineage JSON and HTML graph.</li> <li><code>--open</code>: Open the generated HTML graph in the default browser.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-build-metadata","title":"<code>pipeline build-metadata</code>","text":"<p>Build a pipeline definition metadata by reading the Flink dml SQL content for the given dml file.</p> <p>Usage:</p> <pre><code>$ pipeline build-metadata [OPTIONS] DML_FILE_NAME PIPELINE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>DML_FILE_NAME</code>: The path to the DML file. e.g. $PIPELINES/table-name/sql-scripts/dml.table-name.sql  [required]</li> <li><code>PIPELINE_PATH</code>: Pipeline path, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-delete-all-metadata","title":"<code>pipeline delete-all-metadata</code>","text":"<p>Delete all pipeline definition json files from a given folder path</p> <p>Usage:</p> <pre><code>$ pipeline delete-all-metadata [OPTIONS] PATH_FROM_WHERE_TO_DELETE\n</code></pre> <p>Arguments:</p> <ul> <li><code>PATH_FROM_WHERE_TO_DELETE</code>: Delete metadata pipeline_definitions.json in the given folder  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-build-all-metadata","title":"<code>pipeline build-all-metadata</code>","text":"<p>Go to the hierarchy of folders for dimensions, views and facts and build the pipeline definitions for each table found using recursing walk through</p> <p>Usage:</p> <pre><code>$ pipeline build-all-metadata [OPTIONS] PIPELINE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>PIPELINE_PATH</code>: Pipeline path, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-report","title":"<code>pipeline report</code>","text":"<p>Generate a report showing the static pipeline hierarchy for a given table using its pipeline_definition.json</p> <p>Usage:</p> <pre><code>$ pipeline report [OPTIONS] TABLE_NAME PIPELINE_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>TABLE_NAME</code>: The table name containing pipeline_definition.json. e.g. customer_analytics_c360. The name has to exist in inventory as a key.  [required]</li> <li><code>PIPELINE_PATH</code>: Pipeline path, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--yaml</code>: Output the report in YAML format</li> <li><code>--json</code>: Output the report in JSON format</li> <li><code>--children-too / --no-children-too</code>: By default the report includes only parents, this flag focuses on getting children  [default: no-children-too]</li> <li><code>--parent-only / --no-parent-only</code>: By default the report includes only parents  [default: parent-only]</li> <li><code>--output-file-name TEXT</code>: Output file name to save the report.</li> <li><code>--open</code>: Open the generated HTML graph in the default browser.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-healthcheck","title":"<code>pipeline healthcheck</code>","text":"<p>Generate a healthcheck report of a given product pipeline</p> <p>Usage:</p> <pre><code>$ pipeline healthcheck [OPTIONS] PRODUCT_NAME INVENTORY_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>PRODUCT_NAME</code>: The product name. e.g. qx, aqem, mx. The name has to exist in inventory as a key.  [required]</li> <li><code>INVENTORY_PATH</code>: Pipeline path, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-deploy","title":"<code>pipeline deploy</code>","text":"<p>Deploy a pipeline from a given table name , product name or a directory taking into account the execution plan. It can run the deployment in parallel or sequential. Four approaches are possible: 1. Deploy from a given table name 2. Deploy from a product name 3. Deploy from a directory 4. Deploy from a table list file name</p> <p>Usage:</p> <pre><code>$ pipeline deploy [OPTIONS] INVENTORY_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>INVENTORY_PATH</code>: Path to the inventory folder, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--table-name TEXT</code>: The table name containing pipeline_definition.json.</li> <li><code>--product-name TEXT</code>: The product name to deploy.</li> <li><code>--table-list-file-name TEXT</code>: The file containing the list of tables to deploy.</li> <li><code>--exclude-table-file-name TEXT</code>: The file containing the list of tables to exclude from the deployment.</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID. If not provided, it will create a pool.</li> <li><code>--dml-only / --no-dml-only</code>: By default the deployment will do DDL and DML, with this flag it will deploy only DML  [default: no-dml-only]</li> <li><code>--may-start-descendants / --no-may-start-descendants</code>: The children deletion will be done only if they are stateful. This Flag force to drop table and recreate all (ddl, dml)  [default: no-may-start-descendants]</li> <li><code>--force-ancestors / --no-force-ancestors</code>: When reaching table with no ancestor, this flag forces restarting running Flink statements.  [default: no-force-ancestors]</li> <li><code>--cross-product-deployment / --no-cross-product-deployment</code>: By default the deployment will deploy only tables from the same product. This flag allows to deploy tables from different products.  [default: no-cross-product-deployment]</li> <li><code>--dir TEXT</code>: The directory to deploy the pipeline from. If not provided, it will deploy the pipeline from the table name.</li> <li><code>--parallel / --no-parallel</code>: By default the deployment will deploy the pipeline in parallel. This flag will deploy the pipeline in parallel.  [default: no-parallel]</li> <li><code>--max-thread INTEGER</code>: The maximum number of threads to use when deploying the pipeline in parallel.  [default: 1]</li> <li><code>--pool-creation / --no-pool-creation</code>: By default the deployment will not create a compute pool per table. This flag will create a pool.  [default: no-pool-creation]</li> <li><code>--version TEXT</code>: The default version to use, at the table level for modified flink statements. But when set the tool will increasing existing version in SQL. Example of string is '_v2'. Only used if table_list_file_name is provided.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-build-execution-plan","title":"<code>pipeline build-execution-plan</code>","text":"<p>From a given table, this command goes all the way to the full pipeline and assess the execution plan taking into account parent, children and existing Flink Statement running status. It does not deploy. This is a command for analysis.</p> <p>Usage:</p> <pre><code>$ pipeline build-execution-plan [OPTIONS] INVENTORY_PATH\n</code></pre> <p>Arguments:</p> <ul> <li><code>INVENTORY_PATH</code>: Path to the inventory folder, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; required]</li> </ul> <p>Options:</p> <ul> <li><code>--table-name TEXT</code>: The table name to deploy from. Can deploy ancestors and descendants.</li> <li><code>--product-name TEXT</code>: The product name to deploy from. Can deploy ancestors and descendants of the tables part of the product.</li> <li><code>--dir TEXT</code>: The directory to deploy the pipeline from.</li> <li><code>--table-list-file-name TEXT</code>: The file containing the list of tables to deploy.</li> <li><code>--exclude-table-file-name TEXT</code>: The file containing the list of tables to exclude from the deployment.</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID to use as default.</li> <li><code>--dml-only / --no-dml-only</code>: By default the deployment will do DDL and DML, with this flag it will deploy only DML  [default: no-dml-only]</li> <li><code>--may-start-descendants / --no-may-start-descendants</code>: The descendants will not be started by default. They may be started differently according to the fact they are stateful or stateless.  [default: no-may-start-descendants]</li> <li><code>--force-ancestors / --no-force-ancestors</code>: This flag forces restarting running ancestorsFlink statements.  [default: no-force-ancestors]</li> <li><code>--cross-product-deployment / --no-cross-product-deployment</code>: By default the deployment will deploy only tables from the same product. This flag allows to deploy tables from different products when considering descendants only.  [default: no-cross-product-deployment]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-report-running-statements","title":"<code>pipeline report-running-statements</code>","text":"<p>Assess for a given table, what are the running dmls from its descendants. When the directory is specified, it will report the running statements from all the tables in the directory.</p> <p>Usage:</p> <pre><code>$ pipeline report-running-statements [OPTIONS] [INVENTORY_PATH]\n</code></pre> <p>Arguments:</p> <ul> <li><code>[INVENTORY_PATH]</code>: Path to the inventory folder, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; default: ./tests/data/flink-project/pipelines]</li> </ul> <p>Options:</p> <ul> <li><code>--dir TEXT</code>: The directory to report the running statements from. If not provided, it will report the running statements from the table name.</li> <li><code>--table-name TEXT</code>: The table name containing pipeline_definition.json to get child list</li> <li><code>--product-name TEXT</code>: The product name to report the running statements from.</li> <li><code>--from-date TEXT</code>: The date from which to report the metrics from. Format: YYYY-MM-DDThhss</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-undeploy","title":"<code>pipeline undeploy</code>","text":"<p>From a given sink table, this command goes all the way to the full pipeline and delete tables and Flink statements not shared with other statements.</p> <p>Usage:</p> <pre><code>$ pipeline undeploy [OPTIONS] [INVENTORY_PATH]\n</code></pre> <p>Arguments:</p> <ul> <li><code>[INVENTORY_PATH]</code>: Path to the inventory folder, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES; default: ./tests/data/flink-project/pipelines]</li> </ul> <p>Options:</p> <ul> <li><code>--table-name TEXT</code>: The sink table name from where the undeploy will run.</li> <li><code>--product-name TEXT</code>: The product name to undeploy from</li> <li><code>--no-ack / --no-no-ack</code>: By default the undeploy will ask for confirmation. This flag will undeploy without confirmation.  [default: no-no-ack]</li> <li><code>--cross-product / --no-cross-product</code>: By default the undeployment will process tables from the same product (valid with product-name). This flag allows to undeploy tables from different products.  [default: no-cross-product]</li> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID to use as default.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-prepare","title":"<code>pipeline prepare</code>","text":"<p>Execute the content of the sql file, line by line as separate Flink statement. It is used to alter table. For deployment by adding the necessary comments and metadata.</p> <p>Usage:</p> <pre><code>$ pipeline prepare [OPTIONS] SQL_FILE_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>SQL_FILE_NAME</code>: The sql file to prepare tables from.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--compute-pool-id TEXT</code>: Flink compute pool ID to use as default.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"command/#pipeline-analyze-pool-usage","title":"<code>pipeline analyze-pool-usage</code>","text":"<p>Analyze compute pool usage and assess statement consolidation opportunities.</p> <p>This command will: - Analyze current usage across compute pools (optionally filtered by product or directory) - Identify running statements in each pool - Assess opportunities for statement consolidation - Generate optimization recommendations using simple heuristics</p> <p>Examples:     # Analyze all pools     shift-left pipeline analyze-pool-usage</p> <pre><code># Analyze for specific product\nshift-left pipeline analyze-pool-usage --product-name saleops\n\n# Analyze for specific directory\nshift-left pipeline analyze-pool-usage --directory /path/to/facts/saleops\n\n# Combine product and directory filters\nshift-left pipeline analyze-pool-usage --product-name saleops --directory /path/to/facts\n</code></pre> <p>Usage:</p> <pre><code>$ pipeline analyze-pool-usage [OPTIONS] [INVENTORY_PATH]\n</code></pre> <p>Arguments:</p> <ul> <li><code>[INVENTORY_PATH]</code>: Pipeline path, if not provided will use the $PIPELINES environment variable.  [env var: PIPELINES]</li> </ul> <p>Options:</p> <ul> <li><code>-p, --product-name TEXT</code>: Analyze pool usage for a specific product only</li> <li><code>-d, --directory TEXT</code>: Analyze pool usage for pipelines in a specific directory</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"compute_pool_optimizer/","title":"Compute Pool Usage Analyzer","text":""},{"location":"compute_pool_optimizer/#overview","title":"Overview","text":"<p>The Compute Pool Usage Analyzer is a new feature designed to analyze current compute pool usage across your Flink environment and assess opportunities for statement consolidation. This tool can help optimize resource allocation and reduce costs by identifying underutilized pools and providing consolidation recommendations.</p>"},{"location":"compute_pool_optimizer/#features","title":"Features","text":""},{"location":"compute_pool_optimizer/#comprehensive-analysis","title":"\ud83d\udd0d Comprehensive Analysis","text":"<ul> <li>Pool Usage Assessment: Analyzes CFU usage across all compute pools</li> <li>Statement Discovery: Identifies all running statements and their pool assignments</li> <li>Efficiency Metrics: Calculates statements-per-CFU ratios and efficiency scores</li> <li>Scoped Analysis: Filter analysis by product or directory for targeted optimization</li> </ul>"},{"location":"compute_pool_optimizer/#smart-consolidation-heuristics","title":"\ud83c\udfaf Smart Consolidation Heuristics","text":"<ul> <li>Product-based Grouping: Groups statements from the same data product</li> <li>Resource-based Grouping: Identifies underutilized pools for consolidation</li> <li>Efficiency-based Grouping: Finds pools with single statements that could be combined</li> </ul>"},{"location":"compute_pool_optimizer/#detailed-reporting","title":"\ud83d\udcca Detailed Reporting","text":"<ul> <li>Usage Statistics: Per-pool CFU usage and statement counts</li> <li>Consolidation Recommendations: Specific suggestions for optimization</li> <li>Cost Savings Estimates: Potential CFU savings from consolidation</li> </ul>"},{"location":"compute_pool_optimizer/#usage","title":"Usage","text":""},{"location":"compute_pool_optimizer/#command-line-interface","title":"Command Line Interface","text":"<p>The analyzer supports flexible filtering options to focus analysis on specific areas:</p> <pre><code># Analyze all pools (global view)\nshift-left pipeline analyze-pool-usage /path/to/pipelines\n\n# Analyze for specific product (recommended for focused optimization)\nshift-left pipeline analyze-pool-usage --product-name saleops\n\n# Analyze for specific directory \nshift-left pipeline analyze-pool-usage --directory /path/to/facts/saleops\n\n# Combine product and directory filters for precision targeting\nshift-left pipeline analyze-pool-usage --product-name saleops --directory /path/to/facts\n\n# Short form options\nshift-left pipeline analyze-pool-usage -p saleops -d /path/to/facts\n</code></pre>"},{"location":"compute_pool_optimizer/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from shift_left.core.compute_pool_usage_analyzer import ComputePoolUsageAnalyzer\n\n# Create analyzer instance\nanalyzer = ComputePoolUsageAnalyzer()\n\n# Global analysis\nreport = analyzer.analyze_pool_usage(inventory_path=\"/path/to/pipelines\")\n\n# Product-specific analysis\nreport = analyzer.analyze_pool_usage(\n    inventory_path=\"/path/to/pipelines\",\n    product_name=\"saleops\"\n)\n\n# Directory-specific analysis  \nreport = analyzer.analyze_pool_usage(\n    inventory_path=\"/path/to/pipelines\",\n    directory=\"/path/to/facts/saleops\"\n)\n\n# Combined filtering\nreport = analyzer.analyze_pool_usage(\n    inventory_path=\"/path/to/pipelines\",\n    product_name=\"saleops\", \n    directory=\"/path/to/facts\"\n)\n\n# Print summary\nsummary = analyzer.print_analysis_summary(report)\nprint(summary)\n\n# Access detailed data\nfor pool in report.pool_stats:\n    print(f\"Pool {pool.pool_name}: {pool.usage_percentage:.1f}% used, {pool.statement_count} statements\")\n\nfor rec in report.recommendations:\n    print(f\"Recommendation: {rec.reason}\")\n    print(f\"Potential savings: {rec.estimated_cfu_savings} CFUs\")\n</code></pre>"},{"location":"compute_pool_optimizer/#analysis-output","title":"Analysis Output","text":""},{"location":"compute_pool_optimizer/#console-summary","title":"Console Summary","text":"<p>The analyzer provides a comprehensive console summary including:</p> <pre><code>============================================================\nCOMPUTE POOL USAGE ANALYSIS SUMMARY  \n============================================================\nAnalysis Date: 2024-12-19 10:30:15\nEnvironment: env-abc123\nAnalysis Scope: Product: saleops, Directory: /path/to/facts\n\nOVERALL STATISTICS:\n  Total Pools: 3 (filtered from 8 total pools)\n  Total Statements: 7 (filtered from 25 total statements)\n  Total CFU Used: 15\n  Total CFU Capacity: 30\n  Overall Utilization: 50.0%\n  Overall Efficiency: 0.47 statements/CFU\n\nPOOL USAGE DETAILS:\n  saleops-facts-pool (pool-123):\n    Usage: 8/15 CFU (53.3%)\n    Statements: 4\n    Efficiency: 0.50 statements/CFU\n\n  saleops-staging-pool (pool-456):\n    Usage: 7/15 CFU (46.7%)\n    Statements: 3\n    Efficiency: 0.43 statements/CFU\n\nCONSOLIDATION RECOMMENDATIONS:\n  consolidate_product_saleops:\n    Complexity: LOW\n    Estimated CFU Savings: 5\n    Statements to Move: 3\n    Reason: Product saleops statements scattered across 2 pools can be consolidated\n============================================================\n</code></pre>"},{"location":"compute_pool_optimizer/#detailed-json-report","title":"Detailed JSON Report","text":"<p>A detailed JSON report is saved with complete analysis data including scope information:</p> <pre><code>{\n  \"created_at\": \"2024-12-19T10:30:15\",\n  \"environment_id\": \"env-abc123\", \n  \"analysis_scope\": {\n    \"product_name\": \"saleops\",\n    \"directory\": \"/path/to/facts\"\n  },\n  \"total_pools\": 3,\n  \"total_statements\": 7,\n  \"pool_stats\": [...],\n  \"statement_groups\": [...],\n  \"recommendations\": [...]\n}\n</code></pre>"},{"location":"compute_pool_optimizer/#analysis-scoping-strategy","title":"Analysis Scoping Strategy","text":""},{"location":"compute_pool_optimizer/#recommended-analysis-approach","title":"\ud83c\udfaf Recommended Analysis Approach","text":"<p>1. Product-Level Analysis (Most Common) <pre><code>shift-left pipeline analyze-pool-usage --product-name saleops\n</code></pre> - Best for: Day-to-day optimization decisions - Benefits: Clear ownership boundaries, manageable scope - Use case: \"How can we optimize the saleops product pools?\"</p> <p>2. Directory-Level Analysis <pre><code>shift-left pipeline analyze-pool-usage --directory /path/to/facts\n</code></pre> - Best for: Layer-specific optimization (facts, dimensions, sources) - Benefits: Focuses on similar workload patterns - Use case: \"Are our fact tables efficiently distributed across pools?\"</p> <p>3. Combined Product + Directory <pre><code>shift-left pipeline analyze-pool-usage --product-name saleops --directory /path/to/facts\n</code></pre> - Best for: Precision targeting of specific areas - Benefits: Maximum focus, actionable recommendations - Use case: \"Optimize just the saleops fact tables\"</p> <p>4. Global Analysis <pre><code>shift-left pipeline analyze-pool-usage\n</code></pre> - Best for: Strategic overview, capacity planning - Benefits: Complete picture, cross-product insights - Use case: \"What's our overall pool efficiency across all products?\"</p>"},{"location":"compute_pool_optimizer/#consolidation-heuristics","title":"Consolidation Heuristics","text":""},{"location":"compute_pool_optimizer/#1-product-based-grouping","title":"1. Product-based Grouping","text":"<ul> <li>Groups statements belonging to the same data product</li> <li>Enhanced with scoping: When analyzing by product, focuses on cross-pool consolidation within that product</li> <li>Recommends consolidation to improve resource utilization</li> </ul>"},{"location":"compute_pool_optimizer/#2-resource-based-grouping","title":"2. Resource-based Grouping","text":"<ul> <li>Identifies pools with low statement density (&lt; 0.5 statements per CFU)</li> <li>Enhanced with scoping: Only considers pools within the filtered scope</li> <li>Focuses on improving overall resource efficiency</li> </ul>"},{"location":"compute_pool_optimizer/#3-efficiency-based-grouping","title":"3. Efficiency-based Grouping","text":"<ul> <li>Finds pools running only single statements</li> <li>Enhanced with scoping: Prioritizes single-statement pools within the analysis scope</li> <li>Calculates consolidation potential for better CFU utilization</li> </ul>"},{"location":"compute_pool_optimizer/#recommendations","title":"Recommendations","text":"<p>The analyzer generates specific recommendations with:</p> <ul> <li>Migration Complexity: LOW/MEDIUM/HIGH based on number of statements</li> <li>CFU Savings Estimates: Potential resource reduction within scope</li> <li>Target Pool Suggestions: Best pools for consolidation</li> <li>New Pool Recommendations: When creating new pools is more efficient</li> </ul>"},{"location":"compute_pool_optimizer/#integration-points","title":"Integration Points","text":""},{"location":"compute_pool_optimizer/#existing-architecture-integration","title":"Existing Architecture Integration","text":"<ul> <li>Uses existing <code>compute_pool_mgr</code> for pool data</li> <li>Leverages <code>statement_mgr</code> for statement information  </li> <li>Integrates with pipeline inventory system for product/directory filtering</li> <li>Follows established error handling patterns</li> </ul>"},{"location":"compute_pool_optimizer/#cli-integration","title":"CLI Integration","text":"<ul> <li>Added to <code>pipeline</code> command group with intuitive options</li> <li>Follows existing CLI patterns and styling</li> <li>Provides both summary and detailed output options</li> </ul>"},{"location":"compute_pool_optimizer/#benefits","title":"Benefits","text":""},{"location":"compute_pool_optimizer/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Focused Savings: Target specific products or directories for immediate impact</li> <li>Reduced CFU Usage: Consolidate underutilized pools within scope</li> <li>Improved Efficiency: Better statements-per-CFU ratios where it matters most</li> <li>Strategic Planning: Different scopes for different optimization goals</li> </ul>"},{"location":"compute_pool_optimizer/#operational-benefits","title":"Operational Benefits","text":"<ul> <li>Incremental Approach: Start with one product, expand to others</li> <li>Team Alignment: Product-based analysis aligns with team boundaries</li> <li>Reduced Risk: Smaller scope reduces migration complexity</li> <li>Better Utilization: More efficient resource usage in target areas</li> </ul>"},{"location":"compute_pool_optimizer/#example-workflows","title":"Example Workflows","text":""},{"location":"compute_pool_optimizer/#product-team-workflow","title":"Product Team Workflow","text":"<pre><code># Weekly optimization check for saleops team\nshift-left pipeline analyze-pool-usage --product-name saleops\n\n# Focus on facts if issues found\nshift-left pipeline analyze-pool-usage --product-name saleops --directory /path/to/facts\n</code></pre>"},{"location":"compute_pool_optimizer/#platform-team-workflow","title":"Platform Team Workflow","text":"<pre><code># Monthly strategic review\nshift-left pipeline analyze-pool-usage\n\n# Deep dive into underutilized areas\nshift-left pipeline analyze-pool-usage --directory /path/to/sources\n</code></pre>"},{"location":"compute_pool_optimizer/#cost-optimization-workflow","title":"Cost Optimization Workflow","text":"<pre><code># Identify biggest opportunities by product\nfor product in saleops marketing analytics; do\n  shift-left pipeline analyze-pool-usage --product-name $product\ndone\n</code></pre>"},{"location":"compute_pool_optimizer/#future-enhancements","title":"Future Enhancements","text":"<p>While the current implementation uses simple heuristics with powerful scoping, it provides a foundation for more sophisticated optimization approaches:</p> <ol> <li>O/R Problem Integration: Full operations research optimization within scoped boundaries</li> <li>Machine Learning: Pattern recognition for usage optimization per product/directory</li> <li>Cost Modeling: Detailed cost analysis and projections with scope awareness  </li> <li>Automated Migration: Seamless statement movement within filtered scope</li> <li>Capacity Planning: Predictive resource requirement modeling per product</li> <li>Cross-Product Analysis: Identify shared resources and consolidation opportunities across products</li> </ol>"},{"location":"compute_pool_optimizer/#technical-details","title":"Technical Details","text":""},{"location":"compute_pool_optimizer/#models","title":"Models","text":"<ul> <li><code>PoolUsageStats</code>: Per-pool usage statistics (filtered by scope)</li> <li><code>StatementGroup</code>: Groups of statements for consolidation (within scope)</li> <li><code>ConsolidationRecommendation</code>: Specific optimization suggestions (scope-aware)</li> <li><code>PoolAnalysisReport</code>: Complete analysis results with scope metadata</li> </ul>"},{"location":"compute_pool_optimizer/#filtering-logic","title":"Filtering Logic","text":"<ul> <li>Product Filtering: Uses pipeline inventory to map statements to products</li> <li>Directory Filtering: Uses inventory table folder paths for directory matching</li> <li>Combined Filtering: Applies both filters for precision targeting</li> <li>Fallback Behavior: Without inventory, performs global analysis</li> </ul>"},{"location":"compute_pool_optimizer/#dependencies","title":"Dependencies","text":"<ul> <li>Pydantic models for data validation</li> <li>Existing compute pool and statement management systems</li> <li>Pipeline inventory integration for product/directory context</li> <li>FlinkTableReference model for metadata extraction</li> </ul> <p>This enhanced feature provides immediate value for targeted resource optimization while establishing the foundation for more advanced, scope-aware optimization capabilities in the future.</p>"},{"location":"compute_pool_optimizer/#quick-start-examples","title":"Quick Start Examples","text":"<pre><code># Start with your main product\nshift-left pipeline analyze-pool-usage --product-name your-product\n\n# If recommendations found, drill down to specific layers\nshift-left pipeline analyze-pool-usage --product-name your-product --directory facts\n\n# For platform teams - strategic overview\nshift-left pipeline analyze-pool-usage\n\n# Focus on efficiency issues\nshift-left pipeline analyze-pool-usage --directory sources  # Often has single-statement pools\n</code></pre>"},{"location":"contributing/","title":"Contribute to this repository","text":"<p>This chapter addresses how to support the development of this open source project.</p> <p>Anyone can contribute to this repository and associated projects.</p> <p>There are multiple ways to contribute: report bugs and suggest improvements, improve the documentation, and contribute code.</p>"},{"location":"contributing/#bug-reports-documentation-changes-and-feature-requests","title":"Bug reports, documentation changes, and feature requests","text":"<p>If you would like to contribute to the project in the form of encountered bug reports, necessary documentation changes, or new feature requests, this can be done through the use of the repository's Issues list.</p> <p>Before opening a new issue, please check the existing list to make sure a similar or duplicate item does not already exist.  When you create your issues, please be as explicit as possible and be sure to include the following:</p> <ul> <li> <p>Bug reports</p> <ul> <li>Specific project version</li> <li>Deployment environment</li> <li>A minimal, but complete, setup of steps to recreate the problem</li> </ul> </li> <li> <p>Documentation changes</p> <ul> <li>URL to existing incorrect or incomplete documentation (either in the project's GitHub repo or external product documentation)</li> <li>Updates required to correct current inconsistency</li> <li>If possible, a link to a project fork, sample, or workflow to expose the gap in documentation.</li> </ul> </li> <li> <p>Feature requests</p> <ul> <li>Complete description of project feature request, including but not limited to, components of the existing project that are impacted, as well as additional components that may need to be created.</li> <li>A minimal, but complete, setup of steps to recreate environment necessary to identify the new feature's current gap.</li> </ul> </li> </ul> <p>The more explicit and thorough you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be.  When creating the GitHub issue for your bug report, documentation change, or feature request, be sure to add as many relevant labels as necessary (that are defined for that specific project).  These will vary by project, but will be helpful to the maintainers in quickly triaging your new GitHub issues.</p>"},{"location":"contributing/#code-contributions","title":"Code contributions","text":"<p>We really value contributions, and to maximize the impact of code contributions, we request that any contributions follow the guidelines below.  If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only.  These are few projects that help on-board new contributors to the overall process.</p>"},{"location":"contributing/#coding-and-pull-requests-best-practices","title":"Coding and Pull Requests best practices","text":"<ul> <li> <p>Please ensure you follow the coding standard and code formatting used throughout the existing code base.</p> <ul> <li>This may vary project by project, but any specific diversion from normal language standards will be explicitly noted.</li> </ul> </li> <li> <p>One feature / bug fix / documentation update per pull request</p> <ul> <li>Always pull the latest changes from upstream and rebase before creating any pull request.  </li> <li>New pull requests should be created against the <code>integration</code> branch of the repository, if available.</li> <li>This ensures new code is included in full-stack integration tests before being merged into the <code>main</code> branch</li> </ul> </li> <li> <p>All new features must be accompanied by associated tests.</p> <ul> <li>Make sure all tests pass locally before submitting a pull request.</li> <li>Include tests with every feature enhancement, improve tests with every bug fix</li> </ul> </li> </ul>"},{"location":"contributing/#github-and-git-flow","title":"Github and git flow","text":"<p>The following core principles for the management of this code is using the <code>gitflow</code> process with separate <code>main</code> and <code>develop</code> branches for a structured release process. </p> <ul> <li>main Branch: This branch always reflects the production-ready, stable code. Only thoroughly tested and finalized code is merged into <code>main</code>. Commits are tagged in <code>main</code> with version numbers for easy tracking of releases.</li> <li>develop Branch: This branch serves as the integration point for all new features and ongoing development. Feature branches are created from <code>develop</code> and merged back into it after completion.</li> <li>Creating a Release Branch: When a set of features in develop is deemed ready for release, a new release branch is created from <code>develop</code>. This branch allows for final testing, bug fixes, and release-specific tasks without interrupting the ongoing development work in develop.</li> <li>Finalizing the Release: Only bug fixes and necessary adjustments are made on the <code>release</code> branch. New feature development is strictly avoided.</li> <li> <p>Merging and Tagging: Once the release branch is stable and ready for deployment, it's merged into two places:</p> <ul> <li><code>main</code>: The release branch is merged into main, effectively updating the production-ready code with the new release.</li> <li><code>develop</code>: The release branch is also merged back into develop to ensure that any bug fixes or changes made during the release preparation are incorporated into the ongoing development work.</li> </ul> </li> <li> <p>Tagging: After merging into main, the merge commit is tagged with a version number (e.g., v1.0.0) to mark the specific release point in the repository's history.</p> </li> <li>Cleanup: After the release is finalized and merged, the release branch can be safely deleted</li> </ul>"},{"location":"contributing/#git-commands-for-the-different-development-tasks","title":"git commands for the different development tasks","text":"<ol> <li> <p>Create a <code>feature</code> branch from  the <code>develop</code> branch     <pre><code>git checkout -b  feature_a develop\n</code></pre></p> </li> <li> <p>Do your work:</p> <ul> <li>Write your code</li> <li>Write your unit tests and potentially your integration tests</li> <li>Pass your tests locally</li> <li>Commit your intermediate changes as you go and as appropriate to your <code>feature</code> branch</li> <li>Repeat until satisfied</li> </ul> </li> <li> <p>Create a pull request against the same targeted upstream <code>develop</code> branch.</p> <p>Creating a pull request</p> </li> </ol> Forked repository special treatment <p>If you forked the main repository, once the pull request has been reviewed, accepted and merged into the <code>develop</code> branch, you should synchronize your remote and local forked github repository <code>main</code> branch with the upstream main branch. To do so:</p> <ul> <li> <p>Pull to your local forked repository the latest changes upstream (that is, the pull request).     <pre><code>git pull upstream main\n</code></pre></p> </li> <li> <p>Push those latest upstream changes pulled locally to your remote forked repository.     <pre><code>git push origin main\n</code></pre></p> </li> </ul>"},{"location":"contributing/#release-management","title":"Release management","text":"<p>The script (<code>build_and_release.sh</code>) automates the CLI build and release branch creation process following the following instructions.</p> <ol> <li>Creates release branch: When code is ready, create a new release branch off the <code>develop</code> branch. Run all tests and fixes on this release branch until ready for release.from <code>develop</code> (e.g., <code>v0.1.34</code>)</li> <li>Updates version numbers in:</li> <li><code>src/shift_left/shift_left/cli.py</code></li> <li><code>src/shift_left/pyproject.toml</code></li> <li>Builds the wheel package using <code>uv build .</code></li> <li>Cleans old wheels (keeps last 10 wheel and tar.gz files)</li> <li>Updates CHANGELOG.md with recent commits from <code>main..develop</code></li> <li>Commits all changes with descriptive commit message to the release branch</li> <li>Validate non-regression tests</li> <li>Merge to main</li> <li>Tag the release</li> <li>Merge back to develop</li> <li>Create GitHub release referencing the new tag</li> <li>Delete release branch</li> </ol>"},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>uv package manager installed (installation guide)</li> <li>Git repository with <code>main</code> and <code>develop</code> branches</li> <li>Current working directory should be the <code>shift_left_utils</code> root directory</li> </ul>"},{"location":"contributing/#usage","title":"Usage","text":"<pre><code>./build_and_release.sh &lt;version&gt;\n# Example\n./build_and_release.sh 0.1.34\n</code></pre>"},{"location":"contributing/#what-you-need-to-do-manually-after-running-the-script","title":"What You Need to Do Manually After Running the Script","text":"<p>The script will provide you with the exact commands to run next:</p> <ol> <li>Test the release branch and make any necessary fixes (see the runRegressionTest.sh in src/shift_left)</li> <li>Merge to main: <pre><code>git checkout main\ngit merge --no-ff v0.1.34\ngit push\n</code></pre></li> <li>Tag the release: <pre><code>git tag -a 0.1.34 -m \"Release version 0.1.34\"\ngit push origin 0.1.34\n</code></pre></li> <li>Merge back to develop: <pre><code>git checkout develop\ngit merge --no-ff v0.1.34\ngit push\n</code></pre></li> <li>Create GitHub release referencing the new tag, in the git web page, copy, paste the 0.1.33 section of the changelog to the release note.</li> <li>Delete release branch: <pre><code>git branch -d v0.1.34\n</code></pre></li> </ol> <p>Files Modified by the Script - <code>src/shift_left/shift_left/core/utils/app_config.py</code> - Updates <code>__version__</code> variable - <code>src/shift_left/pyproject.toml</code> - Updates project version - <code>CHANGELOG.md</code> - Adds new version entry with recent commits - <code>src/shift_left/dist/</code> - Builds new wheel and cleans old ones</p>"},{"location":"contributing/#environment-set-up-for-developers","title":"Environment set up for developers","text":"<p>We are using uv as a new Python package manager. See uv installation documentation then follow the next steps to set your environment for development:</p> <ul> <li>Fork the <code>https://github.com/jbcodeforce/shift_left_utils</code> repo in your github account.</li> <li>Clone your repo to your local computer.</li> <li>Add the upstream repository: see guide for step 1-3 here: forking a repo</li> <li>Verify you have set up the pre-requisited</li> <li> <p>Install uv for Python package management     <pre><code># Install uv if not already installed\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n# To update existing version\nuv self update\n</code></pre></p> </li> <li> <p>Python Environment: Ensure Python 3.12+ and create a virtual environment    <pre><code>uv venv --python 3.12.0\nsource .venv/bin/activate  # On Windows WSL: .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install Dependencies: Use <code>uv</code> package manager (recommended)    <pre><code>cd src/shift_left\n# Install project dependencies\nuv sync\n</code></pre></p> </li> <li> <p>Define a config.yaml file to keep the important parameters of the CLI.      <pre><code>cp src/shift_left/shift_left/core/templates/config_tmpl.yaml ./config.yaml\n</code></pre></p> </li> <li> <p>Get the credentials for the Confluent Cloud Kafka cluster and Flink compute pool, modify the config.yaml file</p> </li> </ul> the structure of the config.yaml <p>This file is to keep configuration to access Confluent Cloud resources. It is grouped in 5 parts: </p> <ul> <li>kafka to access Kafka cluster</li> <li>confluent_cloud to manage resources at the organization level</li> <li>flink: to manage Statement and compute pools</li> <li>app: to define CLI parameters</li> <li>registry: to access schema registry (not used)</li> </ul> <p>The Flink, and Confluent Cloud api keys and secrets are different.</p> <ul> <li>Set the CONFIG_FILE environment variable to point to the config.yaml file. The following is used for integration tests.</li> </ul> <pre><code>export CONFIG_FILE=shift_left_utils/src/shift_left/tests/config.yaml\n</code></pre> <ul> <li> <p>Install shift_left Tool:    <pre><code># under src/shift_left\nls -al dist/\n# select the last version, for example:\nuv tool install dist/shift_left-0.1.28-py3-none-any.whl\n# Verify installation\nshift_left --help\nshift_left version\n</code></pre></p> <p>You can also use <code>pip</code> if you have an existing Python environment: <pre><code>pip install src/shift_left/dist/shift_left-0.1.28-py3-none-any.whl\n</code></pre></p> </li> <li> <p>Make sure you are logged into Confluent Cloud and have defined at least one compute pool.</p> </li> </ul>"},{"location":"contributing/#development-activities","title":"Development activities","text":"<p>See the code explanation and design approach chapter.</p>"},{"location":"contributing/#install-dependencies-and-the-cli-as-uv-tool","title":"Install dependencies and the cli as uv tool","text":"<ul> <li> <p>Install dependencies and tool for iterative development, under the <code>src/shift_left</code> folder (the one with the <code>uv.lock</code>  and <code>pyproject.toml</code> files)     <pre><code>uv tool list\nuv tool install .\n</code></pre></p> </li> <li> <p>Verify it is installed locally (version will differ)     <pre><code>uv tool list \nshift-left v0.1.28\n</code></pre></p> </li> <li> <p>Uninstall:     <pre><code>uv tool list\nuv tool uninstall shift_left\n</code></pre></p> </li> </ul> <p>(The version number is specified in <code>pyproject.toml</code> file)</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>This section addresses the things to know for running tests for the CLI.</p> Test projects under data <p>The <code>tests/data</code> folder includes two test projects:</p> <p>1/ flink-project and 2/ dbt-project</p> <p>The <code>dbt-project</code> include a dbt project to migrate source SQL from. The flink-project is the outcome of the migration with data engineering tuning the SQL and deploy them with Make.</p> <p>Those projects are used for unit tests and even to explain how to use the CLI by end user.</p> <ul> <li>Be sure to set the PIPELINES environment variable to point to the <code>tests/data/flink-project/pipelines</code> (be sure to be under the <code>src/shift_left</code> folde): </li> </ul> <pre><code>export PIPELINES=$(pwd)/tests/flink-project/pipelines\n</code></pre> <ul> <li>Running the cli from python code, be sure to be under the <code>src/shift_left</code> folder and use:</li> </ul> <pre><code> uv run shift_left\n</code></pre> <ul> <li>It is also possible to test the CLI with python:</li> </ul> <pre><code>uv run shift_left pipeline build-metadata $PIPELINES/facts/p1/fct_order $PIPELINES\n</code></pre> <p>To avoid redundant tests, the tests are grouped in three sets:</p> <ol> <li>The CLI modules tests: will test the CLI functions and the delegate functions from core</li> <li>The core modules tests: to test functions not addressed by CLI tests</li> <li>The utils modules tests: to test function not addressed by CLI or core tests</li> </ol> <p>The test should supports debugging and test in Terminal, so all file accesses are relative to the test class.</p> <p>Tests are executed in a virtual environment with python 3 and pytest.</p> <pre><code>uv run pytest -s tests/it/core/test_table_mgr.py\nuv run pytest -s tests/ut/core/test_project_mgr.py\nuv run pytest -s tests/ut/core/test_pipeline_mgr.py\n</code></pre> <ul> <li>Test the CLIs</li> </ul> <pre><code>uv run pytest -s tests/cli/test_project_cli.py\n</code></pre> <p>To avoid comflict between test execution, each test gets its own copy of data into a temporary folder defined in the pytest fixture in <code>ut/core/conftest.py</code>. Tests can run simultaneously without conflicts. The temporary directories are automatically removed after tests.</p>"},{"location":"contributing/#debug-core-functions","title":"Debug core functions","text":"<p>Use the following settings for vscode based IDE</p> <pre><code>    {\n        \"name\": \"Python Debugger: current file\",\n        \"type\": \"debugpy\",\n        \"request\": \"launch\",\n        \"program\": \"${file}\",\n        \"console\": \"integratedTerminal\",\n        \"cwd\": \"${fileDirname}\",\n        \"env\": {\n            \"CONFIG_FILE\": \"${fileWorkspaceFolder}/config.yaml\"\n            },\n    },\n</code></pre>"},{"location":"contributing/#build","title":"Build","text":"<ul> <li> <p>To install the tool locally with uv and to support hot code update      <pre><code>uv tool install . -e  \n</code></pre></p> </li> <li> <p>Rebuild the CLI as a wheel packaging:      <pre><code>uv build .\n</code></pre>     under the src/shift_left project.</p> </li> <li> <p>Deploy the CLI from the wheel: 1/ first the last wheel number and then 2/ execute the commands like:     <pre><code>uv tool uninstall shift_left\nuv tool install shift_left@dist/shift_left-0.1.46-py3-none-any.whl\n</code></pre></p> </li> <li> <p>To Build the docs/command.md documentation from the code run:     <pre><code># under the shift_left_utils/src/shift_left folder\n./updateDoc.sh\n# same as\nuv run typer shift_left/cli.py utils docs --output ../../docs/command.md\n</code></pre></p> </li> <li> <p>Recompile a requirements.txt for pip users:     <pre><code>uv pip compile pyproject.toml -o requirements.txt\n</code></pre></p> </li> </ul>"},{"location":"contributing/#publish-to-pypi","title":"Publish to PyPI","text":"<p>From <code>src/shift_left</code>, build then publish only the current version to avoid uploading older wheels in <code>dist/</code>:</p> <pre><code>cd src/shift_left\nuv build .\nVERSION=$(grep '^version' pyproject.toml | cut -d'\"' -f2)\nUV_PUBLISH_TOKEN=pypi-xxxxxxxx uv publish dist/shift_left-${VERSION}*\n</code></pre>"},{"location":"dev_process/","title":"Flink Project Development Process","text":"Version <p>Created February 2025 - </p> <p>This chapter details a direct migration path from batch ETL jobs processing fact tables to source tables, into Flink SQL pipelines. Prior project setup, as outlined in the provided note, is required.</p> <p>Source project can be standard SQL project, Data Build Tool project, or even ksql project. The source project folder is referenced with the $SRC_FOLDER environment variable. </p> <p>The migration method begins with a fact table. At a high level, the development flow is illustrated in the following diagram:</p> <p></p> <ol> <li>Start with a Fact or Dimension table. Process the fact table through the entire hierarchy up to the sources, and save the outcomes to an intermediate staging folder, as manual work will be required. The command to do so is shift_left table migrate</li> <li>From the generated code, update the Fink statements for the source tables. Complete the deduplication logic in the generated DML to reflect the dbt logic applied for deduplication. The new table should use an upsert append log with a specific primary key. Since the source topic may not have the same primary key, it is crucial to perform primary key refactoring. If you do not have access to the original source topic, a DDL will be created in the tests folder for unit testing. The DDL content, primarily column definitions, can be inferred from the <code>show create table</code> command. Move the resulting folder to pipeline/sources.</li> <li>Remove unnecessary intermediate steps, specifically those involving deduplication logic, as this has already been addressed in the source DML statements. Update any other intermediate statements with specific business logic and move the resulting statements to the pipeline/intermediates folder.</li> <li>Execute any DDLs for the sink tables and intermediate tables.</li> <li>If the source topics do not contain records, use the tool to create test data only at the source table level. Ensure consistency in the data for the various join conditions.</li> <li>Execute the remaining DMLs.</li> <li>Validate the data in the output topics designated as sink tables.--- This revision improves clarity and structure while maintaining the original meaning.</li> <li>Start to manage pipelines, see dedicated session in recipes chapter</li> </ol> <p>&gt;&gt; next see recipe or shift_left command references</p>"},{"location":"perf_test/","title":"Testing Flink Solution","text":"<p>As a generic test plan for Flink project, we propose to address the following test activities:</p> <ol> <li>Unit tests</li> <li>Integration tests with production data</li> <li>Performance testing</li> </ol>"},{"location":"perf_test/#three-main-test-activities","title":"Three main test activities","text":""},{"location":"perf_test/#1-unit-testing","title":"1. Unit Testing","text":"<ul> <li> <p>SQL Query Testing: For each DML flink statement defined in the project pipelines folder, Data Engineer may define test suites with test data to validate the logic of the SQL statement. The test harness tooling is used for that, as it authorized to test individual Flink SQL query in isolation, by creating temporary input and output tables for running test suites.</p> <ul> <li>When window operators are used, special timing concerns need to be addressed</li> <li>Unit test should be able to verify watermark handling and event time processing</li> </ul> </li> <li> <p>For User Define Function: The test need to support unit test of the function code, but also validate input/output data, error handling and edge cases.</p> </li> </ul>"},{"location":"perf_test/#2-integration-testing","title":"2. Integration Testing","text":"<p>As Confluent Cloud for Flink is natively integrated with Kafka, the goal of integration testings is to isolate tests from the source/ replicated tables,  down to sink to data lakehouse.</p> <ul> <li>Test end-to-end data flow from source to sink</li> <li>Verify message serialization/deserialization</li> <li> <p>Validate schema evolution handling</p> </li> <li> <p>State Management is relevant for CP Flink or Open-source Flink. For Confluent cloud the state management is transparent for the users. The classical items are:</p> <ul> <li>Test state backend operations</li> <li>Verify checkpointing and savepoint functionality</li> <li>Test state recovery scenarios</li> </ul> </li> </ul>"},{"location":"perf_test/#3-performance-testing","title":"3. Performance Testing","text":"<p>The goal are: </p> <ol> <li>To measure how it takes to deploy Flink Statements for a table, a pipeline, a data product</li> <li>To assess how long it takes for a data created at the source is available for end consumption. </li> <li>To assess how long it takes to process the lag of data in the first topics representing raw data</li> </ol> <p>As an example, we will take a classical shift left processing architecture with a timing representation:</p> <ul> <li> <p>Throughput Testing</p> <ul> <li>Measure maximum processing rate (events/second)</li> <li>Evaluate parallel processing capabilities</li> </ul> </li> <li> <p>Latency Testing</p> <ul> <li>Measure end-to-end processing latency</li> </ul> </li> <li> <p>Scalability Testing</p> <ul> <li>Test horizontal scaling by assessing CFU increase up to the limit, control those statements to assess if we need to split them.</li> <li>Test with increasing data volumes</li> </ul> </li> <li> <p>Scalability Testing</p> <ul> <li>Test horizontal scaling (adding/removing task managers)</li> <li>Evaluate job manager performance</li> <li>Test with increasing data volumes</li> <li>Monitor resource utilization</li> </ul> </li> </ul> <p>Metrics to use:</p> <ul> <li>Get time stamp when the statement is created</li> <li>Time stamp when deploy is started and finished</li> <li>Time stamp when the LAG is processed</li> <li>Number of message per second processed per Flink statement</li> <li>Time stamp from source to destination for a given transaction</li> </ul>"},{"location":"perf_test/#4-pipeline-management-testing","title":"4. Pipeline Management Testing","text":"<ul> <li> <p>Monitoring and Alerting</p> <ul> <li>Test metric collection and reporting</li> <li>Verify alert thresholds</li> <li>Test failure detection and notification</li> <li>Validate logging mechanisms</li> </ul> </li> <li> <p>Operational Testing</p> <ul> <li>Test job cancellation and restart</li> <li>Verify savepoint creation and restoration</li> <li>Test job scaling operations</li> <li>Validate backup and recovery procedures</li> </ul> </li> </ul>"},{"location":"perf_test/#5-confluent-cloud-specific-testing","title":"5. Confluent Cloud Specific Testing","text":"<ul> <li> <p>Cloud Integration</p> <ul> <li>Test connectivity to Confluent Cloud</li> <li>Verify authentication and authorization</li> <li>Test network security and encryption</li> <li>Validate cloud resource management</li> </ul> </li> <li> <p>Cost Optimization</p> <ul> <li>Monitor resource utilization</li> <li>Test auto-scaling configurations</li> <li>Evaluate cost-effective configurations</li> <li>Test resource cleanup procedures</li> </ul> </li> </ul>"},{"location":"perf_test/#6-security-testing","title":"6. Security Testing","text":"<ul> <li> <p>Authentication and Authorization</p> <ul> <li>Test access control mechanisms</li> <li>Verify encryption in transit and at rest</li> <li>Test security configurations</li> <li>Validate audit logging</li> </ul> </li> </ul>"},{"location":"perf_test/#7-disaster-recovery-testing","title":"7. Disaster Recovery Testing","text":"<ul> <li> <p>Failure Scenarios</p> <ul> <li>Test node failures</li> <li>Test network partition scenarios</li> <li>Test data center failures</li> <li>Validate recovery procedures</li> </ul> </li> </ul>"},{"location":"perf_test/#test-environment-requirements","title":"Test Environment Requirements","text":"<ul> <li> <p>Development Environment</p> </li> <li> <p>Staging Environment</p> <ul> <li>Dedicated Confluent Cloud environment</li> <li>Production-like configuration</li> <li>Monitoring and logging setup</li> </ul> </li> <li> <p>Production Environment</p> <ul> <li>Production Confluent Cloud setup</li> <li>Production-grade monitoring</li> <li>Backup and recovery systems</li> </ul> </li> </ul>"},{"location":"perf_test/#test-tools-and-frameworks","title":"Test Tools and Frameworks","text":"<ul> <li> <p>Testing Tools</p> <ul> <li>Flink Test Harness</li> <li>Prometheus for metrics</li> <li>Grafana for visualization</li> </ul> </li> <li> <p>CI/CD Integration</p> <ul> <li>Automated test execution</li> <li>Test result reporting</li> <li>Deployment automation</li> <li>Environment provisioning</li> </ul> </li> </ul>"},{"location":"perf_test/#success-criteria","title":"Success Criteria","text":"<ul> <li> <p>Performance Metrics</p> <ul> <li>Throughput: &gt; X events/second</li> <li>Latency: &lt; Y milliseconds</li> <li>Resource utilization: &lt; Z%</li> </ul> </li> <li> <p>Reliability Metrics</p> <ul> <li>Uptime: &gt; 99.9%</li> <li>Data consistency: 100%</li> <li>Recovery time: &lt; X minutes</li> </ul> </li> <li> <p>Operational Metrics</p> <ul> <li>Deployment success rate: 100%</li> <li>Rollback success rate: 100%</li> <li>Alert accuracy: &gt; 99%</li> </ul> </li> </ul>"},{"location":"pipeline_mgr/","title":"Pipeline Management","text":"Version <p>Created Mars 21- 2025 - Update 5/28/25</p> <p>The goals of this chapter is to present the requirements, design, and validation of the pipeline management tools.</p>"},{"location":"pipeline_mgr/#context","title":"Context","text":""},{"location":"pipeline_mgr/#statement-evolution","title":"Statement Evolution","text":"<p>Modifying streaming workloads over time, without serious side effects, is challenging: a workload\u2019s downstream dependency chains run perpetually and expect continuous output. Any Flink DAG code is immutable, therefore a quick deployment strategy is to do not modify Flink process flow. When a SQL statement is started, it reads the source tables from the beginning (or any specified offset) and the operators, defined in the statement, build their state. Source or sink operators use the latest schema version for key and value at the time of deployment. </p> <p>The general strategy for query evolution is to replace the existing statement and the corresponding tables it maintains with a new statement and new tables. Let take a simple example of a Flink statement joining two tables and with an output table also used by a join within a second statement:</p> Stateful Flink statement evolution <p>The streaming processing has processed all blue records and a new version needs to be applied at a time from which new records processed may have an impact.</p> <p>The typical change to consider are:</p> <ul> <li>adding a new column in a select statement</li> <li>removing a field in a select statement</li> <li>adding aggregation dimension or new joins, with full historical records reprocessing,</li> </ul> <p>For each Flink Statement deployment some questions need to be assessed:</p> <ul> <li>What should be done with the pre-existing output records? </li> <li>Does the update impact the meaning ot the output? (change the dimensionality of an aggregation)</li> <li>What happens if the new output format breaks compatibility with the old? </li> <li>How can the old and new data structures be made to work together, and what if they can't?</li> </ul> <p>Which can be summarized, which statement need to backfill for the earliest or not. </p> <p>As illustrated in the [query evolution - basic strategy section], the process is to stop Flink statements to change, create a new table _v2, and once lag is recovered, swap consumers from the old topic to the new one.</p> <p></p>"},{"location":"pipeline_mgr/#flink-statement-interdependancies","title":"Flink Statement interdependancies","text":"<p>Flink statements are inherently interdependent, consuming and joining tables produced by other statements, forming a complex pipeline. Careful deployment is crucial. The following diagram illustrates this interconnectedness for a simple example and outlines a pipeline management strategy.</p> a pipeline of Flink statements <p>This graph is generated by running a report like: <code>shift_left pipeline report fct_order --graph</code></p> Test data <p>The folder src/shift_left/tests/data includes a Flink-project with the DDLs and DMLs to support the graph illustrated above, and it is used in all the test cases.</p>"},{"location":"pipeline_mgr/#core-concepts-for-managing-flink-statement-pipelines","title":"Core concepts for managing Flink Statement pipelines","text":"<p>The recipe chapter has how-to descriptions for the specific commands to use during development and during the pipeline management by system reliability engineers. The following high level concepts are the foundations for this management:</p> <ol> <li>The git folder is the source of truth for pipeline definitions. </li> <li> <p>The table inventory, which lists all the Flink tables of a project, is used as the foundation to find basic metadata about Flink statements. It must be created with a simple command like:</p> <pre><code>shift_left table build-inventory $PIPELINES\n</code></pre> <p>The <code>inventory.json</code> is persisted in the $PIPELINES folder and committed in git. It will be extensively used by any pipeline commands. It could be updated at each PR by a CI tool.</p> </li> <li> <p>For each table <code>pipeline_definition json</code> file, includes a single level of information about the pipeline. Those files are built from the sink tables going up to the sources. During the Flink development phase, developers may use this tool to build the metadata:</p> <pre><code>shift_left pipeline build_metadata $PIPELINES/facts/p1/fct_order/sql_scripts/dml.fct_order.sql $PIPELINES\n</code></pre> <p>The created file may look like:</p> <pre><code>{\n    \"table_name\": \"fct_order\",\n    \"type\": \"fact\",\n    \"dml_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\",\n    \"ddl_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/ddl.fct_order.sql\",\n    \"path\": \"pipelines/facts/p1/fct_order\",\n    \"state_form\": \"Stateful\",\n    \"parents\": [\n        {\n            \"table_name\": \"int_table_2\",\n            \"type\": \"intermediate\",\n            \"dml_ref\": \"pipelines/intermediates/p1/int_table_2/sql-scripts/dml.int_table_2.sql\",\n            \"ddl_ref\": \"pipelines/intermediates/p1/int_table_2/sql-scripts/ddl.int_table_2.sql\",\n            \"path\": \"pipelines/intermediates/p1/int_table_2\",\n            \"state_form\": \"Stateful\",\n            \"parents\": [],\n            \"children\": [\n                {\n                \"table_name\": \"fct_order\",\n                \"type\": \"fact\",\n                \"dml_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\",\n                \"ddl_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/ddl.fct_order.sql\",\n                \"path\": \"pipelines/facts/p1/fct_order\",\n                \"state_form\": \"Stateful\",\n                \"parents\": [],\n                \"children\": []\n                }\n            ]\n        },\n        {\n            \"table_name\": \"int_table_1\",\n            \"type\": \"intermediate\",\n            \"dml_ref\": \"pipelines/intermediates/p1/int_table_1/sql-scripts/dml.int_table_1.sql\",\n            \"ddl_ref\": \"pipelines/intermediates/p1/int_table_1/sql-scripts/ddl.int_table_1.sql\",\n            \"path\": \"pipelines/intermediates/p1/int_table_1\",\n            \"state_form\": \"Stateful\",\n            \"parents\": [],\n            \"children\": [\n                {\n                \"table_name\": \"fct_order\",\n                \"type\": \"fact\",\n                \"dml_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\",\n                \"ddl_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/ddl.fct_order.sql\",\n                \"path\": \"pipelines/facts/p1/fct_order\",\n                \"state_form\": \"Stateful\",\n                \"parents\": [],\n                \"children\": []\n                }\n            ]\n        }\n    ],\n    \"children\": []\n}\n</code></pre> <p>Developers or SREs may use others command to go over all facts, dimensions or views folders, to create all the <code>pipeline_definition.json</code> from each of the dml of those tables:</p> <pre><code>shift_left pipeline build-all-metadata $PIPELINES/facts\n</code></pre> <p>Note that going while walking up a second pipeline from a new sink, may modify the pipeline_definitions.json of an existing parent table, to update the list of children with the new sink. Same for new intermediate table. The parents and children lists are in fact Sets so there is no duplicate entry if a table is used by multiple pipelines.</p> </li> <li> <p>A hierarchy view of a pipeline can be used for reporting, or by the developer to understand the complex tree:</p> <pre><code>shift_left pipeline report fct_order --json\n</code></pre> <p>The optins are <code>--json</code>, <code>--simple</code>,  <code>--yaml</code> or <code>--graph</code> can be used. </p> </li> <li> <p>Hierarchy view is used to deploy a selected table, its parents and eventually its children. </p> </li> <li> <p>For deployment they are some heuristic to follow:</p> <ol> <li>when deploying a Flink statement, for any parent (tables used for select or joins) currently not running, the tools needs to start them</li> <li>When the current intermediate table, one with children and parents, is generating events from a stateful processing, the output table needs to be recreated, the consumers who consumed from the earliest needs to be restarted. Eventually this means cascading down the children graph to restart all the impacted children.</li> <li>For a current intermediate table, if a child is stateful, then the tool needs to restart this child once the current topic / table is created. If the child is stateless then offset management needs to be done: the children is stopped, and restarted from last committed offset. </li> <li>The deployment follows a sources to sink deployment to ensure that for a given table all parents are running.</li> </ol> </li> </ol>"},{"location":"pipeline_mgr/#multi-cluster-support","title":"Multi-cluster support","text":"<p>Flink is deployed at the environment level. An environment may have multiple Kafka clusters. A schema registry is also at the environment level. The tool needs to support conflicting consistency: do not overwrite schema, separate cluster or topics within the same cluster. The following figure illustrates those concepts:</p> <p></p> <p>Therefore the following are important requirements to address:</p> <ul> <li>Tables are created by flink job and may avoid conflicting so a naming convention is needed, sepcially when a unique cluster is used for dev and staging: dev-;  <li>Flink job (dml-*) needs to follow a naming convention to easily find compute pool to statement allocation in the Console.</li> <li>Sources topics may have been created by external systems so it may also being created with naming convention, or the same source topics used by the different deployment.</li> <li>Schemas in the registry needs to support context and each created table definition uses the <code>key.avro-registry.schema-context</code>  = '.flink-stage' or '.flink-dev'  settings.</li> <li>Table name needs to follow a naming convention to separate dev and stage.</li> <li>When stage is a separate kafka cluster then the catalog and database names are used to define the scope of the created tables.</li> table name schema context src_p1_table_1 .flink-dev src_p1_table_2 .flink-dev src_p1_table_3 .flink-dev p1_fct_table_1 .flink-dev src_p1_table_1 .flink-stage src_p1_table_2 .flink-stage src_p1_table_3 .flink-stage p1_fct_table_1 .flink-stage <p>The shift_left <code>config..yaml</code> file needs to be specific per deployment target. The table illustrates the parameters specific for a given environment</p> Use case Goal Parameter to set Deploy to dev env and cluster Tables are create with prefix <code>kafka.cluster_type: dev</code> and  <code>flink.catalog_name: dev-env</code> <code>flink.database_name: dev-cluster</code> Deploy to stage same kafka cluster, same env Tables are create with prefix to separate from dev <code>kafka.cluster_type: stage</code> and  <code>flink.catalog_name: dev-env</code> <code>flink.database_name: dev-cluster</code> Deploy to stage different kafka cluster same environment Tables are create with prefix but the database name is different too. <code>kafka.cluster_type: stage</code> and   <code>flink.catalog_name: dev-env</code> <code>flink.database_name: stage-cluster</code>"},{"location":"pipeline_mgr/#different-constraints-for-pipeline-deployment","title":"Different constraints for pipeline deployment","text":""},{"location":"pipeline_mgr/#deploying-a-fact-table","title":"Deploying a fact table","text":"<p>During development, Flink SQL developers use the makefile: see this recipe to deploy statement. While preparing for staging or integration tests, it may be relevant to deploy a full pipeline from a sink table. For example SREs want to deploy the sink <code>fct_order</code> table. To make the DML running successfuly, as it joins two tables, both tables need to be created. So the tool needs to walk up the hierarchy to deploy parents, up to the source. The white colored topic and Flink statements are currently running, tables and topics have messages. Before deploying the <code>fct_order dml</code>, the tool needs to assess what are the current parents table running. If there are missing tables, the tool needs to deploy those, taking into consideration parents of parents. For example, for the <code>int_table_1</code> which is not created, the tool needs first to run the DDL <code>src_table_1</code> and any <code>DML for src_table_1</code>. (in the test the dml of the sources are just inserting records, but in real project, those DMLs may consume from an existing Kafka topic created via CDC), thne run the <code>int_table_1</code> DDL and DML, to finally deploy the <code>fct_order</code> DDL and DML. </p> Sink table deployment - with parent deployment <p>The red color highlights what is the goal of the deployment. The white represents what is stable, while the orange elements are impacted by the deployment.  </p> Step to demonstrate a sink table deployment <ul> <li>Remove any older logs with <code>rm ~/.shift_left/logs/*.log*</code></li> <li>Be sure <code>config.yaml</code> has the good parameters in particular the flink and Confluent cloud access API keys, secrets, a default compute_pool_id and the appropriate logging level.</li> <li>Defines the PIPELINES and CONFIG_FILE environment variables</li> <li>Ensure the table inventory is up to date, if not run <code>shift_left table build-inventory $PIPELINES</code></li> <li>If for any reason, the pipeline definitions for the given pipeline needs to be recreated, run: <code>shift_left pipeline build-metadata fct_order $PIPELINES</code></li> <li>Deploy the fact table: <code>shift_left pipeline deploy  --table-name fct_order</code></li> <li>Verify in the Confluent Cloud console the Flink statements running and the topics created, or run the command: <code>shift_left pipeline report-running-statements --table-name fct_order</code>.</li> </ul>"},{"location":"pipeline_mgr/#deploying-an-intermediate-table","title":"Deploying an intermediate table","text":"<p>Intermediate table deployment, will follow the same principle as above if parent in the upward hierarchy are not running, but most important, it may impact children. The behavior of the deployment will be different if the DML are stateful, for the current DML to deploy but also for the children. The red color means those elements will be recreated, and orange means they may be impacted for a re-deployment. For stateful with earliest-offset consumption, will mean the topic needs to be recreated and the downstream children recreated. For stateless, stopping, getting the offset and restarting from the saved offset will work.</p> Intermediate table deployment"},{"location":"pipeline_mgr/#deploying-source-table","title":"Deploying source table","text":"<p>For source processing, it may impact more children elements. Most of those processing are doing deduplication or transforming to upsert table with different primary keys, which means becoming stateful.</p> Source table deployment"},{"location":"pipeline_mgr/#more-complex-graph-for-testing","title":"More complex graph for testing","text":"<p>The following graph represents a more complex network of dependencies to illustrate the following patterns:</p> <ul> <li>intermediate table Z has multiple parents with their own sources. So deploy Z will mean assessing X, and Y trees</li> <li>Running every node may impact restarting any children</li> </ul> Topology used for test <p>topics are not represented between Flink Statements, only the four source topics.</p> <p>The navigation to the parents needs to follow a depth first search to get all non running parents, while restarting children should follow a breath first search.</p> <p>The approach is also to build an execution plan and then execute the plan in the order of the definition.</p>"},{"location":"pipeline_mgr/#tool-requirements","title":"Tool Requirements","text":"<p>The following list presents the requirements to implement for the shift_left deploy command:</p> <ul> <li> The expected command to deploy should be as simple as:</li> </ul> <pre><code>shift_left pipeline deploy [OPTIONS] INVENTORY_PATH\n\n**Options**:\n\n    * `--table-name TEXT`: The table name containing pipeline_definition.json.\n    * `--compute-pool-id TEXT`: Flink compute pool ID. If not provided, it will create a pool.\n    * `--dml-only / --no-dml-only`: By default the deployment will do DDL and DML, with this flag it will deploy only DML  [default: no-dml-only]\n    * `--may-start-children / --no-may-start-children`: The children deletion will be done only if they are stateful. This Flag force to drop table and recreate all (ddl, dml)  [default: no-may-start-children]\n    * `--force-sources / --no-force-sources`: When reaching table with no ancestor, this flag forces restarting running Flink statements.  [default: no-force-sources]\n    * `--dir TEXT`: The directory to deploy the pipeline from. If not provided, it will deploy the pipeline from the table name. \n</code></pre> <ul> <li> <p> Deploy dml - ddl: Given the table name, executes the dml and ddl to deploy a pipeline. If the compute pool id is present it will use it. If not, it will get the existing pool_id from the table already deployed, if none is defined it will create a new pool and assign the pool_id. A deployment may impact children statement depending of the semantic of the current DDL and the children's one.</p> </li> <li> <p> Support deploying only DML, or both DDL and DML (default)</p> </li> <li> Deploying a DDL, means dropping existing table if exists.</li> <li> Deploying a non existing sink means deploying all its parents if not already deployed, up to the sources. This will be the way to deploy a pipeline. In this case deploy first the sources, ddl and dml, except if already running as it means the current table was created by another pipeline. This is recursive.</li> <li> For a given table with children, deploy the current table, and for each children redeploy the DML, if the DML is stateful. When stateless, manage the offset and modify the DML to read from the retrieved offset.</li> <li> Support deleting a full pipeline: delete tables not used by other pipeline: the number of children is 1 or all the children are not running.</li> </ul>"},{"location":"recipes/","title":"Recipes Summary","text":"Version <ul> <li>Created January 2025.</li> <li>Update: 05/28/2025. </li> </ul> <p>This chapter details the standard activities to manage a Confluent Cloud Flink project with the <code>shift_left</code> tool when doing a ETL to real-time migration project. The recipes address new project initiative or a migration project from an existing SQL based ETL solution.</p> <p>As introduced in the context chapter the CLI groups a set of commands to manage project, tables, and Flink statements as part of pipelines.</p> <p>The audience for the recipes chapter are the Data Engineers and the SREs.</p>"},{"location":"recipes/#tool-context","title":"Tool context","text":"<p>The <code>shift_left</code> CLI may help to support different Data Engineers and SREs activities. For that, the tool will build and use a set of different components. The figures below groups what the different components are for different use cases.</p> <p>The major development activity constructs managed by the tools are:</p> <ul> <li>Flink Project, with the structure of the different folders.</li> <li>Flink Table with the structure of a table folder structure, with sql-scripts, makefile and tests.</li> <li>Flink table inventory to keep an up to date inventory of table, with the references to the ddl and dml. Tables not in the inventory do not exist.</li> <li>Flink Statement pipeline definition: metadata about a table and its parents and / or children. Sink tables have only parents, source tables have only children.</li> <li>Flink SQL statement for DDL and DML.</li> <li>For complex DML it may make sense to have dedicated test definitions.</li> </ul>"},{"location":"recipes/#data-engineer-centric-use-cases","title":"Data engineer centric use cases","text":"<p>Developers/ Data Engineers are responsible to develop the Flink SQL statements and the validation tests. </p> <p>The table folder structure, table inventory and pipeline_definitions are generated and managed by tools. No human edit is needed and even discouraged.</p> <p>See the following recipes to support the management of those elements:</p> <p> Create a Flink project  Add table folder structure</p> <p> Validate development best practices  Use the Test harness  Understanding Flink semantic</p> <p>Validate running statements</p> <p>Understand Flink statement dependencies Verify execution plan for each table, product or folder Deploy a pipeline</p>"},{"location":"recipes/#deployment-centric-use-cases-for-sres","title":"Deployment centric use cases for SREs","text":"<p>For pipeline deployment, there are very important elements that keep the deployment consistent, most of them are described above, but the execution plan is interesting tool to assess, once a pipeline is defined, how it can be deployed depending if SREs deploy from the source, the sink or an intermediate. (See pipeline managmeent section)</p> <p>The involved recipes are:</p> <p> Build Table inventory  Build dependency metadata Understand Flink statement dependencies</p> <p>Review the current execution plan from a Flink statement Deploy a Flink Statement taking into consideration its execution plan</p>"},{"location":"recipes/#migration-use-cases","title":"Migration use cases","text":"<p>This use case applies only when the source project is available and based on dbt, Spark SQL or ksqlDB. The approach is to use the <code>shift_left table migrate</code> command to use an agentic workflow, using LLM models to do the translation, syntax validation, and improvement using different AI agents.</p> <p></p> <ul> <li>See the Migrate existing SQL to Flink SQL using AI chapter for details.</li> </ul>"},{"location":"recipes/#setup","title":"Setup","text":"<p>To use the CLI, be sure to follow the setup instructions.</p>"},{"location":"recipes/#the-configyaml-file","title":"The config.yaml file","text":"<p>The <code>config.yaml</code> file is crucial to set up and is used by the tool via the CONFIG_FILE environment variable. See instructions. This file should be setup per Confluent Cloud environment. The <code>shift_left</code> tool run simple validation of this config_file during the starting phase.</p> Practices <p>Define one config.yaml per environment (dev, stage, prod) and save them in the <code>$HOME/.shift_left</code> folder.</p>"},{"location":"recipes/#project-related-tasks","title":"Project related tasks","text":""},{"location":"recipes/#create-a-flink-project-structure","title":"Create a Flink project structure","text":"<p>This activity is done when starting a new Flink project. The lead developer will jump start a project by running this command to create folder and git init.</p> <ul> <li>Get help for the shift_left project management CLI</li> </ul> <pre><code>shift_left project --help\n</code></pre> <ul> <li>To create a new project:</li> </ul> <pre><code>shift_left project init &lt;project_name&gt; &lt;project_path&gt; \n# example for a default Kimball project\nshift_left project init flink-project ../\n# For a project more focused on developing data as a product\nshift_left project init flink-project ../ --project-type data-product\n</code></pre> Output <pre><code>```sh\nmy-flink-project\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 logs\n\u251c\u2500\u2500 pipelines\n\u2502   \u251c\u2500\u2500 common.mk\n\u2502   \u2514\u2500\u2500 data_product_1\n\u2502       \u251c\u2500\u2500 dimensions\n\u2502       \u251c\u2500\u2500 facts\n\u2502       \u2502   \u2514\u2500\u2500 fct_order\n\u2502       \u2502       \u251c\u2500\u2500 Makefile\n\u2502       \u2502       \u251c\u2500\u2500 sql-scripts\n\u2502       \u2502       \u2502   \u251c\u2500\u2500 ddl.fct_order.sql\n\u2502       \u2502       \u2502   \u2514\u2500\u2500 dml.fct_order.sql\n\u2502       \u2502       \u251c\u2500\u2500 tests\n\u2502       \u2502       \u2514\u2500\u2500 tracking.md\n\u2502       \u251c\u2500\u2500 intermediates\n\u2502       \u2514\u2500\u2500 sources\n\u2514\u2500\u2500 staging\n```\n</code></pre>"},{"location":"recipes/#list-the-topics","title":"List the topics","text":"<p>Build a txt file with the list of topic for the Kafka Cluster defined in the config.yaml. </p> <pre><code>shift_left project list-topics $PIPELINES\n</code></pre> <p>Each topic has a json object to describe its metadata.</p> topic metadata <pre><code>   {\n    'kind': 'KafkaTopic',\n    'metadata': {\n        'self': 'https://p....confluent.cloud/kafka/v3/clusters/lkc-..../topics/....audit-trail',\n        'resource_name': 'crn:///kafka=lkc-..../topic=....audit-trail....'\n    },\n    'cluster_id': 'lkc-.....',\n    'topic_name': '....audit-trail',\n    'is_internal': False,\n    'replication_factor': 3,\n    'partitions_count': 1,\n    'partitions': {'related': 'https://.../partitions'},\n    'configs': {'related': 'https://...../configs'},\n    'partition_reassignments': {'related': 'https://..../partitions/-/reassignment'},\n    'authorized_operations': []\n}\n</code></pre>"},{"location":"recipes/#copy-all-the-statements-for-a-given-product-name","title":"Copy all the statements for a given product name","text":"<p>The goal is to extract all the Flink statements for a given product with their cross-product parents. </p> <ul> <li> <p>First be sure $PIPELINES is from the source git repository folder</p> </li> <li> <p>Run the command that will copy all the table part of the product (c360 is the product name), fro, $PIPELINES to a target folder:     <pre><code>shift_left project isolate-data-product c360 $PIPELINES ~/Code/data/c360-only\n</code></pre></p> </li> <li> <p>Change the PIPELINES environment variables, build table inventory and metadata:     <pre><code>export PIPELINES=/Users/jerome/Documents/Code/customers/mc/aqem-only/pipelines\nshift_left table build-inventory\nshift_left pipeline build-all-metadata\n</code></pre></p> </li> <li> <p>Assess a build execution plan, after configuring the config.yaml, to verify the coherence of the statement relationships:     <pre><code>shift_left pipeline build-execution-plan --product-name c360\n</code></pre></p> </li> </ul>"},{"location":"recipes/#table-related-tasks","title":"Table related tasks","text":"<p>On a day to day basis Data Engineer may need to add a table and the SQL statements to create and insert records to the new table. The table has to land in one of the hierarchy: facts, dimensions, views, intermediates, sources.</p>"},{"location":"recipes/#add-a-table-structure-to-the-flink-project","title":"Add a table structure to the Flink project","text":"<ul> <li>Get help for the <code>shift_left</code> table management CLI</li> </ul> <pre><code>shift_left table --help\n</code></pre> <ul> <li>Create a new table folder structure to start writing SQL statements using basic templates: (see the command description). There are two mandatory arguments: the table name and the folder to write the new table structure to. Be sure to following table name naming convention.</li> </ul> <pre><code>shift_left table init fct_user $PIPELINES/facts --product-name p3\n</code></pre> Output example <pre><code>facts\n    \u2514\u2500\u2500 p3\n        \u2514\u2500\u2500 fct_user\n            \u251c\u2500\u2500 Makefile\n            \u251c\u2500\u2500 sql-scripts\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.p3_fct_user.sql\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.p3_fct_user.sql\n            \u251c\u2500\u2500 tests\n            \u2514\u2500\u2500 tracking.md\n</code></pre> <ul> <li>Another example for an intermediate table:</li> </ul> <pre><code>shift_left table init int_p3_users $PIPELINES/intermediates --product-name p3\n</code></pre> Output example <pre><code>intermediates\n    \u2514\u2500\u2500 p3\n        \u2514\u2500\u2500 int_p3_users\n            \u251c\u2500\u2500 Makefile\n            \u251c\u2500\u2500 sql-scripts\n            \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.int_p3_users.sql\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.int_p3_users.sql\n            \u251c\u2500\u2500 tests\n            \u2514\u2500\u2500 tracking.md\n</code></pre> <p>See also the table naming convention section.</p>"},{"location":"recipes/#using-makefile-for-deployment","title":"Using Makefile for deployment","text":"<p>During the development of the DDL and DML it may be more efficient to use the confluent CLI to deploy the Flink statements. To make the things easier a Makefile exists in each table folder to support the deployment during development. Each Confluent Cloud cli commands are defined in a common makefile that is used by any Makefile within a 'table' folder. As an example the fact table <code>fct_order</code> has a Makefile at the same level of the sql-scripts.</p> <pre><code>\u2500\u2500 fct_order\n    \u251c\u2500\u2500 Makefile\n    \u251c\u2500\u2500 sql-scripts\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.p1_fct_order.sql\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.p1_fct_order.sql\n</code></pre> <p>The Makefile import the <code>common.mk</code></p> Change the relative path <p>It may be needed to change the relative path to the common.mk file when the number of sub folders from the $PIPELINES is greater or less that 3 levels from the current folder.</p> <ul> <li>To setup, install make on Windows, or for MacOS: <code>brew install make</code></li> <li>Be sure to be on a confluent cli above 4.24. To upgrade: <code>confluent update</code></li> <li>Be sure when starting a new session to have login to Confluent cloud and set the good endpoint:</li> </ul> <pre><code>make init\n</code></pre> <ul> <li>To create one Flink dynamic table using the target Flink compute pool do:</li> </ul> <pre><code>make create_flink_ddl\n</code></pre> <p>This action creates the Kafka topic with the name of the table and creates the schema definitions for the key and the value in the Schema Registry of the Confluent Cloud environment. A DDL execution will terminate and the Flink job statement is set to be Completed. The Make tsarget also ask for deleting the DDL Flink statement as we need to keep a unique name for the Flink statement. As a Flink-developer role, you need to be able to delete Flink Statement.</p> How it works <p>Each makefile is reusing targets defined in a common.mk file that is under the $PIPELINES folder. This file uses environment variables so each Developer can have their own compute pool, and work in different environment. See setup env variables section. See the top of the common.mk file for the usage of those environment variables the template is here. </p> <ul> <li>Verify the completion of the job using cli:</li> </ul> <pre><code>make describe_flink_ddl\n</code></pre> <ul> <li>For DML deplouyment use: (This job will run forever until stopped or deleted)</li> </ul> <pre><code>make create_flink_dml\n</code></pre> <ul> <li>Verify the running job using cli:</li> </ul> <pre><code>make describe_flink_dml\n</code></pre> <ul> <li>Sometime, developer may need to delete the created topics and schemas, for that the makefile target is:</li> </ul> <pre><code>make drop_table_&lt;table_name&gt;\n</code></pre> <ul> <li>Each Flink Statement is named, so it may be relevant to delete a created statement with the command:</li> </ul> <pre><code>make delete_flink_statements\n</code></pre>"},{"location":"recipes/#discover-the-current-source-dependencies","title":"Discover the current source dependencies","text":"<p>When doing a migration project it may be interesting to understand the current SQL statement relationship with the tables it uses.</p> <p>It is assumed the source project, is constructed with the Kimball approach. Existing Data Platforms have such capabilities, but we found interesting to get a simple tool that goes within the source SQL content file and build a dependencies graph. The current SQL parser is good to parse dbt SQL file, as Flink SQL.</p> <p>Starting with a single fact table, the following command will identify the dependency hierarchy and include elements to track the migration project:</p> <pre><code>shift_left table search-source-dependencies $SRC_FOLDER/facts/fact_education_document.sql $SRC_FOLDER\n</code></pre> <ul> <li>The output may look like in the followring report:</li> </ul> <pre><code>-- Process file: $SRC_FOLDER/facts/fact_education_document.sql\nTable: fact_education_document in the SQL ../facts/fact_education_document.sql  depends on:\n  - int_education_completed  in SQL ../intermediates/int_education_completed.sql\n  - int_non_abandond_training  in SQL ../intermediates/int_non_abandond_training.sql\n  - int_unassigned_curriculum  in SQL ../intermediates/int_unassigned_curriculum.sql\n  - int_courses   in SQL ../intermediates/int_courses.sql\n  - int_docs_wo_training_data  in SQL ../intermediates/docs/int_docs_wo_training_data.sql\n</code></pre>"},{"location":"recipes/#validate-naming-convention-and-best-practices","title":"Validate naming convention and best practices","text":"<p>It is possible to assess a set of basic rules against all files in the $PIPELINES folder</p> <pre><code>shift_left table validate-table-names $PIPELINES\n</code></pre> <p>Some standard reported violations:</p> Error message Action Comments CREATE TABLE statement CREATE TABLE IF NOT EXISTS statement Missing 'IF NOT EXISTS' WRONG FILE NAME <code>&lt;ddl|dml&gt;.&lt;int|src&gt;_&lt;product&gt;_&lt;table_name&gt;.sql</code> Refer DDL,DML file naming standards WRONG TABLE NAME <code>&lt;src|int&gt;_&lt;product&gt;_&lt;table_name&gt;</code> Refer Table naming standards WRONG <code>changelog.mode</code> <code>changelog.mode</code> should be <code>upsert</code> for source, intermediate. <code>retract</code> for sink None WRONG <code>kafka.cleanup-policy</code> <code>kafka.cleanup-policy</code> should be <code>delete</code> for source, intermediate. <code>compact</code> for sink Note <code>delete</code> is default <code>key.avro-registry.schema-context</code> NOT FOUND <code>.flink-dev</code> or <code>.flink-stage</code> Needed for 2 clusters 1 Schema Registry setup ... dev &amp; stage <code>value.avro-registry.schema-context</code> NOT FOUND <code>.flink-dev</code> or <code>.flink-stage</code> Needed for 2 clusters 1 Schema Registry setup ... dev &amp; stage MISSING pipeline definition pipeline_definition.json is missing Refer to GIT repo directory structure standards INVALID pipeline ddl_ref <code>ddl_ref</code> path in <code>pipeline_definition.json</code> doesnt exist Run the <code>shift_left</code> utils tool to resolve INVALID pipeline dml_ref <code>dml_ref</code> path in <code>pipeline_definition.json</code> doesnt exist Run the <code>shift_left</code> utils tool to resolve INVALID pipeline table_name <code>table_name</code> in <code>pipeline_definition.json</code> is invalid Due to a bug in the past the tool may write <code>No-Table</code>. Investigate and resolve"},{"location":"recipes/#a-table-name-naming-convention","title":"A table name naming convention","text":"<p>The folder name may be used as a table name in a lot of commands. The following naming convention is defined:</p> <ul> <li>Sources:  Template: <code>src_&lt;product&gt;_&lt;table_name&gt;</code>   Examples: <code>src_p1_records, src_p2_users</code> </li> <li>Intermediates:  Template: <code>int_&lt;product&gt;_&lt;table_name&gt;</code> Examples: <code>int_p1_action, int_p2_inventory_event</code></li> <li>Facts or dimensions: Template: <code>&lt;product&gt;_&lt;fct|dim&gt;&lt;table_name&gt;</code> Examples: <code>p1_dim_user_role,  p2_fct_event_link</code></li> <li>Materialized Views: Template: <code>&lt;product&gt;_mv_&lt;table_name&gt;</code>  Examples: <code>p2_mv_cancel_event, p2_mv_config_users</code></li> </ul>"},{"location":"recipes/#ddl-and-dml-file-naming-convention","title":"DDL and DML File naming convention","text":"<ul> <li> <p>Sources:</p> <ul> <li>Template: <code>&lt;ddl|dml&gt;.src_&lt;product&gt;_&lt;table_name&gt;.sql</code></li> <li>Examples: <code>dml.src_p1_table_1.sql</code></li> </ul> </li> <li> <p>Intermediates:</p> <ul> <li>Template: <code>&lt;ddl|dml&gt;.int_&lt;product&gt;_&lt;table_name&gt;.sql</code></li> <li>Examples: <code>dml.p1_int_table_1.sql</code></li> </ul> </li> <li> <p>Facts, Dimensions &amp; Views:</p> <ul> <li>Template: <code>&lt;ddl|dml&gt;.&lt;product&gt;_&lt;fct|dim|mv&gt;_&lt;table_name&gt;.sql</code></li> <li>Examples: <code>ddl.p1_fct_table_1.sql, dml.p1_fct_table_1.sql, ddl.p1_mv_table_1.sql</code></li> </ul> </li> </ul>"},{"location":"recipes/#understand-the-current-flink-statement-relationship","title":"Understand the current Flink Statement relationship","text":"<p>When the DML is created with the FROM, JOINS,... it is possible to use the tool to get the hierarchy of parents and build the pipeline_definition.json file for a table. It is strongly recommended to use sink tables, like <code>dimensions, facts or views</code> dml statements.</p> <ol> <li> <p>It is important that table inventory is up to date. See this section</p> </li> <li> <p>Run the command to update the current pipeline definition metadata and update the parents ones, recursively.</p> </li> </ol> <pre><code>shift_left pipeline build-metadata $PIPELINES/dimensions/p1/dim_event/sql-scripts/dml.p1_dim_event.sql\n</code></pre> <p>This will build the <code>pipeline_definition.json</code>.</p> a simple metadata file <pre><code>{\n\"table_name\": \"fct_order\",\n\"type\": \"fact\",\n\"dml_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\",\n\"ddl_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/ddl.fct_order.sql\",\n\"path\": \"pipelines/facts/p1/fct_order\",\n\"state_form\": \"Stateful\",\n\"parents\": [\n    {\n        \"table_name\": \"int_table_2\",\n        \"type\": \"intermediate\",\n        \"dml_ref\": \"pipelines/intermediates/p1/int_table_2/sql-scripts/dml.int_table_2.sql\",\n        \"ddl_ref\": \"pipelines/intermediates/p1/int_table_2/sql-scripts/ddl.int_table_2.sql\",\n        \"path\": \"pipelines/intermediates/p1/int_table_2\",\n        \"state_form\": \"Stateful\",\n        \"parents\": [],\n        \"children\": [\n            {\n            \"table_name\": \"fct_order\",\n            \"type\": \"fact\",\n            \"dml_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\",\n            \"ddl_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/ddl.fct_order.sql\",\n            \"path\": \"pipelines/facts/p1/fct_order\",\n            \"state_form\": \"Stateful\",\n            \"parents\": [],\n            \"children\": []\n            }\n        ]\n    },\n    {\n        \"table_name\": \"int_table_1\",\n        \"type\": \"intermediate\",\n        \"dml_ref\": \"pipelines/intermediates/p1/int_table_1/sql-scripts/dml.int_table_1.sql\",\n        \"ddl_ref\": \"pipelines/intermediates/p1/int_table_1/sql-scripts/ddl.int_table_1.sql\",\n        \"path\": \"pipelines/intermediates/p1/int_table_1\",\n        \"state_form\": \"Stateful\",\n        \"parents\": [],\n        \"children\": [\n            {\n            \"table_name\": \"fct_order\",\n            \"type\": \"fact\",\n            \"dml_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\",\n            \"ddl_ref\": \"pipelines/facts/p1/fct_order/sql-scripts/ddl.fct_order.sql\",\n            \"path\": \"pipelines/facts/p1/fct_order\",\n            \"state_form\": \"Stateful\",\n            \"parents\": [],\n            \"children\": []\n            }\n        ]\n    }\n],\n\"children\": []\n}\n</code></pre>"},{"location":"recipes/#understand-flink-statement-physical-plan","title":"Understand Flink Statement Physical Plan","text":"<p>The Flink statement physical plan shows how Flink executes your query and it is available via the console wit the \"EXPLAIN\" keyword. When a project includes a lot of Flink statements it makes sense to automate the discovery of potential issues, as part of a quality assurance process. Shift left tools propose two reports: 1/ at the table level, as a Data engineer will get from the console, and 2/ at the data product level to get a full assessment of all the Flink statements classified within a product.</p> <ul> <li> <p>Execute <code>explain</code> on one table, using a specific compute pool id, and persist the report under the $HOME/.shift_left folder. If the compute-pool-id is not specified it will use the default one in the config.yaml:     <pre><code>shift_left table explain --table-name &lt;tablename&gt; --compute-pool-id &lt;compute-pool-id&gt;\n</code></pre></p> </li> <li> <p>Execute for all the tables within a product, forcing keeping the reports for each table.     <pre><code>shift_left table explain --product-name &lt;productname&gt; --compute-pool-id &lt;compute-pool-id&gt; --persist-report\n</code></pre></p> </li> </ul>"},{"location":"recipes/#update-tables-content-recursively","title":"Update tables content recursively","text":"<p>There are some cases when we need to apply a set of updates to a lot of Flink Statement files in one run. The update can be coded into a python file which includes classes that implement an interface. The following updates are already supported.</p> Update name Type of changes Class name CompressionType Add the <code>'kafka.producer.compression.type' = 'snappy'</code> in the WITH section of the DDL <code>shift_left.core.utils.table_worker.Change_CompressionType</code> ChangeModeToUpsert Set the <code>'changelog.mode' = 'upsert'</code> configuration <code>shift_left.core.utils.table_worker.ChangeChangeModeToUpsert</code> ChangePK_FK_to_SID apply replace(\"_pk_fk\", \"_sid\") to dml <code>shift_left.core.utils.table_worker.ChangePK_FK_to_SID</code> Concat_to_Concat_WS change the type of concat to use <code>shift_left.core.utils.table_worker.Change_Concat_to_Concat_WS</code> ReplaceEnvInSqlContent Replace the name of the raw topic with the environement prefix <code>shift_left.core.utils.table_worker.ReplaceEnvInSqlContent</code> <p>Example of a command to modify all the DDL files in the $PIPELINES folder hierarchy:</p> <pre><code>shift_left table update-tables $PIPELINES  --ddl --class-to-use shift_left.core.utils.table_worker.Change_CompressionType\n</code></pre> Extending the TableWorker <p>The class is in shift_left.core.utils.table_worker.py module. Each new class needs to extend the TabeWorker interface</p> <pre><code>class TableWorker():\n\"\"\"\nWorker to update the content of a sql content, applying some specific logic\n\"\"\"\ndef update_sql_content(sql_content: str) -&gt; Tuple[bool, str]:\n    return (False, sql_content)\n\n\nclass YourOwnUpdate(TableWorker):\n\n def update_sql_content(sql_content: str) -&gt; Tuple[bool, str]:\n    # your code here\n    return updated, new_content\n</code></pre>"},{"location":"recipes/#specific-task-to-update-all-makefiles","title":"Specific task to update all Makefiles","text":"<p>During the life of a project it may be relevant to add specific Make target, change naming convention,... therefore there is a need to be able to change all the Makefile within a <code>pipelines</code> folder. The command is simple and will take the updated template as source and apply specific tuning per table folder:</p> <pre><code>shift_left table update-all-makefiles $PIPELINES\n</code></pre> <p>As of now the template is in the source folder: <code>shift_left_utils/src/shift_left/shift_left/core/templates/makefile_ddl_dml_tmpl.jinja</code>.</p>"},{"location":"recipes/#build-table-inventory","title":"Build table inventory","text":"<p>The inventory is built by crowling the Flink project <code>pipelines</code> folder and by looking at each dml to get the table name. The inventory is a hashmap with the key being the table name and the value is a <code>FlinkTableReference</code> defined as:</p> <pre><code>class FlinkTableReference(BaseModel):\n    table_name: Final[str] \n    ddl_ref: Optional[str]\n    dml_ref: Optional[str]\n    table_folder_name: str\n</code></pre> <ul> <li>To build an inventory file do the following command:</li> </ul> <pre><code>shift_left table build-inventory $PIPELINES\n</code></pre> Example of inventory created <pre><code>```json\n\"src_table_2\": {\n    \"table_name\": \"src_table_2\",\n    \"dml_ref\": \"../examples/flink_project/pipelines/sources/p1/src_table_2/sql-scripts/dml.src_table_2.sql\",\n    \"table_folder_name\": \"../examples/flink_project/pipelines/sources/p1/src_table_2\"\n},\n\"src_table_3\": {\n    \"table_name\": \"src_table_3\",\n    \"dml_ref\": \"../examples/flink_project/pipelines/sources/p1/src_table_3/sql-scripts/dml.src_table_3.sql\",\n    \"table_folder_name\": \"../examples/flink_project/pipelines/sources/p1/src_table_3\"\n},\n\"src_table_1\": {\n    \"table_name\": \"src_table_1\",\n    \"dml_ref\": \"../examples/flink_project/pipelines/sources/src_table_1/sql-scripts/dml.src_table_1.sql\",\n    \"table_folder_name\": \"../examples/flink_project/pipelines/sources/src_table_1\"\n},\n\"int_table_2\": {\n    \"table_name\": \"int_table_2\",\n    \"dml_ref\": \"../examples/flink_project/pipelines/intermediates/p1/int_table_2/sql-scripts/dml.int_table_2.sql\",\n    \"table_folder_name\": \"../examples/flink_project/pipelines/intermediates/p1/int_table_2\"\n},\n\"int_table_1\": {\n    \"table_name\": \"int_table_1\",\n    \"dml_ref\": \"../examples/flink_project/pipelines/intermediates/p1/int_table_1/sql-scripts/dml.int_table_1.sql\",\n    \"table_folder_name\": \"../examples/flink_project/pipelines/intermediates/p1/int_table_1\"\n},\n\"fct_order\": {\n    \"table_name\": \"fct_order\",\n    \"dml_ref\": \"../examples/flink_project/pipelines/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\",\n    \"table_folder_name\": \"../examples/flink_project/pipelines/facts/p1/fct_order\"\n}\n```\n</code></pre> <p>The <code>inventory.json</code> file is saved under the $PIPELINES folder and it is used intensively by the <code>shift_left</code> cli.</p> Update the inventory <p>Each time a new table is added or renamed, it is recommended to run this command. It can even be integrated in a CI pipeline.</p>"},{"location":"recipes/#work-with-pipelines","title":"Work with pipelines","text":"<p>The table inventory, as created in previous recipe, is important to get the pipeline metadata created. The approach is to define metadata for each table to keep relevant information for the DDL and DML files, the parents and children relationships. The recursive information is limited to 2 levels to avoid infinite loops.</p> <p>A source table will not have parent, while a sink table will not have children. Intermediate tables have both.</p> <p>For deeper analysis of the challenges of deploying Flink statements, we recommend reading Confluent Cloud Flink product documentation - Query evolution chapter and the pipeline manager chapter.</p>"},{"location":"recipes/#build-structured-pipeline-metadata-and-walk-through","title":"Build structured pipeline metadata and walk through","text":"<p>A pipeline is discovered by walking from the sink to the sources via intermediate statements. Each pipeline is a list of existing dimension and fact tables, and for each table the tool creates the <code>pipeline_definition.json</code>.</p> <p>The project folder structure for a table looks like in the following convention:</p> <p><code>&lt;facts | intermediates | dimensions | sources&gt;/&lt;product_name&gt;/&lt;table_name&gt;</code></p> <p>The <code>pipeline_definition.json</code> is persisted under the <code>&lt;table_name&gt;</code> folder. </p> <p>The tool needs to get an up-to-date inventory, see previous section to build it.</p> <ul> <li>Build all the <code>pipeline_definition.json</code> from a given sink by specifying the DML file path:</li> </ul> <pre><code>shift_left pipeline build-metadata $PIPELINES/facts/p1/fct_order/sql-scripts/dml.fct_order.sql\n# you can add a folder, as CLI argument, to get the path to the inventory. By default it uses the environment variable.\nshift_left pipeline build-metadata $PIPELINES/facts/p1/fct_order/sql-scripts/dml.fct_order.sql $PIPELINES\n</code></pre> <p>The tool goes over the SQL content and gets table names from the JOIN, and the FROM predicates. It will remove any CTE and any extracted string that are not table names. The regular expression may have some misses, so the tool searches within the table inventory, anyway.</p> <p>The following figure illustrates what will happen for each parent of the referenced table. The first figure illustrates existing Flink statement hierarchy, which means, existing tables have a pipeline_definition.json file in their repective folder, created from previous run. </p> <p>As a developer add the sink table E, the tool is re-executed so that the E and C pipeline definitions are newly created, while the Z, which already exists, has its pipeline_definition children list updated, by adding the child C.</p> <p>The tool will keep existing files and merge content.</p> The different pipeline_definition.json impacted <p>The sink table E will have a new metadata file:</p> <pre><code>{\n\"table_name\": \"E\",\n\"type\": \"fact\",\n\"parents\": [\n    {\n        \"table_name\": \"C\",\n        \"type\": \"intermediate\",\n        \"path\": \"pipelines/intermediates/p1/C\",\n}\n</code></pre> <p>Table C will also have a new one:</p> <pre><code>{\n\"table_name\": \"C\",\n\"type\": \"intermediate\",\n\"parents\": [\n    {\n        \"table_name\": \"Z\",\n        \"type\": \"intermediate\",\n        \"path\": \"pipelines/intermediates/p/Z\",\n    }],\n\"children\": [ { \"table_name\": \"E\", ...} ]\n}\n</code></pre> <p>While Z get a new child:</p> <pre><code>{\n\"table_name\": \"Z\",\n\"type\": \"intermediate\",\n\"parents\": [{ \"table_name\": \"X\", ...},\n        { \"table_name\": \"Y\", ...},\n    ],\n\"children\": [ { \"table_name\": \"D\", ...},\n                { \"table_name\": \"P\", ...},\n                 { \"table_name\": \"C\", ...}\n            ]\n}\n</code></pre>"},{"location":"recipes/#delete-pipeline_definitionjson-files-for-a-given-folder","title":"Delete pipeline_definition.json files for a given folder","text":"<p>Delete all the <code>pipeline_definition.json</code> files from a given folder. The command walk down the folder tree to find table folder.</p> <pre><code>shift_left pipeline delete-all-metadata $PIPELINES\n</code></pre> <p>Only the facts tables:</p> <pre><code>shift_left pipeline delete-all-metadata $PIPELINES/facts\n</code></pre>"},{"location":"recipes/#define-pipeline-definitions-for-all-the-tables-within-a-folder-hierarchy","title":"Define pipeline definitions for all the tables within a folder hierarchy","text":"<p>It may be relevant to update all the metadata definitions within a given folder. Start by deleting all and then recreate:</p> <ul> <li>For facts or dimensions</li> </ul> <pre><code>shift_left pipeline delete-all-metadata $PIPELINES/facts\n#\nshift_left pipeline build-all-metadata $PIPELINES/facts\n</code></pre> <ul> <li>For all pipelines</li> </ul> <pre><code>shift_left pipeline delete-all-metadata $PIPELINES\n#\nshift_left pipeline build-all-metadata $PIPELINES\n# same as env variable will be used\nshift_left pipeline build-all-metadata\n</code></pre>"},{"location":"recipes/#modify-sql-content-during-deployment","title":"Modify SQL content during deployment","text":"<p>Sometime there are requirements to adapt the content of the SQL statement during deployment into different environments. By environment we mean Kafka cluster, Confluent Cloud environment and schema registry. The shift_left tool defines a contract via the TableWorker class with an <code>update_sql_content</code> function to implement:</p> core.utils.table_worker.py<pre><code>class TableWorker():\n    def update_sql_content(sql_content: str, string_to_change_from: str= None, string_to_change_to: str= None) -&gt; Tuple[bool, str]:\n    \"\"\"\n        Args:\n            sql_content (str): The original SQL statement to transform\n            column_to_search (Optional[str], optional): Column name to search for in tenant filtering.\n                Required for dev environment tenant filtering. Defaults to None.\n            product_name (Optional[str], optional): Product identifier for environment-specific logic.\n                Used in dev environment filtering and topic naming. Defaults to None.\n\n        Returns:\n            Tuple[bool, str]: A tuple containing:\n                - bool: True if the SQL content was modified, False otherwise\n                - str: The transformed SQL content\n    \"\"\"\n        return (False, sql_content)\n</code></pre> <p>When Data engineers or SREs need to develop specific transformations, they can create an extension class and modify the config.yaml. Here are the steps:</p> <ol> <li>Clone the shift_left git repository</li> <li>Under the src/shift_left/shift_left folder, create an extension folder (same level as core and cli_commands folder):     <pre><code>\u2514\u2500\u2500 shift_left\n\u251c\u2500\u2500 dist\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 shift_left\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 cli.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 cli_commands\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 core\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 utils\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 table_worker.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 extensions\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 sql_content_processing.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 test_sql_content_processing.py\n</code></pre></li> <li>Add your class in a python code and a unit test module. The class needs to extends the TableWorker     <pre><code>class ModifySqlContentForDifferentEnv(TableWorker):\n    def __init__(self):\n        pass\n\n    def update_sql_content(self, sql_content: str, column_to_search: Optional[str] = None, product_name: Optional[str] = None) -&gt; Tuple[bool, str]:\n        # implement logic here\n</code></pre></li> <li> <p>Change the config.yaml to reference your class:     <pre><code>app:\n    sql_content_modifier: shift_left.extensions.sql_content_processing.ModifySqlContentForDifferentEnv\n</code></pre></p> </li> <li> <p>Build the shift_left cli, change the version in the <code>pyproject.toml</code> file:     <pre><code>uv build .\n</code></pre></p> </li> </ol> <p>This build process can be automated via a git action workflow.</p>"},{"location":"recipes/#build-pipeline-static-reports","title":"Build pipeline static reports","text":"<p>A static report is built by looking at the SQL content of the DML Flink statement and build list of parents of each table. The build-all-metadata may update t he meta-data file children part as other table consumes current table. As an example if <code>Table A</code> has zero children, but 6 parents the first version of its <code>pipeline_definition.json</code> will include only its 6 parents. When building <code>Table B</code> dependencies, as it uses <code>Table A</code>, then the <code>Table A</code> children will have the Table B as child:</p> <pre><code>{\n   \"table_name\": \"table_a\",\n   \"product_name\": \"p1\",\n   ....\n    \"children\": [\n            {\n               \"table_name\": \"table_b\",\n               \"product_name\": \"p1\",\n               \"type\": \"intermediate\",\n               \"dml_ref\": \"pipelines/intermediates/p1/table_b/sql-scripts/dml.int_p1_table_b.sql\",\n               \"ddl_ref\": \"pipelines/intermediates/p1/table_b/sql-scripts/ddl.int_p1_table_b.sql\",\n               \"path\": \"pipelines/intermediates/aqem/table_b\",\n               \"state_form\": \"Stateful\",\n               \"parents\": [],\n               \"children\": []\n            }\n         ]\n</code></pre> <p>To avoid having too much data the parents and children lists of those added child are empty.</p> <p>Once those metadata files are created a lot of things can be done, and specially understand the static relationship between statements.</p> <ul> <li>Get a report from one sink table to n sources: </li> </ul> <pre><code>shift_left pipeline report fct_table\n# explicitly specifying the pipeline folder. (this is to demonstrate that it can run in STAGING folder too)\nshift_left pipeline report fct_table $PIPELINES\n</code></pre> <ul> <li>Get a report from one source to n sinks:</li> </ul> <pre><code>shift_left pipeline report src_table\n</code></pre> <p>The same approach works for intermediate tables. The tool supports different output:</p> <ul> <li>A very minimalist graph view</li> </ul> <pre><code>shift_left pipeline report fct_table --graph\n</code></pre> <ul> <li>json extract</li> </ul> <pre><code>shift_left pipeline report fct_table --json\n</code></pre> <ul> <li>yaml</li> </ul> <pre><code>shift_left pipeline report fct_table --yaml\n</code></pre>"},{"location":"recipes/#assess-a-flink-statement-execution-plan","title":"Assess a Flink Statement execution plan","text":"<p>The execution plan is a topological sorted Flink statement hierarchy enriched with current state of the running DMLs, with a plan to start non running parents, and update or restart the children depending of the upgrade mode: stateless or stateful.</p> <ul> <li> <p>Here is a basic command for an intermediate table:     <pre><code>shift_left pipeline build-execution-plan --table-name int_table_2 \n</code></pre></p> </li> <li> <p>The options that are relevant to control the execution plan:</p> </li> </ul> Option Why --compute-pool-id will run the statements into one compute pool --may-start-descendants Deploying a table will children, will restart all descedants --force-ancestors Rare to use. Only when redeploying all product or directory --cross-product-deployment Enfore touching cross product table. not recommended -- <ul> <li> <p>This command will take into account the following parameters in the config.yaml:     <pre><code>flink.flink_url\nflink.api_key\nflink.api_secret\nflink.compute_pool_id\nflink.catalog_name\nflink.database_name\n</code></pre></p> </li> <li> <p>The execution plan can be extended to a product, so a user may see all the dependencies for a given product:     <pre><code>shift_left pipeline build-execution-plan --product-name p2\n</code></pre></p> </li> <li> <p>It may be interesting to get the execution plan for a given layer, like all the facts or intermediates, so the granulatity is at the directory level, so it can be intermediate, facts... or even combination of level/product like:     <pre><code>shift_left pipeline build-execution-plan --dir $PIPELINES/intermediates/p2\n</code></pre></p> </li> </ul> The outcome of an execution plan <p>The information reported is taking into account the table hierarchy and the state of the Statement currently running:</p> <pre><code>To deploy fct_order the following statements need to be executed in the order\n\n--- Parents impacted ---\n</code></pre> For any help of pipeline commands <pre><code>shift_left pipeline --help\n</code></pre>"},{"location":"recipes/#accessing-running-statements","title":"Accessing running statements","text":"<p>The execution plan helps to get valuable information and reports can be built from there:</p> <ul> <li>Get the current Flink statement running in the environment specified in the config.yaml under: <code>confluent_cloud.environment_id</code> for a given table</li> </ul> <pre><code># Get help\nshift_left pipeline report-running-statements  --help\n# Running Flink Statements\nshift_left pipeline report-running-statements --table-name fct_order\n</code></pre> <p>The list will include all the ancestors of the current table with the statement name, comput pool id and statement running status.</p> <ul> <li>The report can be used to get all the statements for a given product. This will generate two files under the $HOME/.shift_left folder: csv and json files.</li> </ul> <pre><code>shift_left pipeline report-running-statements --product-name p1\n</code></pre> <ul> <li>Finally the report can be run from the content of a folder, which basically addresses the use case to look at intermediates or facts for a given product or cross products:</li> </ul> <pre><code>shift_left pipeline report-running-statements --dir $PIPELINES/facts/p1\n</code></pre>"},{"location":"recipes/#pipeline-deployment","title":"Pipeline Deployment","text":"<p>There are multiple choices to deploy a Flink Statement:</p> <ol> <li> <p>During the Flink statement development activity, <code>confluent cli</code> may be used but <code>shift_left table init</code> command created an higher level of abstraction, using a Makefile within the table folder. This makefile directly use the <code>confluent cli</code>.     <pre><code>make create_flink_ddl\nmake create_flink_dml\nmake drop_table_&lt;table_name&gt;\n</code></pre></p> </li> <li> <p>Use the shift_left CLI to do a controlled deployment of a table. The tool uses the pipeline metadata to walk down the static relationship between Flink statements to build an execution plan and then deploy it. Data engineers should run the <code>pipeline build-execution-plan</code> to assess the deep relationship between statements and understand the state of running Flink jobs.</p> </li> <li> <p>Deploy statements:</p> <ul> <li> <p>Deploy for one table without updating ancestors. It will start ancestors not just started. The deployment will take the full pipeline from the source to sink giving a sink table name. <pre><code>shift_left pipeline deploy --from-table p1_fct_order\n</code></pre></p> </li> <li> <p>Deploy for one table and restart its ancestors <pre><code>shift_left pipeline deploy --from-table p1_fct_order --force-ancestors\n</code></pre></p> </li> <li>Deploy one table and its children <pre><code>shift_left pipeline deploy  --from-table p1_fct_order --may-start-descendants\n</code></pre></li> <li>Which can be combined to start everything within the scope of a pipeline <pre><code>shift_left  pipeline deploy --from-table p1_fct_order  --force-ancestors --may-start-descendants\n</code></pre></li> <li>Which can be combined to start everything within the scope of a pipeline, defaulting to the given compute-pool-id if not found: <pre><code>shift_left pipeline deploy --from-table p1_fct_order  --force-ancestors --may-start-descendants --compute-pool-id lfcp-2...\n</code></pre></li> <li>Deploy a set of tables within the same layer: <pre><code>shift_left pipeline deploy --dir $PIPELINES/sources/p2 --force-ancestors --may-start-descendants\n</code></pre></li> <li>Deploy a set of tables for a given product: <pre><code>shift_left pipeline deploy --product-name p2 --force-ancestors --may-start-descendants\n</code></pre></li> </ul> </li> </ol>"},{"location":"recipes/#undeploy-pipeline","title":"Undeploy pipeline","text":"<p>It is possible to undeploy a pipeline from a given table:</p> <pre><code>shift_left pipeline undeploy --table-name p1_fct_order\n</code></pre> <p>Or at the product level:</p> <pre><code>shift_left pipeline undeploy --product-name p1\n</code></pre>"},{"location":"recipes/#troubleshooting-the-cli","title":"Troubleshooting the CLI","text":"<p>When starting the CLI, it creates a logs folder under the $HOME/.shift_left/logs folder. The level of logging is specified in the <code>config.yaml</code> file in the app section:</p> <pre><code>app:\n  logging: INFO\n</code></pre>"},{"location":"setup/","title":"Environment Setup - Deeper dive","text":"Versions <p>Created January 2025 - Updated 03/21/25: setup guide separation between user of the CLI and developer of this project. Update 08/09: config file explanation and how to get the parameters Update 12/05/25: move content to tutorial</p> <p>The new setup lab addresses how to setup the <code>shift_left</code> cli and how to validate the installation. </p> <p>This chapter addresses configuration review and tuning. </p>"},{"location":"setup/#the-configyaml-file","title":"The config.yaml file","text":"<p>The configuration file <code>config.yaml</code> is used intensively to tune the <code>shift_left</code> per environment and will be referenced by the environment variables: CONFIG_FILE. You should have different config.yaml for the different kafka cluster, schema registry and Flink environment.</p> <ul> <li> <p>Copy the <code>config_tmpl.yaml</code> template file to keep some important parameters for the CLI.    <pre><code>curl  https://raw.githubusercontent.com/jbcodeforce/shift_left_utils/refs/heads/main/src/shift_left/shift_left/core/templates/config_tmpl.yaml  -o  ./config.yaml\n</code></pre></p> </li> <li> <p>Modify the <code>config.yaml</code> with values from your Confluent Cloud settings. See the tabs below for the different sections of this file: </p> </li> </ul> Kafka sectionConfluent cloudFlinkApp <ul> <li>Get the Kakfa cluster URL using the <code>Confluent Console &gt; Cluster Settings</code> page. The URL has the cluster id and a second id that is used for the RESP API.</li> </ul> <pre><code>kafka:\n  bootstrap.servers: lkc-2qxxxx-pyyyy.us-west-2.aws.confluent.cloud:9092\n  cluster_type: stage\n  cluster_id: lkc-2qxxxx\n</code></pre> <ul> <li>Get environment ID from the <code>Environment details</code> in the Confluen Console. The cloud provider and region. </li> </ul> <p><pre><code>confluent_cloud:\n  environment_id:  env-20xxxx\n  region:  us-west-2\n  provider:  aws\n  organization_id: 5329.....96\n</code></pre> * The <code>organization_id</code> is defined under the <code>user account &gt; Organization settings</code></p> <ul> <li> <p>Flink settings are per environment. Get the URL endpoint by going to <code>Environments &gt; one_of_the_env &gt; Flink &gt; Endpoints</code>, copy the private or public endpoint <pre><code>flink:\n    compute_pool_id: lfcp-0...\n    catalog_name:  dev-flink-us-west-2\n    database_name:  dev-flink-us-west-2\n</code></pre></p> </li> <li> <p>The compute pool id is used as default for running Flink query.</p> </li> <li>Catalog name is the name of the environment and database name is the name of the Kafka cluster</li> </ul> <p>The app section defines a set of capabilities to tune the cli. <pre><code>app:\n    accepted_common_products: ['common', 'seeds']\n    sql_content_modifier: shift_left.core.utils.table_worker.ReplaceEnvInSqlContent\n    dml_naming_convention_modifier: shift_left.core.utils.naming_convention.DmlNameModifier\n    compute_pool_naming_convention_modifier: shift_left.core.utils.naming_convention.ComputePoolNameModifier\n    data_limit_where_condition : rf\"where tenant_id in ( SELECT tenant_id FROM tenant_filter_pipeline WHERE product = {product_name})\"\n    data_limit_replace_from_reg_ex: r\"\\s*select\\s+\\*\\s+from\\s+final\\s*;?\"\n    data_limit_table_type: source\n    data_limit_column_name_to_select_from: tenant_id\n    post_fix_unit_test: _ut\n    post_fix_integration_test: _it\n</code></pre></p> <ul> <li><code>post_fix_unit_test, post_fix_integration_test</code> are used to append the given string to table name during unit testing and integration test respectively.</li> <li>The <code>data_limit_replace_from_reg_ex, data_limit_table_type, data_limit_column_name_to_select_from</code> are used to add data filtering to all the source tables based on one column name to filter. The regex specifies to file the <code>select * from final</code> which is the last string in most Flink statements using CTEs implementation. </li> <li><code>sql_content_modifier</code> specifies the custom class to use to do some SQL content modification depending of the target environment. This is a way to extend the CLI logic to specific usage.</li> </ul>"},{"location":"setup/#configuration-file-setup","title":"Configuration File Setup","text":"<ul> <li> <p>Update the content of the config.yaml to reflect your Confluent Cloud environment. (For the commands used for migration, you do not need Kafka settings.)   <pre><code># Confluent Cloud Configuration\nconfluent_cloud:\n  organization_id: \"YOUR_CLUSTER_ID\"\n  environment_id: \"YOUR_ENVIRONMENT_ID\"\n  region: \"YOUR_REGION\"\n  provider: \"aws\"\nflink:\n  compute_pool_id: \"YOUR_COMPUTE_POOL_ID\"\n  catalog_name: \"environment_name\"\n  database_name: \"kafka_cluster_name\"\n</code></pre></p> </li> <li> <p>Set the following environment variables before using the tool. This can be done by:     <pre><code> curl  https://raw.githubusercontent.com/jbcodeforce/shift_left_utils/refs/heads/main/src/shift_left/shift_left/core/templates/set_env_temp ./set_env\n</code></pre></p> <p>Modify the CONFIG_FILE, FLINK_PROJECT, SRC_FOLDER, SL_LLM_* variables</p> </li> <li> <p>Source it:     <pre><code>source set_env\n</code></pre></p> </li> <li> <p>Validate config.yaml     <pre><code>shift_left project validate-config\n</code></pre></p> </li> </ul> Security access <p>The config.yaml file is ignored in Git. So having the keys in this file is not a major concern, as it is used by the developers only. But it may be possible, in the future, to access secrets using a Key manager API. This could be a future enhancement.</p>"},{"location":"setup/#environment-variables","title":"Environment variables","text":"<p>This document explains how to use environment variables to securely manage API keys and secrets instead of storing them in config.yaml files.</p> <p>The Shift Left utility now supports environment variables for sensitive configuration values. Environment variables take precedence over config.yaml values, allowing you to:</p> <ul> <li>Keep sensitive data out of configuration files</li> <li>Use different credentials for different environments</li> <li>Securely manage secrets in CI/CD pipelines</li> <li>Follow security best practices</li> </ul>"},{"location":"setup/#kafka-section","title":"Kafka Section","text":"Environment Variable Config Path Description <code>SL_KAFKA_API_KEY</code> <code>kafka.api_key</code> Kafka API key <code>SL_KAFKA_API_SECRET</code> <code>kafka.api_secret</code> Kafka API secret"},{"location":"setup/#confluent-cloud-section","title":"Confluent Cloud Section","text":"Environment Variable Config Path Description <code>SL_CONFLUENT_CLOUD_API_KEY</code> <code>confluent_cloud.api_key</code> Confluent Cloud API key <code>SL_CONFLUENT_CLOUD_API_SECRET</code> <code>confluent_cloud.api_secret</code> Confluent Cloud API secret"},{"location":"setup/#flink-section","title":"Flink Section","text":"Environment Variable Config Path Description <code>SL_FLINK_API_KEY</code> <code>flink.api_key</code> Flink API key <code>SL_FLINK_API_SECRET</code> <code>flink.api_secret</code> Flink API secret"},{"location":"setup/#priority-order","title":"Priority Order","text":"<p>The setting will use the following order:</p> <ol> <li>Environment Variables (highest priority)</li> <li>Config.yaml values (fallback)</li> <li>Default values set in the code</li> </ol> <p>If an environment variable is set, it will override the corresponding value in config.yaml. If a value is set in config.yaml it will be override the default value.</p>"},{"location":"setup/#setting-environment-variables","title":"Setting Environment Variables","text":""},{"location":"setup/#bashzsh","title":"Bash/Zsh","text":"<pre><code>export SL_KAFKA_API_KEY=\"your-kafka-api-key\"\nexport SL_KAFKA_API_SECRET=\"your-kafka-api-secret\"\nexport SL_FLINK_API_KEY=\"your-flink-api-key\"\nexport SL_FLINK_API_SECRET=\"your-flink-api-secret\"\nexport SL_CONFLUENT_CLOUD_API_KEY=\"your-confluent-cloud-api-key\"\nexport SL_CONFLUENT_CLOUD_API_SECRET=\"your-confluent-cloud-api-secret\"\n</code></pre>"},{"location":"setup/#using-env-file","title":"Using .env File","text":"<p>Create a <code>.env</code> file (don't commit this to version control): <pre><code>SL_KAFKA_API_KEY=your-kafka-api-key\nSL_KAFKA_API_SECRET=your-kafka-api-secret\nSL_FLINK_API_KEY=your-flink-api-key\nSL_FLINK_API_SECRET=your-flink-api-secret\nSL_CONFLUENT_CLOUD_API_KEY=your-confluent-cloud-api-key\nSL_CONFLUENT_CLOUD_API_SECRET=your-confluent-cloud-api-secret\n</code></pre></p> <p>Then load it before running your application: <pre><code>set -a &amp;&amp; source .env &amp;&amp; set +a\n</code></pre></p> <p>Never commit secrets to version control: Use <code>.gitignore</code> to exclude <code>.env</code> files</p>"},{"location":"setup/#common-issues","title":"Common Issues","text":"<ol> <li>\"Missing environment variables\" error</li> <li>Check that environment variable names are correct (case-sensitive)</li> <li>Verify that variables are exported in your shell</li> <li> <p>Use `env | grep SL to see what's set</p> </li> <li> <p>To see all supported environment variables, you can call the help function in Python: <pre><code>from shift_left.core.utils.app_config import print_env_var_help\nprint_env_var_help()\n</code></pre></p> </li> </ol>"},{"location":"tracking/","title":"Feature tracking","text":"<p>If you want to contribute see the coding and design chapter to review the components and how to develop this project</p>"},{"location":"tracking/#cli-commands","title":"CLI Commands","text":"CLI Command Code UT IT Documentation Recipe project init init Y Y Y Y project list-topics list_topics N Y Y Y project list-compute-pools list_compute_pools N Y Y N project delete-all-compute-pools delete_all_compute_pools N N N N project clean-completed-failed-statements clean_completed_failed_statements N Y N N project validate-config validate_config N Y N N project list-modified-files list_modified_files N Y N N table init init Y Y Y Y table build-inventory build_inventory Y Y Y N table search-source-dependencies search_source_dependencies Y Y Y N table migrate migrate Y N Y Y table update-makefile update_makefile Y Y Y N table update-all-makefiles update_all_makefiles Y N N N table validate-table-names validate_table_names Y N Y N table update-tables update_tables Y N Y N table init-unit-tests init_unit_tests Y Y N N table run-unit-tests run_unit_tests Y Y Y Y table run-validation-tests run_validation_tests Y N N N table delete-unit-tests delete_unit_tests Y N N N table explain explain Y N N N pipeline build-metadata build_metadata Y N Y Y pipeline delete-all-metadata delete_all_metadata Y N Y N pipeline build-all-metadata build_all_metadata Y N Y N pipeline report report Y Y Y N pipeline deploy deploy Y N Y N pipeline build-execution-plan build_execution_plan Y Y N N pipeline report-running-statements report_running_statements Y N Y N pipeline undeploy undeploy Y Y Y N pipeline prepare prepare Y N N N pipeline analyze-pool-usage analyze_pool_usage Y N N N"},{"location":"tracking/#future-items-to-consider","title":"Future items to consider","text":"<ul> <li>Define a Data as a product project structure</li> <li>Integration tests</li> <li>Persist Flink statements retrieved in API in duckdb for short term replay</li> </ul>"},{"location":"coding/","title":"Code structure and development practices","text":"<p>This chapter is about code design, code organization.</p>"},{"location":"coding/#the-components","title":"The components","text":"<p>The <code>shift_left</code> CLI includes three main sections: 1/ project, 2/ table and 3/ pipeline</p> <p>Those layers organize the code to support the main set of commands used during the development and maintenance of a Flink project.</p> <p></p> <p>The green components are the ones exposing features and commands to the end user via the CLI. The set of commands are documented here. This command documentation is created automatically.</p> <p>The white components are a set of services to support the end-user facing commands and to offer functions to other services. They try to group concerns related to the following resources:</p> <ul> <li>Project</li> <li>Table</li> <li>Pipeline</li> <li>Flink Statement</li> <li>Confluent Cloud compute pools</li> <li>Tests for test harness</li> <li>Deployment and execution plan to manage execution plans.</li> </ul> <p>Finally a set of lower level utilities are used to do some integration with local files and remote REST API.</p>"},{"location":"coding/#code-organization","title":"Code organization","text":"<p>The cli.py is based of typer module and is the top level access to the cli execution.</p> <pre><code>\u2502\u00a0\u00a0 \u2514\u2500\u2500 shift_left\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 cli.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 cli_commands\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 pipeline.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 project.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 table.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 core\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 compute_pool_mgr.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 connector_mgr.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 deployment_mgr.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 metric_mgr.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 models\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 flink_compute_pool_model.py\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 flink_statement_model.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 pipeline_mgr.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 process_src_tables.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 project_manager.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 statement_mgr.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 table_mgr.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.mk\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 config_tmpl.yaml\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_table_skeleton.jinja\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 dedup_dml_skeleton.jinja\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 dml_src_tmpl.jinja\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 makefile_ddl_dml_tmpl.jinja\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_dedup_statement.jinja\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 tracking_tmpl.jinja\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 test_mgr.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 utils\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 app_config.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 ccloud_client.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 file_search.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 flink_sql_code_agent_lg.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 ksql_code_agent.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 naming_convention.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 prompts\n\u2502\u00a0\u00a0             \u2502\u00a0\u00a0 \u2514\u2500\u2500 ksql_fsql\n\u2502\u00a0\u00a0             \u2502\u00a0\u00a0     \u251c\u2500\u2500 mandatory_validation.txt\n\u2502\u00a0\u00a0             \u2502\u00a0\u00a0     \u251c\u2500\u2500 refinement.txt\n\u2502\u00a0\u00a0             \u2502\u00a0\u00a0     \u2514\u2500\u2500 translator.txt\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 report_mgr.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 sql_parser.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 table_worker.py\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 translator_to_flink_sql.py\n</code></pre>"},{"location":"coding/#unit-testing","title":"Unit testing","text":"<p>All test cases are under tests/ut and executed with</p> <pre><code>uv run pytest -s tests/ut\n</code></pre>"},{"location":"coding/#integration-tests","title":"Integration tests","text":"<p>All test cases are under tests/it and executed with</p> <pre><code>uv run pytest -s tests/it\n</code></pre>"},{"location":"coding/#classical-scenarios","title":"Classical scenarios","text":"<p>Be source to set environment variables for testing: </p> <pre><code>export FLINK_PROJECT=./tests/data/flink-project\nexport PIPELINES=$FLINK_PROJECT/pipelines\nexport STAGING=$FLINK_PROJECT/staging\nexport SRC_FOLDER=./tests/data/dbt-project\n\nexport TOPIC_LIST_FILE=$FLINK_PROJECT/src_topic_list.txt \nexport CONFIG_FILE=./tests/config.yaml\nexport CCLOUD_ENV_ID=env-\nexport CCLOUD_ENV_NAME=j9r-env\nexport CCLOUD_KAFKA_CLUSTER=j9r-kafka\nexport CLOUD_REGION=us-west-2\nexport CLOUD_PROVIDER=aws\nexport CCLOUD_CONTEXT=login-jboyer@confluent.io-https://confluent.cloud\nexport CCLOUD_COMPUTE_POOL_ID=lfcp-\n</code></pre>"},{"location":"coding/#work-with-sql-migration","title":"Work with SQL migration","text":"<ol> <li> <p>Assess dependencies from the source project (see the tests/data/dbt-project folder) with the command:</p> <pre><code>uv run shift_left table search-source-dependencies $SRC_FOLDER/facts/p7/fct_user_role.sql\n</code></pre> </li> <li> <p>Migrate a fact table and the others related ancestors to Staging using the table name that will be the folder name too.</p> <pre><code>uv run shift_left table migrate user_role $SRC_FOLDER/facts/p7/fct_user_role.sql $STAGING --recursive\n</code></pre> <p>The tool creates a folder in <code>./tests/data/flink-project/staging/facts/p7</code>, run the LLM , and generate migrate Flink SQL statement</p> </li> </ol>"},{"location":"coding/#work-on-pipeline-deployment","title":"Work on pipeline deployment","text":"<ol> <li>Be sure the inventory is created: <code>uv run shift_left table build-inventory $PIPELINES</code></li> </ol>"},{"location":"coding/#developers-notes","title":"Developer's notes","text":"<p>The modules to support the management of pipeline is <code>pipeline_mgr.py</code> and <code>deployment_mgr.py</code>.</p> <ul> <li>Testing a Flink deployment see [test - ]</li> </ul> <p>For deployment the approach is to build a graph from the table developer want to deploy. The graph includes the parents and then the children. The graph is built reading static information about the relationship between statement, and then go over each statement and assess if for this table the dml is running. For a parent it does nothing</p>"},{"location":"coding/#logs","title":"Logs","text":"<p>All the logs are under $HOME/.shift_left/logs folder with a sub folder matching the session: when the tool is started a trace like in the following example displays the folder name:     <pre><code>--------------------------------------------------------------------------------\n| SHIFT_LEFT Session started at 2025-07-23 19:29:50 LOGS folder is : .../.shift_left/logs/07-23-25-19-29-50-MF86 \n</code></pre></p>"},{"location":"coding/improve_pipeline_with_AI/","title":"Improve pipeline quality with AI","text":"<ul> <li>Use AI to help managing flow with intent: goal is to bring ingestion and transformation to anyone. Chat with the Confluent Flink assistant to create pipeline, filters for error, route to DLQ, build aggregates, prepare data from cdc and sink connector to S3 or to Database or with Tableflow. </li> <li>Should help for beginner to build PoC or build prototype fast</li> <li>Look at sql content to do grouping if there is a 1 to 1 dependency</li> <li>Assess deployed statements and propose improvement, help Data engineer to manage and monitor Flink statements.</li> <li>AI can help tracking data and lineage and create Atlas metadata files</li> <li>Looking at records to process, and flink statement logic it can decide to use one compute pool per statement or group</li> <li>Propose solution when statement has logic related to compute field values from date, and be reassessed every day</li> <li>Look at data content to understand it</li> </ul>"},{"location":"coding/llm_based_translation/","title":"SQL Translation Code Review","text":"Version <p>Created Dec - 2024  Updated Nov 25 - 2025</p> <p>The current AI based migration implementation supported by this tool enables migration of:</p> <ul> <li>dbt/Spark SQL to Flink SQL</li> <li>ksqlDB to Flink SQL</li> </ul> <p>The approach uses LLM agents local or remote. This document covers the design approach, development environment setup, and how to execute test to tune the AI based SQL migrations, specially how to extend the AI prompts or workflows to get better results.</p> <p>The implementation uses the OpenAI SDK, so different LLM models can be used, as soon as they support OpenAI. The <code>qwen3:30b</code> or <code>qwen-coder-30b-a3b-instruct-mlx-4bit</code> models can be used locally using Osaurus on Mac M3 Pro with 36GB RAM., or Ollama on Linux VM. Other models running remotely and supporting OpenAI APIs may be used too using your own API key. </p>"},{"location":"coding/llm_based_translation/#review-the-end-user-lab","title":"Review the end-user lab","text":"<p>Review the end user lab for ksql migration to understand the how the data engineer can use the tool for migration.</p> Example of Output <pre><code>process SQL file ../src-dbt-project/models/facts/fct_examination_data.sql\nCreate folder fct_exam_data in ../flink-project/staging/facts/p1\n\n--- Start translator AI Agent ---\n--- Done translator Agent: \nINSERT INTO fct_examination_data\n...\n--- Start clean_sql AI Agent ---\n--- Done Clean SQL Agent: \n--- Start ddl_generation AI Agent ---\n--- Done DDL generator Agent:\nCREATE TABLE IF NOT EXISTS fct_examination_data (\n    `exam_id` STRING,\n    `perf_id` STRING,\n...\n</code></pre> <p>For a given table, the tool creates one folder with the table name, a Makefile to help manage the Flink statements with Confluent CLI, a <code>sql-scripts</code> folder for the Flink DDL and DML statements. </p> <p>Example of created folders:</p> <pre><code>facts\n    \u2514\u2500\u2500 fct_examination_data\n        \u251c\u2500\u2500 Makefile\n        \u251c\u2500\u2500 sql-scripts\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.fct_examination_data.sql\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.fct_examination_data.sql\n        \u2514\u2500\u2500 tests\n</code></pre> <p>As part of the process, developers need to validate the generated DDL and update the PRIMARY key to reflect the expected key. This information is hidden in many files in dbt, and key extraction is not yet automated by the migration tools.</p> <p>Attention, the DML is not executable until all dependent input tables are created.</p> <ul> <li> <p>The ksqldb files to test, the migration from, are in src/shift_left/tests/data/ksql-project/sources</p> </li> <li> <p>Status of the working migration: See test_ksql_migration.py::TestKsqlMigrations code.</p> </li> </ul> Source Status Test Case splitting_tutorial 4 DDLs \u2705 4 DMLs \u2705 test_ksql_splitting_tutorial merge_tutorial 3 DDLs \u2705 2 DMLs \u2705 test_ksql_merge_tutorial"},{"location":"coding/llm_based_translation/#current-agentic-approach","title":"Current Agentic Approach","text":"<p>The current agentic workflow includes:</p> <ol> <li>Translate the given SQL content to Flink SQL</li> <li>Validate the syntax and semantics</li> <li>Generate DDL derived from DML</li> <li>Get human validation to continue or not the automation</li> <li>Deploy and test with validation agents [optional]</li> </ol> <p>The system uses validation agents that execute syntactic validation and automatic deployment, with feedback loops injecting error messages back to translator agents when validation fails.</p>"},{"location":"coding/llm_based_translation/#architecture-overview","title":"Architecture Overview","text":"<p>The multi-agent system with human-in-the-loop validation may use Confluent Cloud's Flink REST API to deploy a generated Flink statement. The following diagram represents the different agents working together:</p> <p>Both agents support two validation modes:</p> <ol> <li>Mandatory Validation: Always performed, checks syntax and best practices</li> <li>Live Validation: Optional, validates against live Confluent Cloud for Apache Flink</li> </ol> <p>The validation process includes:</p> <ul> <li>Syntax checking</li> <li>Semantic validation</li> <li>Iterative refinement (up to 3 attempts)</li> <li>Human-in-the-loop confirmation for live validation</li> </ul> <p>When validation fails, agents use specialized refinement prompts to:</p> <ul> <li>Analyze the specific error message</li> <li>Consider validation history</li> <li>Generate corrected SQL that addresses the identified issues</li> <li>Provide explanations of changes made</li> </ul> <p>This creates a self-correcting translation pipeline that improves accuracy through iterative feedback.</p>"},{"location":"coding/llm_based_translation/#agent-roles","title":"Agent Roles","text":"<p>As agent is a combination of LLM reference, prompts, and tool definitions, there will be different implementation of those agents if we do ksqlDB to Flink SQL or from Spark to Flink.</p>"},{"location":"coding/llm_based_translation/#ksqldb-to-flink-agents","title":"KsqlDB to Flink agents","text":"<p>Supporting class of the workflow is ksqlDB code agent.</p> <p>Each agent uses specialized system prompts stored in external files:</p> Agent Scope Prompt File Translator Raw KSQL to Flink SQL translation <code>ai/prompts/ksql_fsql/translator.txt</code> Table Detection Identify multiple CREATE statements <code>ai/prompts/ksql_fsql/table_detection.txt</code> Validation Validate Flink SQL constructs <code>ai/prompts/ksql_fsql/mandatory_validation.txt</code> Refinement Fix deployment errors <code>ai/prompts/ksql_fsql/refinement.txt</code>"},{"location":"coding/llm_based_translation/#code","title":"Code","text":"<p>The <code>src/shift_left/shift_left/ai</code> directory contains a suite of Large Language Model (LLM) agents designed to translate SQL from various dialects to Apache Flink SQL, with validation and iterative refinement capabilities.</p>"},{"location":"coding/llm_based_translation/#spark-to-flink-agents","title":"Spark to Flink agents","text":"<p>Supporting class of the workflow is Spark sql code agent.</p> <p>Same approach for spark SQL with the prompts being in the <code>ai/prompts/spark_fsql</code> folder. Each agent uses specialized system prompts stored in external files:</p> Agent Scope Prompt File Translator Spark SQL to Flink SQL translation <code>ai/prompts/spark_fsql/translator.txt</code> Table Detection Identify multiple CREATE statements <code>ai/prompts/spark_fsql/table_detection.txt</code> Validation Validate Flink SQL constructs <code>ai/prompts/spark_fsql/mandatory_validation.txt</code> Refinement Fix deployment errors <code>ai/prompts/spark_fsql/refinement.txt</code>"},{"location":"coding/llm_based_translation/#class-diagram","title":"Class diagram","text":"<p>The translation system follows a modular, agent-based architecture with three main components:</p> <pre><code>TranslatorToFlinkSqlAgent (Base Class)\n\u251c\u2500\u2500 SparkToFlinkSqlAgent (Spark SQL \u2192 Flink SQL)\n\u2514\u2500\u2500 KsqlToFlinkSqlAgent (KSQL \u2192 Flink SQL)\n</code></pre> <p></p>"},{"location":"coding/llm_based_translation/#translatortoflinksqlagent-translator_to_flink_sqlpy","title":"TranslatorToFlinkSqlAgent (<code>translator_to_flink_sql.py</code>)","text":"<p>Purpose: Base abstract class that defines the common interface and shared functionality for all SQL translation agents.</p> <p>Key Features:</p> <ul> <li>LLM Configuration: Supports multiple models (Qwen, Mistral, Cogito) with configurable endpoints</li> <li>Validation Pipeline: Integrates with Confluent Cloud for Apache Flink for live SQL validation</li> <li>Factory Pattern: Provides <code>get_or_build_sql_translator_agent()</code> for dynamic agent instantiation</li> <li>Error Handling: Base framework for iterative refinement when validation fails</li> </ul>"},{"location":"coding/llm_based_translation/#sparktoflinksqlagent-spark_sql_code_agentpy","title":"SparkToFlinkSqlAgent (<code>spark_sql_code_agent.py</code>)","text":"<p>Purpose: Specialized agent for translating Spark SQL to Flink SQL with enhanced error categorization and refinement.</p> <p>Translation Workflow:</p> <pre><code>Spark SQL Input\n    \u2193\nTranslation Agent (Spark \u2192 Flink DML)\n    \u2193\nDDL Generation Agent (DML \u2192 DDL)\n    \u2193\nPre-validation (Syntax Check)\n    \u2193\nConfluent Cloud Validation\n    \u2193\nIterative Refinement (if errors)\n    \u2193\nFinal Flink SQL (DDL + DML)\n</code></pre> <p>Key Features:</p> <ul> <li>Structured Responses: Uses Pydantic models for consistent LLM output parsing</li> <li>Error Categorization: Classifies errors into specific types (syntax, function incompatibility, type mismatch, etc.)</li> <li>Multi-step Validation: Pre-validation + live validation with up to 3 refinement iterations</li> <li>DDL Auto-generation: Automatically creates table definitions from query logic</li> <li>Validation History: Tracks all validation attempts for debugging</li> </ul> <p>Specialized Models: <pre><code>class SparkSqlFlinkDml(BaseModel):\n    flink_dml_output: str\n\nclass SparkSqlFlinkDdl(BaseModel):\n    flink_ddl_output: str\n    key_name: str\n\nclass SqlRefinement(BaseModel):\n    refined_ddl: str\n    refined_dml: str\n    explanation: str\n    changes_made: List[str]\n</code></pre></p>"},{"location":"coding/llm_based_translation/#ksqltoflinksqlagent-ksql_code_agentpy","title":"KsqlToFlinkSqlAgent (<code>ksql_code_agent.py</code>)","text":"<p>Purpose: Specialized agent for translating KSQL (Kafka SQL) to Flink SQL with multi-table support and comprehensive preprocessing.</p> <p>Translation Workflow: <pre><code>KSQL Input\n    \u2193\n1. Input Cleaning (Remove DROP statements, comments)\n    \u2193\n2. Table Detection (Identify multiple CREATE statements)\n    \u2193\n3. Individual Translation (Process each statement separately)\n    \u2193\n4. Mandatory Validation (Syntax + best practices)\n    \u2193\n5. Optional Live Validation (Confluent Cloud)\n    \u2193\nFinal Flink SQL Collections (DDL[] + DML[])\n</code></pre></p> <p>Key Features:</p> <ul> <li>Multi-table Processing: Automatically detects and processes multiple CREATE TABLE/STREAM statements</li> <li>Input Preprocessing: Removes problematic statements and comments that confuse LLMs</li> <li>Batch Translation: Handles complex KSQL scripts with multiple related statements</li> <li>Mandatory Validation: Always performs syntax and best practices validation</li> <li>File Snapshots: Saves intermediate results for debugging and tracking</li> </ul> <p>Specialized Models: <pre><code>class KsqlFlinkSql(BaseModel):\n    ksql_input: str\n    flink_ddl_output: Optional[str]\n    flink_dml_output: Optional[str]\n\nclass KsqlTableDetection(BaseModel):\n    has_multiple_tables: bool\n    table_statements: List[str]\n    description: str\n</code></pre></p>"},{"location":"coding/llm_based_translation/#a-test-bed","title":"A test bed","text":"<p>The current project includes in the <code>tests/data/</code> folder some examples of Spark and ksql scripts.</p> <pre><code>tests/data/ksql-project\n\u251c\u2500\u2500 common.mk\n\u251c\u2500\u2500 flink-references\n\u251c\u2500\u2500 sources\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aggregation.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-basic-table.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-bigger-file.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-filtering.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-g.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-geo.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-kpi-config-table.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-map_substr.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl-measure_alert.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dml-aggregate.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 filtering.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 geospacial.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 merge_tutorial.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 movements.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 splitter.ksql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 splitting_tutorial.ksql\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 w2_processing.ksql\n</code></pre> <p><code>flink-references</code> includes some migrated solutions used as reference for validating migrations.</p>"},{"location":"coding/llm_based_translation/#prerequisites-and-setup-for-developers","title":"Prerequisites and Setup for Developers","text":"<p>See the environment setup for developers section.</p> <ul> <li>If using <code>uv</code> as python package manager (developers of the shift left tool), install it using the documentation.</li> </ul>"},{"location":"coding/llm_based_translation/#using-cursorai","title":"Using Cursor.ai","text":"<p>If oyu have access to Cursor with  <code>cloud-4.5-sonnet</code> model, a prompt like: <pre><code>using @src/shift_left/shift_left/ai/prompts/ksql_fsql/translator.txt migrate the @src/shift_left/tests/data/ksql-project/sources/w2_processing.ksql  file\n</code></pre></p> <p>Will create a markdown file with the original ksql statements and the flink SQL matching statements at a higher speed than running qwen Ollama locally.</p>"},{"location":"coding/test_harness/","title":"Testing Flink Statement in isolation","text":"Version <p>Created March 21 - 2025  Updated Sept 24 -2025</p> <p>The goals of this chapter is to present the requirements, design and how to use the shift_left test harness commands for Flink statement validation in the context of Confluent Cloud Flink. The tool is packaged as a command in the <code>shift-left</code> CLI.</p> <pre><code>shift_left table --help\n\n# three features are available:\ninit-unit-tests   Initialize the unit test folder and template files for a given table. It will parse the SQL statements to create the insert statements for the unit tests. It is using the table inventory to find the table folder for the given table name.\nrun-unit-tests    Run all the unit tests or a specified test case by sending data to `_ut` topics and validating the results\ndelete-unit-tests      Delete the Flink statements and kafka topics used for unit tests for a given table.\n</code></pre> <p>See usage paragraph</p>"},{"location":"coding/test_harness/#context","title":"Context","text":"<p>We should differentiate two types of testing: Flink statement developer's tests, like unit testing of one flink statement, and Flink statement integration tests which group multiple Flink statements and process data end-to-end.</p> <p>The objectives of a test harness for developers and system testers, is to validate the quality of a new Flink SQL statement deployed on Confluent Cloud for Flink and therefore address the following needs:</p> <ol> <li>be able to deploy a unique flink statement under test (the ones we want to focus on are DMLs, or CTAS)</li> <li>be able to generate test data from the table definition and DML script content - with the developers being able to tune generated test data for each test cases.</li> <li> <p>produce synthetic test data for the n source tables using SQL insert statements or via csv files.</p> <p> </p> </li> <li> <p>validate test result by looking at records in the output table(s) of the Flink statement under test and applying conditions on data to claim the test failed or succeed. As an example:   <pre><code>with result_table as (\n  select * from fct_orders\n  where id = 'order_id_1' and account_name = 'account of bob'\n) \nSELECT CASE WHEN count(*)=1 THEN 'PASS' ELSE 'FAIL' END from result_table;\n</code></pre></p> </li> <li> <p>the flow of defining input data and validation scripts is a test case. The following Yaml definition, define one test with input and output SQL references:   <pre><code>- name: test_p5_dim_event_element_1\n  inputs:\n  - table_name: tenant_dim\n    file_name: ./tests/insert_tenant_dim_1.sql\n    file_type: sql\n  ...\n  outputs:\n  - table_name: p5_dim_event_element\n    file_name: ./tests/validate_p5_dim_event_element_1.sql\n    file_type: sql\n</code></pre></p> </li> <li> <p>Support multiple testcase definitions as a test suite. Test suite execution may be automated for non-regression testing to ensure continuous quality.</p> </li> <li>Once tests are completed, tear down tables and data.   <pre><code>shift_left table delete-unit-tests &lt;table-name&gt;\n</code></pre></li> <li>Do not impact other tables that may be used to do integration tests within the same Kafka Cluster. For that there is a postfix string add to the name of the tables. This postfix is defined in the config.yaml file as:   <pre><code>app:\n  post_fix_unit_test: _ut\n</code></pre>   This post_fix can be anything, but try to use very short string.</li> </ol> <p>The following diagram illustrates the global infrastructure deployment context:</p> Test Harness environment and scope <ul> <li>One the left, the developer's computer is used to run the test harness tool and send Flink statements to Confluent Cloud environment/ compute pool using the REST API. The Flink API key and secrets are used. </li> <li>The Flink statement under test is the same as the one going to production, except the tool may change the name of the source tables to use the specified postfix. The postfix is defined in the config.yaml file as <code>app.post_fix_unit_test</code> parameter.</li> <li>The green cylenders represent Kafka Topics which are mapped to Flink source and sink tables. They are defined specifically by the tool.</li> <li>As any tables created view Flink on Confluent Cloud have schema defined in schema registry, then schema context is used to avoid conflict within the same cluster. </li> </ul> <p>The following diagram illustrates the target unit testing environment:</p> Deployed Flink statements for unit testing"},{"location":"coding/test_harness/#unit-test-usage-and-recipes","title":"Unit-test Usage and Recipes","text":"<ul> <li> <p>Select a table to test the logic from. This test tool is relevant for DML with complex logic. In this example <code>c360_dim_users</code> has a join between two tables: <code>src_c360_users</code>, <code>c360_dim_groups</code>:   <pre><code>INSERT INTO c360_dim_users\n  with valid_users as (\n    SELECT * FROM src_c360_users\n    WHERE user_id IS NOT NULL and group_id IS NOT NULL and tenant_id IS NOT NULL\n  )\n    SELECT\n    -- columns hidden\n    FROM valid_users u\n    LEFT JOIN c360_dim_groups g\n    ON  u.tenant_id = g.tenant_id and u.group_id = g.group_id\n</code></pre></p> </li> <li> <p>Verify the ddl and dml files for the selected table are defined under <code>sql-scripts</code>, verify the table inventory exists and is up-to-date, if not run <code>shift_left table build-inventory $PIPELINES</code></p> </li> <li>Initialize the test code by running the following command: Do not create a lot of test cases upfront, you can add more tests later.   <pre><code>shift_left table init-unit-tests &lt;table_name&gt; --nb-test-cases 1 \n# example with the user_role (using the naming convention)\nshift_left table init-unit-tests c360_dim_users --nb-test-cases 1\n</code></pre></li> </ul> <p>To make it simple, it is recommended to define only one test case.</p> <ul> <li>For each input table, of the dml under test, there will be ddl and dml script files created with the numbered postfix to match the unit test, it supports. For example for the test case with id = 1 the name of the sql is:  <code>insert_c360_dim_groups_1.sql</code>) for inserting records to the table <code>c360_dim_groups</code>.  For each of those input tables, a foundation ddl is created to create the table with \"_ut\" postfix, this is used for test isolation: <code>ddl_c360_dim_groups.sql</code>, <code>ddl_src_c360_users.sql</code> <pre><code>dim_users\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 sql-scripts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.c360_dim_user.sql\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.c360_dim_user.sql\n\u251c\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl_c360_dim_groups.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl_src_c360_users.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 insert_c360_dim_groups_1.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 insert_c360_dim_groups_1.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 insert_src_c360_users_1.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 insert_src_c360_users_1.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test_definitions.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 validate_c360_dim_users_1.sql\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 validate_c360_dim_users_2.sql\n</code></pre></li> </ul> <p>Remarks: the yaml files are not yet used, it is a future extension</p> <p>The test cases are created as you can see in the <code>test_definitions.yaml</code> <pre><code>  foundations:\n  - table_name: c360_dim_groups\n    ddl_for_test: ./tests/ddl_c360_dim_groups.sql\n  - table_name: src_c360_users\n    ddl_for_test: ./tests/ddl_src_c360_users.sql\n  test_suite:\n  - name: test_c360_dim_users_1\n    inputs:\n    - table_name: c360_dim_groups\n      file_name: ./tests/insert_c360_dim_groups_1.sql\n      file_type: sql\n    - table_name: src_c360_users\n      file_name: ./tests/insert_src_c360_users_1.sql\n      file_type: sql\n    outputs:\n    - table_name: c360_dim_users\n      file_name: ./tests/validate_c360_dim_users_1.sql\n      file_type: sql\n</code></pre></p> <p>The two test cases use different approaches to define the data: SQL and CSV files. This is a more flexible solution, so the tool can inject data, as csv rows. The csv data may come from an extract of the kafka topic records.</p> <ul> <li>Data engineers update the content of the insert statements and the validation statements to reflect business requirements. Once done, try unit testing with the command:   <pre><code>shift_left table  run-unit-tests &lt;table_name&gt; --test-case-name test_&lt;table_name&gt;_1 \n</code></pre></li> </ul> <p>A test execution may take some time as it performs the following steps:</p> <ol> <li>Read the test definition.</li> <li>Execute the ddl for the input tables with '_ut' postfix or using the postfix as defined in the config.yaml parameter named: <code>app.post_fix_unit_test</code></li> <li>Insert records in input tables.</li> <li>Create a new output table with the specified postfix.</li> <li>Deploy the DML to test</li> <li>Deploy the validation SQL for each test case.</li> <li>Build test report</li> </ol> <p>This execution is re-entrant, which means it will NOT recreate the DDLs if the topics are already present, and will skip to the next step. </p> <p>Here is an example of outputs: 3 DDLs run successfuly, and the dml under tests is now running ready to generate results. <pre><code>------------------------------------------------------------\n1. Create foundation tables for unit tests for c360_dim_users\n------------------------------------------------------------\nMay execute foundation statement for c360_dim_groups /Users/jerome/Documents/Code/shift_left_utils/src/shift_left/tests/data/flink-project/pipelines/dimensions/c360/dim_users/./tests/ddl_c360_dim_groups.sql on lfcp-xvrvmz\nExecute statement dev-ddl-c360-dim-groups-jb  on: lfcp-xvrvmz\nExecuted statement for table: c360_dim_groups_jb status: COMPLETED\n\nMay execute foundation statement for src_c360_users /Users/jerome/Documents/Code/shift_left_utils/src/shift_left/tests/data/flink-project/pipelines/dimensions/c360/dim_users/./tests/ddl_src_c360_users.sql on lfcp-xvrvmz\nExecute statement dev-ddl-src-c360-users-jb  on: lfcp-xvrvmz\nExecuted statement for table: src_c360_users_jb status: COMPLETED\n\nExecute statement dev-ddl-c360-dim-users-jb  on: lfcp-xvrvmz\nExecuted statement for table: c360_dim_users_jb status: COMPLETED\n\nWait dev-dml-c360-dim-users-jb deployment, increase wait response timer to 20 seconds\nExecute statement dev-dml-c360-dim-users-jb  on: lfcp-xvrvmz\nExecuted statement for table: c360_dim_users_jb status: RUNNING\n</code></pre></p> <p>The second part is for the insert statements which should COMPLETE <pre><code>----------------------------------------\n2. Deploy insert into statements for unit test test_c360_dim_users_1\n----------------------------------------\nRun insert test data for c360_dim_groups_jb\nWait dev-ins-1-c360-dim-groups-jb deployment, increase wait response timer to 20 seconds\nExecute statement dev-ins-1-c360-dim-groups-jb  on: lfcp-xvrvmz\nExecuted statement for table: c360_dim_groups_jb status: COMPLETED\n\nRun insert test data for src_c360_users_jb\nExecute statement dev-ins-1-src-c360-users-jb  on: lfcp-xvrvmz\nExecuted statement for table: src_c360_users_jb status: COMPLETED\n</code></pre></p> <p>The tool is creating a <code>*-test-suite-result.json</code> file under the CLI session folder. </p> <ul> <li> <p>Run your validation script:   <pre><code>shift_left table run-validation-tests  &lt;table_name&gt; --test-case-name &lt;test_case_1&gt; \n</code></pre></p> </li> <li> <p>Clean the tests artifacts created on Confluent Cloud with the command:   <pre><code>shift_left table delete-unit-tests &lt;table_name&gt;\n</code></pre></p> </li> </ul>"},{"location":"coding/test_harness/#running-with-more-data","title":"Running with more data","text":"<p>The second test case created by the <code>shift_left table init-unit-tests</code> command uses a csv file to demonstrate how the tool can manage more data. It is possible to extract data from an existing topics, as csv file. The tool, as of now, is transforming the rows in the csv file as SQL insert values. </p> <p>In the future it could direcly write to a Kafka topics that are the input tables for the dml under test.</p> <p>Data engineers may use the csv format to create a lot of records. Now the challenge will be to define the validation SQL script, but this is another story.</p>"},{"location":"coding/test_harness/#unit-test-harness-faq","title":"Unit Test Harness FAQ","text":"What does 'foundation tables for unit tests' mean? <p>Each input tables for a given DML to tests are foundation tables, as the DDL of the current DML. So create foundation tables means running all the DDLs. These are run only one time. As an example this is the part of the test_definition that will be run: <pre><code>foundations:\n- table_name: c360_dim_groups\n  ddl_for_test: ./tests/ddl_c360_dim_groups.sql\n- table_name: src_c360_users\n  ddl_for_test: ./tests/ddl_src_c360_users.sql\n</code></pre> + the DDL of the <code>c360_dim_users</code> table as the name change.</p> Where are the logs? (the session folder) <p>When shift left is started, there is logs folder created under your <code>$HOME/.shift_left/logs</code> folder, and the name is specified at the beginning of the trace: <code>10-09-25-16-59-31-IZH7</code> <pre><code>--------------------------------------------------------------------------------\n| SHIFT_LEFT - CONFIG_FILE used: ./tests/config-ccloud.yaml - Session started at 2025-10-09 16:59:31 - LOGS folder is : /Users/jerome/.shift_left/logs/10-09-25-16-59-31-IZH7 |\n--------------------------------------------------------------------------------\n</code></pre> The logs is there with the name of the file = to <code>shift_left_cli.log</code>. Other file may be present, like the <code>*-test-suite-result.json</code></p> Should I change the name of the tables in the unit test scripts? <p>NO. The table names of the generated SQL are using _ut. This has to be persisted in the git repository. The table names are changed automatically when running the tests, and the tool uses the config.yaml <code>app.post_fix_unit_test</code> to overwrite, at deployment time, those names.</p> How to re-run my validation SQL? <p>Validation SQL script may take some time to tune and make it works. This is why the command: <code>shift_left table run-validation-tests  &lt;table_name&gt; --test-case-name &lt;test_case_1&gt;</code> is re-entrant and will reexecute the SQL. It will delete Flink previous validation statement.</p> When I discover the input data is not coherent, what can I do? <p>Better to delete all the test artifacts with the command <pre><code>shift_left table delete-unit-tests &lt;table_name&gt;\n</code></pre> and then re-run the unit test.</p> Is it possible to have two validation scripts for the same input? <p>Yes, and it is even recommended to simplify the validation SQLs as they become big. The test_definition supports this well as you can just specify the same input sqls and use another validation reference. It will be in another test case element. <pre><code>  test_suite:\n  - name: test_c360_dim_users_1\n    inputs:\n    - table_name: c360_dim_groups\n      file_name: ./tests/insert_c360_dim_groups_1.sql\n      file_type: sql\n    - table_name: src_c360_users\n      file_name: ./tests/insert_src_c360_users_1.sql\n      file_type: sql\n    outputs:\n    - table_name: c360_dim_users\n      file_name: ./tests/validate_c360_dim_users_1.sql\n      file_type: sql\n  - name: test_c360_dim_users_2\n    inputs:\n    - table_name: c360_dim_groups\n      file_name: ./tests/insert_c360_dim_groups_1.sql\n      file_type: sql\n    - table_name: src_c360_users\n      file_name: ./tests/insert_src_c360_users_1.sql\n      file_type: sql\n    outputs:\n    - table_name: c360_dim_users\n      file_name: ./tests/validate_c360_dim_users_2.sql\n      file_type: sql\n</code></pre></p>"},{"location":"coding/test_harness/#integration-tests","title":"Integration tests","text":""},{"location":"coding/test_harness/#context_1","title":"Context","text":"<p>The logic of integration tests is to validate end-to-end processing for a given pipeline and assess the time to process records from sources to facts or sink tables. Integration tests are designed to test end-to-end data flow across multiple tables and pipelines within a project. This is inherently a project-level concern and not a pipelines concern.</p> <p>The approach is to keep those integration tests at the same level as the <code>pipelines</code> folder of the project, but organize the tests by data product. As an example for the data product, <code>c360</code>, and the analytical data build from the <code>fact_users</code>, the hierarchy will look like:</p> <pre><code>pipelines\ntests\n  \u2514\u2500\u2500 c360\n      \u2514\u2500\u2500 fact_users\n</code></pre> <p>The content of the folder will include all the insert statements for the raw topics of the pipeline and the validation SQLs for intermediates and facts. </p> <p>The synthetic data are injected at the raw topics with unique identifier and time stamp so it will be easier to develop the validation script and compute the end-to-end latency:</p> <p>The following figure illustrates those principles:</p> <p></p> <p>The data to build is for the sink <code>F</code>, so integration tests will validate all the purple Flink statements which are ancestors to <code>F</code>. The integration tests insert data for the two input raw topics used to build the <code>src_x</code> ans <code>src_y</code>.</p> <p>Intermediate validations may be added to assess the state of the intermediate Flink statement output, but the most important one is the SQL validation of the output of <code>F</code>. </p> <p>The tool supports to create insert SQLs and the last validation script. But there are challenges to address. It was assessed that we could use Kafka header to add to metadata attribute: a unique id and a timestamp. The classical way to do so, is to alter the raw tables with:</p> <pre><code>ALTER TABLE raw_groups add headers MAP&lt;STRING, STRING&gt; METADATA;\n</code></pre> <p>Then each input statement to the raw topic includes a map construct to define the correlation id and the timestamp:</p> <pre><code>INSERT INTO raw_users (user_id, user_name, user_email, group_id, tenant_id, created_date, is_active, headers) VALUES\n-- User 1: Initial record\n('user_001', 'Alice Johnson', 'alice.johnson@example.com', 'admin', 'tenant_id_001', '2023-01-15', true, map('cor_id', 'cor_01', 'timestamp', now()));\n</code></pre> <p>but this approach means we need to modify all the intermediate Flink statements to pass those metadata to their output table and the declare the headers in the DDL of the output table(s). </p> <pre><code>insert to output_table\nselect\n  -- ... columns to match output_table\n  headers\nfrom final_table;\n</code></pre> <p>Also at each intermediate statement there will be the following challenges to address:</p> <ul> <li>On any join, which correlation id to use, or does a concatenation approach being used?</li> <li> <p>Which timestamp to use from the two tables joined? min or max?   <pre><code>select\n  -- ..\n  max(a.headers.timestamp, b.header.timestamp)\nfrom table_a a\nleft join table_b b on a.condition = b.condition\n</code></pre></p> </li> <li> <p>Finally how to ensure that, at each table, records are created to the output table(s): it is possible that input record may be filtered out, and not output record is created, meaning the latency is becoming infinite.</p> </li> </ul> <p>So the solution is to adapt and use existing fields in the input to set a <code>cor_id</code> and a <code>timestamp</code>. </p> <p>Instead if generating a timestamp. when the raw_topic is the outcome of Debezium CDC, there is a the <code>ts_ms</code> field that can be used as timestamp, but it also needs to be propagated down the sink.</p>"},{"location":"coding/test_harness/#initialize-the-integration-test","title":"Initialize the integration test","text":"<p>The command to create a scaffolding:</p> <pre><code>shift_left project init-integration-tests F\n</code></pre>"},{"location":"coding/test_harness/#running-the-integration-tests","title":"Running the integration tests:","text":"<pre><code>shift_left project run-integration-tests F\n</code></pre>"},{"location":"coding/test_harness/#tearsdown","title":"Tearsdown:","text":"<pre><code>shift_left project delete-integration-tests F\n</code></pre> <p>With these capabilities, we can also assess the time to process records from source to sink tables.</p>"},{"location":"coding/test_harness/#feature-tracking","title":"Feature tracking","text":"<ul> <li> init tests folder, with data product and sink table folder</li> <li> For test isolation in shared environment and cluster, the name of the table will have a postfix defined in config.yaml and defaulted with <code>_it</code></li> <li> get all alter table for the raw tables</li> <li> get all the insert synthetic data for raw_table</li> <li> build a validation SQL query to validate the message arrive and compute a delta, insert this to a it_test_topic</li> <li> Support Kafka consumer created for output sink table</li> </ul>"},{"location":"coding/test_harness/#unit-test-harness-code-explanation","title":"Unit Test Harness Code Explanation","text":"<p>The classes to support unit tests processing are:</p> <p></p>"},{"location":"mcp/","title":"Shift Left MCP Server Integration","text":"<p>The <code>src/shift_left/shift_left_mcp</code> directory contains the MCP (Model Context Protocol) server integration for the <code>shift_left</code> CLI tool, enabling direct integration with any MCP client like Cursor AI .</p>"},{"location":"mcp/#what-is-mcp","title":"What is MCP?","text":"<p>MCP (Model Context Protocol) is a protocol that allows AI assistants like Cursor to call external tools and services. This integration exposes all shift_left CLI commands as callable tools within Cursor.</p>"},{"location":"mcp/#quick-start","title":"Quick Start","text":""},{"location":"mcp/#install-dependencies","title":"Install Dependencies","text":"<p>Using <code>uv</code> (recommended):</p> <pre><code>cd shift_left_utils/src/shift_left\nuv add mcp\n</code></pre> <p>Or using pip:</p> <pre><code>pip install mcp\n</code></pre>"},{"location":"mcp/#test-the-mcp-server","title":"Test the MCP Server","text":"<p>This is optional but helps to verify the MCP server will work in your environment:</p> <pre><code>in shift_left_utils/src/shift_left\nuv run python -m shift_left_mcp.test_server\n</code></pre>"},{"location":"mcp/#set-environment-variables","title":"Set environment variables","text":"<p>Environment variables must be set before starting Cursor.</p>"},{"location":"mcp/#configure-cursor","title":"Configure Cursor","text":"<ol> <li> <p>Modify the Cursor mcp configuration: Cursor &gt; Settings &gt; Cursor Settings, then select <code>Tools &amp; MCP</code>. Add the mcp configuration but change the cwd path below:     <pre><code>  \"mcpServers\": {\n        \"shift-left-tools\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"run\",\n                \"python\",\n                \"-m\",\n                \"shift_left_mcp\"\n            ],\n            \"cwd\": \"/Users/jerome/Documents/Code/shift_left_utils/src/shift_left\",\n            \"env\": {\n                \"PIPELINES\": \"${env:PIPELINES}\",\n                \"CONFIG_FILE\": \"${env:CONFIG_FILE}\",\n                \"STAGING\": \"${env:STAGING}\",\n                \"SL_CONFLUENT_CLOUD_API_KEY\": \"${env:SL_CONFLUENT_CLOUD_API_KEY}\",\n                \"SL_CONFLUENT_CLOUD_API_SECRET\": \"${env:SL_CONFLUENT_CLOUD_API_SECRET}\",\n                \"SL_FLINK_API_KEY\": \"${env:SL_FLINK_API_KEY}\",\n                \"SL_FLINK_API_SECRET\": \"${env:SL_FLINK_API_SECRET}\",\n                \"SL_KAFKA_API_KEY\": \"${env:SL_KAFKA_API_KEY}\",\n                \"SL_KAFKA_API_SECRET\": \"${env:SL_KAFKA_API_SECRET}\"\n            }\n        }\n    }\n</code></pre></p> </li> <li> <p>Be sure to define the environment variables before re-starting Cursor. Include the <code>SL_*</code> variables to connect to Confluent Cloud. Use a shell script to export those variables and source this script before starting Cursor, or VScode.</p> </li> <li>Restart Cursor: Press Cmd+Q (don't just close the window)</li> <li>Start Cursor from this terminal     <pre><code>open -a Cursor\n</code></pre></li> <li>In Cursor: \"What is the version of shift_left?\"</li> </ol>"},{"location":"mcp/#potential-problem","title":"Potential Problem","text":"<ul> <li>When using prompts like \"what is the version of shift left\" or \"list my kafka topics\" in Cursor, the LLM tries to answer directly instead of calling the MCP tools.</li> </ul> <p>Check View &gt; Output &gt; MCP for connection status.</p> <p>This may come from:</p> <ol> <li>MCP server not configured in Cursor settings</li> <li>Environment variables not set before Cursor starts</li> <li> <p>Cursor not restarted after configuration changes, and started from a Terminal with environment variables set.</p> </li> <li> <p>Tools Not Being Called</p> </li> </ol> <p>The MCP server registered but LLM not recognizing tool calling opportunity. Verify output, try to restart Cursor</p>"},{"location":"mcp/#example-prompts","title":"Example Prompts","text":"<p>Project Management: - \"Initialize a new Kimball project called 'analytics' in ./workspace\" - \"List all Kafka topics in my project at ./my_project\" - \"Show me the available Flink compute pools\" - \"What files changed between main and my branch?\"</p> <p>Table Management: - \"Create a new table called 'customer_events' in ./pipelines/facts\" - \"Build the table inventory for ./pipelines\" - \"Migrate this Spark SQL file to Flink SQL: ./src/customer.sql\" - \"Create unit tests for the fact_orders table\"</p> <p>Pipeline Deployment: - \"Deploy the customer_orders table from ./pipelines inventory\" - \"Deploy all tables in the sales product\" - \"Build metadata for the DML file at ./pipelines/facts/fact_sales/dml.sql\"</p> <p>Cursor will: 1. Understand your intent 2. Automatically select the right shift_left tool 3. Fill in the parameters 4. Execute the command 5. Show you the results</p>"},{"location":"mcp/#architecture","title":"Architecture","text":"<p>The MCP server is organized as a Python package under <code>src/shift_left/shift_left_mcp/</code>:</p> <pre><code>shift_left_mcp/\n\u251c\u2500\u2500 __init__.py          # Package initialization\n\u251c\u2500\u2500 __main__.py          # Entry point (python -m shift_left_mcp)\n\u251c\u2500\u2500 server.py            # MCP server implementation\n\u251c\u2500\u2500 tools.py             # Tool definitions\n\u251c\u2500\u2500 command_builder.py   # CLI command builder\n\u2514\u2500\u2500 test_server.py       # Test suite\n</code></pre>"},{"location":"mcp/#available-tools","title":"Available Tools","text":"<p>The MCP server exposes the following shift_left commands:</p>"},{"location":"mcp/#project-management","title":"Project Management","text":"<ul> <li><code>shift_left_project_init</code> - Initialize new Flink project</li> <li><code>shift_left_project_validate_config</code> - Validate configuration</li> <li><code>shift_left_project_list_topics</code> - List Kafka topics</li> <li><code>shift_left_project_list_compute_pools</code> - List Flink compute pools</li> <li><code>shift_left_project_list_modified_files</code> - Track git changes</li> </ul>"},{"location":"mcp/#table-management","title":"Table Management","text":"<ul> <li><code>shift_left_table_init</code> - Create table structure</li> <li><code>shift_left_table_build_inventory</code> - Build table inventory</li> <li><code>shift_left_table_migrate</code> - Migrate SQL with AI</li> <li><code>shift_left_table_init_unit_tests</code> - Initialize unit tests</li> <li><code>shift_left_table_run_unit_tests</code> - Run unit tests</li> <li><code>shift_left_table_delete_unit_tests</code> - Delete unit tests</li> </ul>"},{"location":"mcp/#pipeline-management","title":"Pipeline Management","text":"<ul> <li><code>shift_left_pipeline_deploy</code> - Deploy Flink pipelines</li> <li><code>shift_left_pipeline_build_metadata</code> - Build pipeline metadata</li> <li><code>shift_left_pipeline_field_lineage</code> - Compute field-level lineage for a table (by name) up to sources; saves lineage.json and field_lineage.html under $HOME/.shift_left/field_lineage</li> </ul>"},{"location":"mcp/#utility","title":"Utility","text":"<ul> <li><code>shift_left_version</code> - Show CLI version</li> </ul>"},{"location":"mcp/#running-the-server","title":"Running the Server","text":"<p>The server can be run in several ways:</p>"},{"location":"mcp/#as-a-python-module-recommended","title":"As a Python Module (Recommended)","text":"<pre><code>uv run python -m shift_left_mcp\n</code></pre>"},{"location":"mcp/#directly","title":"Directly","text":"<pre><code>uv run python src/shift_left/shift_left_mcp/server.py\n</code></pre>"},{"location":"mcp/#via-cursor-automatic","title":"Via Cursor (Automatic)","text":"<p>Once configured in Cursor, the server runs automatically when needed.</p>"},{"location":"mcp/#usage-in-cursor","title":"Usage in Cursor","text":"<p>Once configured, use natural language in Cursor:</p> <p>Examples: - \"Initialize a new Kimball project called 'analytics' in ./workspace\" - \"List all Kafka topics in the current project\" - \"Deploy the customer_orders table\" - \"Migrate this Spark SQL file to Flink SQL\" - \"Create unit tests for the fact_sales table\"</p> <p>Cursor automatically: 1. Detects when shift_left is needed 2. Calls the appropriate MCP tool 3. Displays results 4. Handles errors gracefully</p>"},{"location":"mcp/#configuration","title":"Configuration","text":""},{"location":"mcp/#environment-variables","title":"Environment Variables","text":"<p>The MCP server respects standard shift_left environment variables: - <code>PIPELINES</code> - Main pipeline directory - <code>SRC_FOLDER</code> - Source SQL files directory - <code>STAGING</code> - Staging area for migrations - <code>CONFIG_FILE</code> - Configuration file path</p> <p>Set these in your shell before starting Cursor.</p>"},{"location":"mcp/#development","title":"Development","text":""},{"location":"mcp/#adding-new-tools","title":"Adding New Tools","text":"<ol> <li>Add tool definition to <code>tools.py</code></li> <li>Add command mapping to <code>command_builder.py</code></li> <li>Add test case to <code>test_server.py</code></li> <li>Run tests to verify</li> <li>Update documentation</li> </ol>"},{"location":"mcp/#debugging","title":"Debugging","text":"<p>Enable verbose logging by modifying <code>server.py</code>:</p> <p><pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> View logs in Cursor's Developer Tools (View \u2192 Toggle Developer Tools).</p>"},{"location":"mcp/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/#shift_left-not-found","title":"shift_left Not Found","text":"<p>Ensure the working directory is set correctly in Cursor configuration: <pre><code>\"cwd\": \"/Users/jerome/Documents/Code/shift_left_utils/src/shift_left\"\n</code></pre></p>"},{"location":"mcp/#command-failures","title":"Command Failures","text":"<ol> <li>Test shift_left CLI directly: <code>shift_left version</code></li> <li>Check environment variables are set</li> <li>Verify credentials for Confluent Cloud</li> <li>Review error messages in Cursor output</li> </ol>"},{"location":"mcp/#version","title":"Version","text":"<p>MCP Server Version: 0.1.0 Compatible with: shift_left CLI 0.1.41+</p>"},{"location":"mcp/#mcp-usage-examples-in-cursor","title":"MCP Usage Examples in Cursor","text":"<p>Once you've configured the shift_left MCP server in Cursor, you can use natural language to execute shift_left commands. Here are practical examples:</p>"},{"location":"mcp/#project-management_1","title":"Project Management","text":""},{"location":"mcp/#initialize-a-new-project","title":"Initialize a New Project","text":"<p>You say:</p> <p>\"Create a new Flink project called 'customer_analytics' in my workspace folder using the Kimball structure\"</p> <p>Cursor will: - Call <code>shift_left_project_init</code> - Pass: project_name=\"customer_analytics\", project_path=\"./workspace\", project_type=\"kimball\" - Show the created project structure</p>"},{"location":"mcp/#list-kafka-topics","title":"List Kafka Topics","text":"<p>You say:</p> <p>\"Show me all the Kafka topics in my current project\"</p> <p>Cursor will: - Call <code>shift_left_project_list_topics</code> - Pass: project_path=\".\" - Display all available topics</p>"},{"location":"mcp/#check-compute-pools","title":"Check Compute Pools","text":"<p>You say:</p> <p>\"What Flink compute pools are available in environment env-abc123?\"</p> <p>Cursor will: - Call <code>shift_left_project_list_compute_pools</code> - Pass: environment_id=\"env-abc123\" - List all compute pools with details</p>"},{"location":"mcp/#track-changes-for-blue-green-deployment","title":"Track Changes for Blue-Green Deployment","text":"<p>You say:</p> <p>\"What SQL files changed between main branch and my current branch?\"</p> <p>Cursor will: - Call <code>shift_left_project_list_modified_files</code> - Pass: branch_name=\"main\", file_filter=\".sql\" - Show modified files for targeted deployment</p>"},{"location":"mcp/#table-management_1","title":"Table Management","text":""},{"location":"mcp/#create-a-new-table","title":"Create a New Table","text":"<p>You say:</p> <p>\"Create a new table called 'customer_orders' in the facts folder under the sales product\"</p> <p>Cursor will: - Call <code>shift_left_table_init</code> - Pass: table_name=\"customer_orders\", table_path=\"./pipelines/facts\", product_name=\"sales\" - Create the complete table structure</p>"},{"location":"mcp/#build-table-inventory","title":"Build Table Inventory","text":"<p>You say:</p> <p>\"Build an inventory of all tables in my pipelines directory\"</p> <p>Cursor will: - Call <code>shift_left_table_build_inventory</code> - Pass: pipeline_path=\"./pipelines\" - Generate inventory with metadata for all tables</p>"},{"location":"mcp/#migrate-sql-code","title":"Migrate SQL Code","text":"<p>You say:</p> <p>\"Migrate this Spark SQL file to Flink SQL: /path/to/customer_transform.sql, name it customer_dim, and put it in ./staging. Also validate the result.\"</p> <p>Cursor will: - Call <code>shift_left_table_migrate</code> - Pass: table_name=\"customer_dim\", sql_src_file_name=\"/path/to/customer_transform.sql\", target_path=\"./staging\", source_type=\"spark\", validate=true - Perform AI-powered migration and validation</p>"},{"location":"mcp/#unit-testing","title":"Unit Testing","text":""},{"location":"mcp/#initialize-unit-tests","title":"Initialize Unit Tests","text":"<p>You say:</p> <p>\"Create unit tests for the fact_sales table\"</p> <p>Cursor will: - Call <code>shift_left_table_init_unit_tests</code> - Pass: table_name=\"fact_sales\" - Generate test structure and files</p>"},{"location":"mcp/#run-unit-tests","title":"Run Unit Tests","text":"<p>You say:</p> <p>\"Run the unit tests for customer_dim\"</p> <p>Cursor will: - Call <code>shift_left_table_run_unit_tests</code> - Pass: table_name=\"customer_dim\" - Execute tests and show results</p>"},{"location":"mcp/#clean-up-tests","title":"Clean Up Tests","text":"<p>You say:</p> <p>\"Remove the unit test artifacts for fact_orders from Confluent Cloud\"</p> <p>Cursor will: - Call <code>shift_left_table_delete_unit_tests</code> - Pass: table_name=\"fact_orders\" - Clean up test resources</p>"},{"location":"mcp/#pipeline-deployment","title":"Pipeline Deployment","text":""},{"location":"mcp/#deploy-a-single-table","title":"Deploy a Single Table","text":"<p>You say:</p> <p>\"Deploy the customer_orders table from the pipelines inventory\"</p> <p>Cursor will: - Call <code>shift_left_pipeline_deploy</code> - Pass: inventory_path=\"./pipelines\", table_name=\"customer_orders\" - Deploy DDL and DML with dependency management</p>"},{"location":"mcp/#deploy-by-product","title":"Deploy by Product","text":"<p>You say:</p> <p>\"Deploy all tables in the sales product to compute pool lfcp-abc123\"</p> <p>Cursor will: - Call <code>shift_left_pipeline_deploy</code> - Pass: inventory_path=\"./pipelines\", product_name=\"sales\", compute_pool_id=\"lfcp-abc123\" - Deploy all tables in the product</p>"},{"location":"mcp/#deploy-dml-only","title":"Deploy DML Only","text":"<p>You say:</p> <p>\"Redeploy just the DML for fact_sales, not the DDL\"</p> <p>Cursor will: - Call <code>shift_left_pipeline_deploy</code> - Pass: inventory_path=\"./pipelines\", table_name=\"fact_sales\", dml_only=true - Deploy only the INSERT/SELECT statement</p>"},{"location":"mcp/#blue-green-deployment","title":"Blue-Green Deployment","text":"<p>You say:</p> <p>\"Deploy only the tables that changed, listed in modified_tables.txt\"</p> <p>Cursor will: - Call <code>shift_left_pipeline_deploy</code> - Pass: inventory_path=\"./pipelines\", table_list_file_name=\"modified_tables.txt\" - Deploy only changed tables in dependency order</p>"},{"location":"mcp/#parallel-deployment","title":"Parallel Deployment","text":"<p>You say:</p> <p>\"Deploy all tables in the analytics product in parallel\"</p> <p>Cursor will: - Call <code>shift_left_pipeline_deploy</code> - Pass: inventory_path=\"./pipelines\", product_name=\"analytics\", parallel=true - Deploy tables concurrently where dependencies allow</p>"},{"location":"mcp/#build-pipeline-metadata","title":"Build Pipeline Metadata","text":"<p>You say:</p> <p>\"Generate metadata for the DML file at ./pipelines/facts/fact_sales/dml.fact_sales.sql\"</p> <p>Cursor will: - Call <code>shift_left_pipeline_build_metadata</code> - Pass: dml_file_name=\"./pipelines/facts/fact_sales/dml.fact_sales.sql\", pipeline_path=\"./pipelines\" - Extract and save pipeline metadata</p>"},{"location":"mcp/#utility-commands","title":"Utility Commands","text":""},{"location":"mcp/#check-version","title":"Check Version","text":"<p>You say:</p> <p>\"What version of shift_left is installed?\"</p> <p>Cursor will: - Call <code>shift_left_version</code> - Display version information</p>"},{"location":"mcp/#validate-configuration","title":"Validate Configuration","text":"<p>You say:</p> <p>\"Check if my shift_left configuration is valid\"</p> <p>Cursor will: - Call <code>shift_left_project_validate_config</code> - Validate and report any configuration issues</p>"},{"location":"mcp/#complex-workflows","title":"Complex Workflows","text":""},{"location":"mcp/#complete-migration-workflow","title":"Complete Migration Workflow","text":"<p>You say:</p> <p>\"I need to migrate all Spark SQL files in ./src/spark/ to Flink, validate them, and create the table structures in ./pipelines/staging/\"</p> <p>Cursor might: 1. List files in ./src/spark/ 2. For each file, call <code>shift_left_table_migrate</code> with validation 3. Show results and any errors 4. Suggest next steps</p>"},{"location":"mcp/#blue-green-deployment-workflow","title":"Blue-Green Deployment Workflow","text":"<p>You say:</p> <p>\"Show me what changed since main branch, then deploy only those tables\"</p> <p>Cursor will: 1. Call <code>shift_left_project_list_modified_files</code> with branch_name=\"main\" 2. Save results to a file 3. Call <code>shift_left_pipeline_deploy</code> with table_list_file_name pointing to that file 4. Show deployment progress and results</p>"},{"location":"mcp/#full-table-lifecycle","title":"Full Table Lifecycle","text":"<p>You say:</p> <p>\"Create a new table called 'order_events' in facts, create unit tests for it, then deploy it\"</p> <p>Cursor will: 1. Call <code>shift_left_table_init</code> 2. Call <code>shift_left_table_init_unit_tests</code> 3. Call <code>shift_left_pipeline_deploy</code> 4. Report success at each stage</p>"},{"location":"mcp/#context-aware-assistance","title":"Context-Aware Assistance","text":"<p>Cursor can use file context too:</p> <p>Scenario: You have a Spark SQL file open</p> <p>You say:</p> <p>\"Migrate this file to Flink SQL\"</p> <p>Cursor will: - Detect the file path from context - Infer source_type=\"spark\" - Ask for table name and target path if needed - Execute the migration</p>"},{"location":"mcp/#tips-for-best-results","title":"Tips for Best Results","text":"<ol> <li>Be specific about paths: Use absolute or clear relative paths</li> <li>Mention product names: Helps organize multi-product projects</li> <li>Specify validation: Say \"validate\" when migrating to catch issues early</li> <li>Use environment variables: Reference $PIPELINES when configured</li> <li>Ask for explanations: \"Explain what this command will do before running it\"</li> </ol>"},{"location":"mcp/#error-handling","title":"Error Handling","text":"<p>If something fails, Cursor will: - Show the error message from shift_left - Suggest fixes based on common issues - Help you retry with corrected parameters</p> <p>Example:</p> <p>\"Deploy fact_sales table\"</p> <p>If it fails: - Cursor shows: \"Configuration missing compute_pool_id\" - Suggests: \"Try: Deploy to compute pool lfcp-xyz123\" - Or: \"Set COMPUTE_POOL_ID environment variable\"</p>"},{"location":"mcp/#getting-help","title":"Getting Help","text":"<p>You can ask: - \"What shift_left commands are available?\" - \"Show me examples of deploying a table\" - \"How do I set up unit tests?\" - \"What's the syntax for migrating from Spark to Flink?\"</p> <p>Cursor will explain and show examples!</p>"},{"location":"mcp/#advanced-combining-with-file-operations","title":"Advanced: Combining with File Operations","text":"<p>You say:</p> <p>\"Read the DML file in ./pipelines/facts/fact_orders/, show me the dependencies, then deploy it\"</p> <p>Cursor will: 1. Read the file 2. Parse it to show table dependencies 3. Call <code>shift_left_pipeline_deploy</code> 4. Monitor the deployment</p>"},{"location":"tutorial/","title":"Shift Left Utils Tutorials","text":"<p>Managing Flink SQL project at scale can be a challenge. This set of tutorials will help you learning how to leverage shift_left tool to manage your Flink SQLs projects from initial setup, migration to Flink, day to day flink sqls management, deployment, testing, and quality assurance.</p> Tutorial Description Links Setup Lab Setup your environment for Flink SQLs management using shift_left cli Link Ksql to Flink SQL Migration AI Lab Migrate your KSQLs to Flink SQLs using AI Link Spark SQL to Flink SQL Migration AI Lab Migrate your Spark SQL to Flink SQLs using AI Link Project Management Lab Initialize a Flink project, add tables, unit tests, and admin commands Link Data Engineer day to day work - Lab A lab to review the classical Data Engineer work Link SRE CI/CD process and other tasks - Lab A lab to review the potential CI/CD activity and project deployment Link"},{"location":"tutorial/de_lab/","title":"Lab: Day to Day Data Engineer's Work","text":"<p>This lab presents a simple flow for standard activities Data Engineer may do to develop Confluent Flink Solution.</p>"},{"location":"tutorial/de_lab/#pre-requisite","title":"Pre-requisite","text":"<p>During this lab, you will work on an existing Flink project, therefore clone the repository:</p> <pre><code>git clone https://github.com/jbcodeforce/flink_project_demos.git\ncd flink_project_demos\n</code></pre> <ul> <li>Set your configuration file and environment variables as presented in the setup lab <pre><code>export PIPELINES=$FLINK_PROJECT/pipelines\nexport CONFIG_FILE=$FLINK_PROJECT/config.yaml\n</code></pre></li> </ul>"},{"location":"tutorial/de_lab/#create-table","title":"Create table","text":""},{"location":"tutorial/de_lab/#unit-test-table","title":"Unit test table","text":""},{"location":"tutorial/de_lab/#assess-pipeline","title":"Assess Pipeline","text":"<ul> <li>Start from a white page: The pipeline_definition.json files are, per table and local to your local folder. It may possible those files were created in git. But it is recommended to clean your local copy:     <pre><code>shift_left pipeline delete-all-metadata\n</code></pre></li> </ul> <p>Recall that it will work from $PIPELINES folder.</p> <ul> <li>Rebuild the metadata with your local work.</li> </ul>"},{"location":"tutorial/de_lab/#deploy-one-to-many-tables","title":"Deploy one to many tables","text":""},{"location":"tutorial/migration_ai_lab/","title":"Lab: Migration using AI","text":"<p>The current AI based migration implementation supported by this tool enables migration of:</p> <ul> <li>dbt/Spark SQL to Flink SQL</li> <li>ksqlDB to Flink SQL</li> </ul> <p>The approach uses LLM agents local or remote. After this lab you should be able to use the <code>shift_left</code> tool to partially automate your SQL migration to Flink SQL.</p> <p>The core idea is to leverage LLMs to understand the source SQL semantics and to translate them to Flink SQLs. </p> <p>This is github repositiory is not production ready, the LLM can generate hallucinations, and one to one mapping between source like ksqlDB or Spark to Flink is sometime not the best approach. We expect that this agentic solution could be a strong foundation for better results, and can be enhanced over time.</p> <p>Migration is a one time shot, and should not be a practice to develop Flink solution.</p> Lab Environment <p>The Lab was developed and tested on Mac. <code>shift_tool</code> runs on Mac, Linux and Windows WSL.</p>"},{"location":"tutorial/migration_ai_lab/#prerequisites","title":"Prerequisites","text":"<p>Be sure to have done the Setup Lab to get shift_left cli operational. For the AI based migration the following needs to be done:</p> <ol> <li>A computer with at least 20GB of Free RAM, with GPU - (All development was done on MAC M3 Pro 36GB )</li> <li> <p>Install Ollama <pre><code># Verify the ollama cli:\nollama list \nollama --help\n</code></pre></p> </li> <li> <p>Download the <code>qwen3-coder:30b</code> model:     <pre><code>ollama pull qwen3-coder:30b\n</code></pre></p> </li> <li> <p>Add or update the following environments variables in your <code>set_env_var</code> <pre><code>export SL_LLM_BASE_URL=http://localhost:11434/v1\nexport SL_LLM_MODEL=qwen3-coder:30b\nexport SL_LLM_API_KEY=not_needed_key\n# and the following variables will be use to control `confluent` cli during statement deployment\nexport CCLOUD_ENV_ID=env-\nexport CCLOUD_ENV_NAME=\nexport CCLOUD_KAFKA_CLUSTER=&lt;name of the kafka cluster&gt;\nexport CLOUD_REGION=us-west-2\nexport CLOUD_PROVIDER=aws\nexport CCLOUD_CONTEXT=login-&lt;your email&gt;-https://confluent.cloud\nexport CCLOUD_COMPUTE_POOL_ID=&lt;compute pool id&gt;\n</code></pre></p> </li> <li> <p>Start Ollama server:     <pre><code>ollama serve\n</code></pre></p> </li> </ol>"},{"location":"tutorial/migration_ai_lab/#use-cloud-saas","title":"Use Cloud SaaS","text":"<p>If you are using a sevice like OpenAI, Anthropic, HuggingFace.hub, OpenRouter.ai , AWS Bedrock, use their API key and change the environment variable to refect openAI end points and key. There is also , where you can define an API key: https://openrouter.ai/, to get access to larger models, like <code>qwen/qwen3-coder:free</code> which is free to use for few requests per day (pricing conditions may change).</p>"},{"location":"tutorial/migration_ai_lab/#use-your-own-remote-service","title":"Use your own remote service","text":"<p>Some users have deployed an inference server to their own VPC. The IaC/tf_aws_c2 folder includes the terraform manifests to deploy an EC2 with GPU and ollama as an inference engine. The security group for AWS EC2 firewall is set to use the user IP address so only this machine can interact with the EC2 machine.</p> <p>To run the migration set the following environment variable:</p> <pre><code>export SL_LLM_BASE_URL=\"http://&lt;EC2_PUBLIC_IP&gt;:11434\"\n</code></pre> <p>Then run one of the  migration (see detail in next section):</p> <pre><code>shift_left table migrate  ...\n</code></pre>"},{"location":"tutorial/migration_ai_lab/#migration-context","title":"Migration Context","text":"<p>As described in the introduction, at a high level, data engineers need to take a source project, define a new Flink project, perform migrations, run Flink statement deployments, manage pipelines, and write and execute tests:</p> Shift Left project system context <p>For automatic migration, LLMs alone might not be sufficient to address complex translations in an automated process. Agents help by specializing in specific steps with feedback loops and retries.</p> Complexity of language translation <p>For any programming language translation, we need to start with a corpus of source code. This can be done programmatically from the source language, then for each generated code, implement the semantically equivalent Flink SQL counterparts.</p> <p>The goal of corpus creation is to identify common ksqlDB or Spark SQL constructs (joins, aggregations, window functions, UDFs, etc.), then manually translate a smaller, diverse set of queries to establish translation rules. Using these rules, we can generate permutations and variations of queries. It is crucial to test the generated Flink SQL against a test dataset to ensure semantic equivalence.</p> <p>Build query pairs to represent the source-to-target set as a corpus. For each query pair, include the relevant table schemas. This is vital for the LLM to understand data types, column names, and relationships. It is not recommended to have different prompts for different parts of a SQL statement, as the LLM's strength comes from the entire context. However, there will still be problems for SQL scripts that have many lines of code, as a 200+ line script will reach thousands of tokens.</p> <p>To improve result accuracy, it is possible to use Supervised Fine-tuning techniques:</p> <ul> <li>Fine-tune the chosen LLM on the generated code. The goal is for the LLM to learn the translation patterns and nuances between ksqlDB or Spark SQL and Flink SQL.</li> <li>Prompt Engineering: Experiment with different prompt structures during fine-tuning and inference. A good prompt will guide the LLM effectively. The current implementation leverages this type of prompt: e.g., \"Translate the following Spark SQL query to Flink SQL, considering the provided schema. Ensure semantic equivalence and valid Flink syntax.\"</li> <li>For evaluation assessment, it is recommended to add a step to the agentic workflow to validate the syntax of the generated Flink SQL. Better validation involves assessing semantic equivalence by determining if the Flink SQL query produces the same results as the ksqlDB or Spark SQL query on a given dataset.</li> </ul> <p>For validation, it may be relevant to have a knowledge base of common translation errors. When the Validation Agent reports an error, the Refinement Agent attempts to correct the Flink SQL. It might feed the error message back to the LLM with instructions to fix it. The knowledge base should be populated with human-curated rules for common translation pitfalls.</p> <p>It may be important to explain why a translation was done a certain way to better tune prompts. For complex queries or failures, human review (\"human in the loop\") and correction mechanisms will be essential, with the system learning from these corrections.</p>"},{"location":"tutorial/migration_ai_lab/#limitations","title":"Limitations","text":"<p>LLMs cannot magically translate custom UDFs. This will likely require manual intervention or a separate mapping mechanism. The system should identify and flag untranslatable UDFs.</p> <p>Flink excels at stateful stream processing. Spark SQL's batch orientation means that translating stateful Spark operations (if they exist) to their Flink streaming counterparts would be highly complex and would likely require significant human oversight or custom rules.</p>"},{"location":"tutorial/migration_ai_lab/#spark-sql-to-flink-sql","title":"Spark SQL to Flink SQL","text":"<p>While Spark SQL is primarily designed for batch processing, it can be migrated to Flink real-time processing with some refactoring and tuning. Spark also supports streaming via micro-batching. Most basic SQL operators (SELECT, FROM, WHERE, JOIN) are similar between Spark and Flink. Some Spark SQL built-in functions need different mapping to Flink built-in functions or may be some UDFs. </p> <ul> <li>Example command to migrate one Spark SQL script   <pre><code># set SRC_FOLDER to one of the spark source folder like tests/data/spark-project\n# set STAGING to the folder target to the migrated content\nshift_left table migrate customer_journey $SRC_FOLDER/sources/src_customer_journey.sql $STAGING --source-type spark\n</code></pre></li> </ul>"},{"location":"tutorial/migration_ai_lab/#ksqldb-to-flink-sql","title":"ksqlDB to Flink SQL","text":"<p>ksqlDB has SQL constructs to do stream processing, but this is not an ANSI SQL engine. It is highly integrated with Kafka and uses specific keywords to define such integration. LLM may have limited access to ksql code during the training, so results may not be optimal. </p> <p>The migration and prompts need to support more examples outside of the classical SELECT and CREATE TABLE statements.</p> <ul> <li>Example command to migrate one of ksqlDB script:   <pre><code>export FLINK_PROJECT=$HOME/Documents/Code/shift_left_utils/src/shift_left/tests/data/ksql-project/flink-project\nexport PIPELINES=$FLINK_PROJECT/pipelines\nexport STAGING=$FLINK_PROJECT/../staging\nexport SRC_FOLDER=$(pwd)/src/shift_left/tests/data/ksql-project/sources\n\nshift_left table migrate mig_test $SRC_FOLDER/ddl-measure_alert.ksql $STAGING --source-type ksql --product-name net\n</code></pre></li> </ul>"},{"location":"tutorial/migration_ai_lab/#migration-workflows","title":"Migration Workflows","text":"<p>We propose two different migrations to illustrate ksql to Flink SQL query migration using the Confluent KsqlDB tutorials and one Spark to Flink SQL.</p> <p>You should have cloned the <code>flink_project_demo</code> repository during the setup.</p>"},{"location":"tutorial/migration_ai_lab/#1-project-initialization","title":"1. Project Initialization","text":"<ul> <li>Start a Terminal</li> <li>Create a new Flink git project to keep your migrated Flink statements (e.g., my-flink-demo):</li> </ul> <pre><code># May be create a temporary folder\nmkdir $HOME/Code\n# Initialize project structure\nshift_left project init &lt;your-project&gt; &lt;/path/to/your/folder&gt;\n# example \nshift_left project init my-flink-demo $HOME/Code\n</code></pre> <p>You should get the following project structure, which represents a <code>start schema</code> structure. <pre><code>my-flink-demo\n\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 pipelines\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 common.mk\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dimensions\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 facts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 intermediates\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sources\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 views\n\u2514\u2500\u2500 staging\n.gitignore\n</code></pre></p> <ul> <li>Update the your environment variables, to reflect the project name you just created, taking as source the <code>set_env_var</code> file under tutorial/setup:     <pre><code># .... other secrets omitted\nexport FLINK_PROJECT=$PWD/../my-flink-demo/\nexport PIPELINES=$FLINK_PROJECT/pipelines\nexport STAGING=$FLINK_PROJECT/staging\nexport SRC_FOLDER=$PWD/../flink_project_demos/ksql_tutorial/sources\nexport CONFIG_FILE=$PWD/tutorial/setup/config.yaml\n</code></pre></li> <li>The deployment organization is illustrated in the following figure: The cloned flink_project_demo includes ksql sources queries to migrate, and referenced by the SRC_FOLDER environment variable, the tool will save migrated to Flink SQL files to the STAGING folder.</li> </ul> ksql project organization"},{"location":"tutorial/migration_ai_lab/#2-ksql-to-flink-sql-lab","title":"2. KSQL to Flink SQL Lab","text":"<p>The following steps will help you migrate some of the ksql Tutorial queries, as introduced by Confluent ksql Queries to Confluent Cloud Flink SQL. Those queries are defined as sources in the Flink project demonstration git repository </p>"},{"location":"tutorial/migration_ai_lab/#21-migration-executions","title":"2.1 Migration Executions","text":"<ul> <li> <p>Be sure environment variables are set in your Terminal session:     <pre><code>source tutorial/setup/set_env_var\n</code></pre></p> </li> <li> <p>Migrate a basic splitting ksql query     <pre><code>shift_left table migrate acting_events $SRC_FOLDER/routing/splitting.ksql $STAGING --source-type ksql \n</code></pre></p> <p>The command generates: <pre><code>my-flink-demo\n\u251c\u2500 staging/default/acting_events/sql-scripts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.acting_events_drama.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.acting_events_fantasy.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.acting_events_other.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dml.acting_events_drama.sql\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dml.acting_events_fantasy.sql\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.acting_events_other.sql\n</code></pre></p> <p>The <code>default</code> folder is the name of the data product, it can be set in the cli. We will see this in the next migration</p> </li> <li> <p>Looking at the shift_left logs: the shift_left cli create a folder in $HOME/.shilt_left.logs for each execution. The name of the file (e.g. <code>.shift_left/logs/12-02-25-18-09-34-i9kb</code> )is given at each execution trace:     <pre><code>---------------------------------------- SHIFT_LEFT 0.1.46 ----------------------------------------\n| CONFIG_FILE     : /Users/jerome/Documents/Code/my-flink-demo/config.yaml\n| LOGS folder     : /Users/jerome/.shift_left/logs/12-02-25-18-09-34-i9kb\n| Session started : 2025-12-02 18:09:34\n</code></pre></p> <p>It this then possible to see more details of the migration process.</p> </li> <li> <p>Run a second example:     <pre><code>shift_left table migrate shipped_orders $SRC_FOLDER/joins/stream_stream.ksql $STAGING --source-type ksql --product-name orders\n</code></pre></p> <p>which should create: <pre><code>orders\n\u2514\u2500\u2500 shipped_orders\n    \u251c\u2500\u2500 Makefile\n    \u251c\u2500\u2500 sql-scripts\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.orders.sql\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.shipments.sql\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.shipped_orders.sql\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.shipped_orders.sql\n    \u251c\u2500\u2500 tests\n    \u2514\u2500\u2500 tracking.md\n</code></pre></p> </li> </ul>"},{"location":"tutorial/migration_ai_lab/#22-validation-and-deployment","title":"2.2 Validation and Deployment","text":"<p>There are two ways to validate the deployment, using <code>confluent</code> cli or use the Confluent Cloud Flink Workspace</p> confluentWorkspace <pre><code>confluent login \n# Deploy to Confluent Cloud for Flink\ncd ${STAGING}/default/acting_events\n\n# Deploy DDL statements\nmake create_flink_ddl\n\n# Which may generate a trace like:\n+---------------+--------------------------------------+\n| Creation Date | 2025-12-03 02:25:39.17975            |\n|               | +0000 UTC                            |\n| Name          | dev-usw2-default-ddl-acting-events   |\n| Statement     | CREATE TABLE IF NOT EXISTS           |\n|               | acting_events (     name STRING,     |\n|               |     title STRING,     genre          |\n|               | STRING,     PRIMARY KEY (name)       |\n|               | NOT ENFORCED ) DISTRIBUTED BY        |\n|               | HASH(name) INTO 1 BUCKETS WITH (     |\n|               |    'changelog.mode' = 'append',      |\n|               |  'kafka.retention.time' = '0',       |\n|               | 'kafka.producer.compression.type' =  |\n|               | 'snappy',     'scan.bounded.mode' =  |\n|               | 'unbounded',     'scan.startup.mode' |\n|               | = 'earliest-offset',                 |\n|               | 'value.fields-include' = 'all',      |\n|               | 'value.json-registry.schema-context' |\n|               | = '.flink-dev',     'value.format' = |\n|               | 'json-registry' );                   |\n| Compute Pool  | lfcp-xvrvmz                          |\n| Status        | COMPLETED                            |\n| Status Detail | Command completed                    |\n|               | successfully.                        |\n+---------------+--------------------------------------+\n# Deploy DML statements  \nmake create_flink_dml\n</code></pre> <p>In the Flink Workspace copu/paste one of the ddl or dml </p> Known Issues <p>12/2/2025: The makefile creation needs to be improved as the LLM may generate multiple ddls and dmls. </p>"},{"location":"tutorial/migration_ai_lab/#3-spark-to-flink-sql-lab","title":"3. Spark to Flink SQL Lab","text":"<p>The concept is to take one of the fact table and migrate it.</p>"},{"location":"tutorial/migration_ai_lab/#31-change-environment-variable","title":"3.1 Change environment variable","text":"<p>Change the SRC_FOLGER to point to the existing Spark project about customer 360 analytics: (adapt the path below)</p> <pre><code>export SRC_FOLDER=$HOME/Documents/Code/flink_project_demos/customer_360/c360_spark_processing\n</code></pre>"},{"location":"tutorial/migration_ai_lab/#32-migrate-a-fact-table","title":"3.2 Migrate a fact table","text":"<p>Migrate the fact table:</p> <pre><code> shift_left table migrate fct_customer_360_profile  $SRC_FOLDER/facts/fct_customer_360_profile.sql  $STAGING --source-type spark --product-name c360\n</code></pre> <p>Which should create the following content:</p> <pre><code>staging\n\u251c\u2500\u2500c360\n    \u251c\u2500\u2500 fct_customer_360_profile\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 Makefile\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 sql-scripts\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.support_tickets_raw.sql\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.fct_customer_360_profile.sql\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 tests\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tracking.md\n</code></pre>"},{"location":"tutorial/migration_ai_lab/#33-validate-on-confluent-cloud","title":"3.3 Validate on Confluent Cloud","text":"<p>Same as above Flink Workspace or confluent cli may be used for deployment.</p>"},{"location":"tutorial/migration_ai_lab/#4-prepare-for-pipeline-management","title":"4. Prepare for pipeline management","text":"<p>When the migration is done, and validated with via statement deployment, it is important to move from Staging to Pipelines. As all the table management will be done by shift_left from the pipeline management.</p> <p>The move will be done depending of where the flink sql is a source processing, a dimension, a fact or event a view. Most of the shift left pipelines are processing raw data from topics created by CDC processes. Those topics are sources to the sources processing.</p> <p>The following rule of thumb can be used:</p> Type of processing Candidate Folder Deduplication, filtering sources Joins between sources dimensions Joins between dimensions, aggregates facts Golden records directly queryable by final BI views <ul> <li>Organize the Flink statements into pipeline folders, possibly using sources, intermediates, dimensions, and facts classification. Think about data products. A candidate hierarchy may look like this:   <pre><code>my-flink-app\n\n\u251c\u2500\u2500 pipelines\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 common.mk\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dimensions\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_product_a\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 facts\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_product_a\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 intermediates\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_product_a\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sources\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_product_a\n\u2502   \u2502\u00a0      \u251c\u2500\u2500 src_stream\n\u2502   \u2502\u00a0      \u2502\u00a0\u00a0 \u251c\u2500\u2500 Makefile\n\u2502   \u2502\u00a0      \u2502\u00a0\u00a0 \u251c\u2500\u2500 pipeline_definition.json\n\u2502   \u2502\u00a0      \u2502\u00a0\u00a0 \u251c\u2500\u2500 sql-scripts\n\u2502   \u2502\u00a0      \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.src_stream.sql\n\u2502   \u2502\u00a0      \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.src_stream.sql\n\u2502   \u2502\u00a0      \u2502\u00a0\u00a0 \u251c\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 views\n        \u2514\u2500\u2500 data_product_a\n</code></pre></li> </ul> <p>Once folders are moved to the pipelines it is possible to build a table inventory:</p> <ul> <li>Run after new tables are created   <pre><code>shift_left table build-inventory\n</code></pre></li> </ul> <p>As Flink statements have dependencies, it is important to use shift_left to manage the creation of the dependencies metadata automatically:</p> <ul> <li> <p>Build all the metadata   <pre><code>shift_left pipeline build-all-metadata \n</code></pre></p> </li> <li> <p>Then for any given table, Data Engineer may want to understand the execution plan   <pre><code>shift_left pipeline build-execution-plan --table-name &lt;&gt;\n</code></pre></p> </li> </ul>"},{"location":"tutorial/migration_ai_lab/#5-next","title":"5. Next","text":"<p>This will be detailed within another lab</p> <ul> <li>Add unit tests per table (at least for the complex DML ones) (see test harness)     <pre><code>shift_left table init-unit-tests c360_dim_users --nb-test-cases 1\n</code></pre></li> <li> <p>Add source data into the first tables of the pipeline: under a sources table and tests folder</p> </li> <li> <p>Verify the created records within the sink tables.</p> </li> </ul>"},{"location":"tutorial/project/","title":"Lab: Project Management","text":"<p>This lab focuses on starting a Confluent Flink project at scale, using best practices gathered during different real-life projects.</p> <p>At the highest level, the SDLC for flink project, may include the following activities:</p> <p>This labs introduces such activities.</p>"},{"location":"tutorial/project/#prerequisite","title":"Prerequisite","text":"<p>You have followed the setup lab to get shift_left CLI configured and running. Ensure <code>CONFIG_FILE</code>, <code>PIPELINES</code>, are set as described.</p> <p>In this lab, we are addressing the folling </p> <ul> <li>Initialize a Project using infrastructure as code and CLI.</li> <li>Add tables to add source processing, or building dimensions and facts using a data product approach.</li> <li>Adding unit tests to run on Confluent Cloud.</li> <li>Project Admin Work like building a table inventory, managing table metadata,working on Flink statements, assess table or data product execution plans, find orphan tables. </li> </ul>"},{"location":"tutorial/project/#initialize-a-project","title":"Initialize a Project","text":"<ul> <li>Select a folder from where to create the Flink project</li> <li>Execute the project init command:     <pre><code>shift_left project init &lt;project_name&gt; &lt;project_path&gt;\n# Example for a default Kimball project\nshift_left project init dsp .\n</code></pre></li> </ul> <p>The created folder structure looks like: <pre><code>dsp\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 IaC\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 environments\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dev\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 confluent_tf_graph.md\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 confluent.tf\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 outputs.tf\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 providers.tf\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 terraform.tfvars.example\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 prod\n\u251c\u2500\u2500 pipelines\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 common.mk\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dimensions\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 facts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 intermediates\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sources\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 views\n\u2514\u2500\u2500 staging\n</code></pre></p>"},{"location":"tutorial/project/#leveraging-infrastructure-as-code","title":"Leveraging Infrastructure As Code","text":"<p>The approach is to use Terraform to create Confluent Cloud Environment,  Kafka Cluster, Schema Registry and Compute pool. The created files are under the IaC folder and include:</p> File Purpose providers.tf Terraform provider configuration (Confluent, etc.) confluent.tf Confluent Cloud resources (environment, Kafka, Schema Registry, compute pool) variables.tf Input variables (e.g. region, cluster name) outputs.tf Output values (resource IDs, API keys) <p>To be able to run the terraform, a SRE needs to create, in the Confluent Console,  a service account, and the confluent cloud key and secrets for the terraform cli to use.</p> <p>It is possible to reuse existing environment, schema registry and kafka cluster by setting their ids in the <code>terraform.tfvars</code> file.</p> <pre><code>existing_environment_id         = \"env-xxxxx\"  # Leave null to create new environment\nexisting_kafka_cluster_id       = \"lkc-xxxxx\"  # Leave null to create new cluster\nexisting_schema_registry_id     = \"lsrc-xxxxx\"  # Leave null to use auto-provisioned Schema Registry\nexisting_service_account_id     = \"sa-xxxxx\"   # Leave null to create new service account\nexisting_flink_compute_pool_id  = \"lfcp-xxxxx\"  # Uncomment and set ID to reuse existing\n</code></pre> <p>The elements created are:</p> <pre><code>    flowchart TB\n    subgraph env [Environment]\n        cc_env[cc_env resource]\n        subgraph kafka [Kafka Custer]\n        kafka_cluster[kafka_cluster resource]\n        ssa_kafka_key[sa_kafka_key API Key]\n        end\n        subgraph dsp_sr[Schema Registry]\n        sr[schema_registry resource]\n        sa_sr_key[sa_schema_registry_key API Key]\n        end\n        subgraph flink [Flink Compute Pool]\n            flink_cp[flink_compute_pool resource]\n            sa_flink_key[sa_flink_key API Key]\n        end\n    end\n\n    subgraph sa [Service Accounts]\n        sa_res[service_account resource]\n        sa_env_admin[sa_env_admin RoleBinding]\n    end\n\n    sa_res --&gt;|\"EnvironmentAdmin\"| env</code></pre> <p>In most environment, it will be possible to get the production data to the dev Kafka Cluster. Below is a typical architecture for development using cluster link and schema linking.</p> <p>This means testing Flink statements on real-life data, is very valuable to verify if the statement works, but also assess data quality and data skew.</p>"},{"location":"tutorial/project/#add-a-source-table","title":"Add a source table","text":"<p>The goal for source tables, is to remove duplication, filter records, and maybe do transformation from the raw topic. If the raw topic is coming from a CDC, the record structure matches the table structure in the SQL database (CDC software like Debezium also includes envelop like <code>before</code> and <code>after</code> fields). Some column may be VARCHAR with a json object inside. It may be relevant to extract those information as new column in a Flink table. Each source table is published to its own raw topic, so there will be one Flink statement per raw topic:</p> <p>As an example, this tutorial creates some table for a data product for a customer 360 profiling. The final solution is visible in this git repo</p> <ul> <li>As a Data Engineer the environment variables should be set to point to the created project     <pre><code>export FLINK_PROJECT=$HOME/Code/dsp\nexport PIPELINES=$FLINK_PROJECT/pipelines\nexport STAGING=$FLINK_PROJECT/staging\nexport CONFIG_FILE=$FLINK_PROJECT/config.yaml   # see Setup Lab\n</code></pre></li> <li> <p>The command to add a table in the context of the <code>c360</code> data product is:     <pre><code>shift_left table init src_customers $PIPELINES/sources --product-name c360\n</code></pre></p> </li> <li> <p>Now the folder tree looks like this: <pre><code>\u251c\u2500\u2500 pipelines\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 common.mk\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dimensions\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 facts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 intermediates\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sources\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 c360\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 src_customers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 Makefile\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 sql-scripts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.src_c360_customers.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 dml.src_c360_customers.properties\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.src_c360_customers.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 tracking.md\n</code></pre></p> </li> </ul> <p>The approach to have DDL and DML in different files, to separate the table creation from the insertion logic, is to avoid droping the table each time there is a change to the logic, which happen quite often during Flink SQL development.</p>"},{"location":"tutorial/project/#update-the-ddl-content","title":"Update the DDL content","text":"<p>Change the DDL content in the file: <code>ddl.src_c360_customers.sql</code> to the following content:</p> <pre><code>CREATE TABLE IF NOT EXISTS src_c360_customers (\n    customer_id STRING,\n    first_name STRING,\n    last_name STRING,\n    email STRING,\n    phone STRING,\n    date_of_birth DATE,\n    gender STRING,\n    registration_date TIMESTAMP(3),\n    customer_segment STRING,\n    preferred_channel STRING,\n    address_line1 STRING,\n    city STRING,\n    state STRING,\n    zip_code STRING,\n    country STRING,\n    age_years BIGINT,\n    days_since_registration BIGINT,\n    generation_segment STRING,\n    missing_email_flag BIGINT,\n    missing_phone_flag BIGINT,\nPRIMARY KEY(customer_id) NOT ENFORCED\n) DISTRIBUTED BY HASH(customer_id) INTO 1 BUCKETS\nWITH (\n'changelog.mode' = 'upsert',\n'key.avro-registry.schema-context' = '.flink-dev',\n'value.avro-registry.schema-context' = '.flink-dev',\n'key.format' = 'avro-registry',\n'value.format' = 'avro-registry',\n'kafka.retention.time' = '0',\n'kafka.producer.compression.type' = 'snappy',\n'scan.bounded.mode' = 'unbounded',\n'scan.startup.mode' = 'earliest-offset',\n'value.fields-include' = 'all'\n);\n</code></pre> <p>The table is upsert changelog mode with a primary key, so records will be deduplicated automatically by the Flink engine. As multiple Kafka Clusters are defined in Confluent Cloud environment, and there is one schema registry in the environement, this is good practice to isolate the schema-context in the schema registry. See product schema-context documentation.</p>"},{"location":"tutorial/project/#update-the-dml-content","title":"Update the DML content","text":"<p>Do the same for the <code>dml.src_c360_customers.sql</code>:</p> <pre><code>INSERT INTO src_c360_customers\nSELECT \n    customer_id,\n    first_name,\n    last_name,\n    email,\n    phone,\n    date_of_birth,\n    gender,\n    registration_date,\n    customer_segment,\n    preferred_channel,\n    address_line1,\n    city,\n    state,\n    zip_code,\n    country,\n    TIMESTAMPDIFF(YEAR, CAST(date_of_birth AS TIMESTAMP_LTZ(3)), event_ts) age_years,\n    TIMESTAMPDIFF(DAY, CAST(registration_date AS TIMESTAMP_LTZ(3)), event_ts) as days_since_registration,\n     CASE\n        WHEN TIMESTAMPDIFF(YEAR, CAST(date_of_birth AS TIMESTAMP_LTZ(3)), event_ts)  &lt; 25 THEN 'Gen Z'\n        WHEN TIMESTAMPDIFF(YEAR, CAST(date_of_birth AS TIMESTAMP_LTZ(3)), event_ts)  &lt; 40 THEN 'Millennial'\n        WHEN TIMESTAMPDIFF(YEAR, CAST(date_of_birth AS TIMESTAMP_LTZ(3)), event_ts) &lt; 55 THEN 'Gen X'\n        ELSE 'Boomer+' END AS generation_segment,\n    CASE\n        WHEN email IS NULL\n        OR email = '' THEN 1\n        ELSE 0 END AS missing_email_flag,\n     CASE\n        WHEN phone IS NULL\n        OR phone = '' THEN 1\n        ELSE 0 END AS missing_phone_flag\nFROM customers_raw\n</code></pre> <p>It is possible for a Data Engineer to use VsCode to develop those SQL statements using the Confluent VScode extension. </p> <p>The natural way to develop DML is to use the Confluent Cloud Workspace and build the SQL iteratively, by looking at data, build CTEs and then final <code>insert into</code></p> <p>Copy/paste the above DML will fail as the <code>customers_raw</code> is not created. As seen in previous section, in real-life the Kafka dev cluster may have real data created by replicating source topics, like a <code>customers_raw</code> topic. In this tutorial we need to create the table, we will do that in next section.</p> <p>The <code>shift_left</code> tool has also created a Makefile for each table so Data engineer may use <code>make</code> to deploy DDL and DML. The makefile encapsulates the  <code>confluent cli</code> commands to simplify memorizing the commands with simple common verbs to work on Flink statement:</p> <pre><code>make create_flink_ddl\n\nmake create_flink_dml\n\nmake describe_flink_ddl\n\nmake pause_flink_dml\nmake resume_flink_dml\n\nmake drop_table\n</code></pre> <p>Those verbs are the same for all tables.</p>"},{"location":"tutorial/project/#adding-unit-tests","title":"Adding unit tests","text":"<p><code>shift_left</code> includes a test harness command to introspecte the SQL and to create synthetic data.</p> <ul> <li> <p>The first thing to do is to get a table inventory up-to-date so the tool can search parent definition of the table under tests.      <pre><code>shift_left table build-inventory\n</code></pre></p> <p>This command creates an inventory.json under the $PIPELINES folder. This file is a map : <pre><code>\"src_c360_customers\": {\n    \"table_name\": \"src_c360_customers\",\n    \"product_name\": \"c360\",\n    \"type\": \"source\",\n    \"dml_ref\": \"pipelines/sources/c360/src_customers/sql-scripts/dml.src_c360_customers.sql\",\n    \"ddl_ref\": \"pipelines/sources/c360/src_customers/sql-scripts/ddl.src_c360_customers.sql\",\n    \"table_folder_name\": \"pipelines/sources/c360/src_customers\"\n},\n</code></pre> <p>That file does not need to be commited to the git remote, as the CI/CD process will recreate it at each Pull Requests.</p> <li> <p>Run the following command:     <pre><code>shift_left table init-unit-tests src_customers --nb-test-cases 1\n</code></pre></p> </li> <p>This will create a set of files under the tests folder:</p> <pre><code>\u2502\u00a0\u00a0 \u251c\u2500\u2500 sources\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 c360\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 src_customers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 Makefile\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 pipeline_definition.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 sql-scripts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl.src_c360_customers.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 dml.src_c360_customers.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ddl_customers_raw.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 insert_customers_raw_1.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_definitions.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 validate_src_c360_customers_1.sql\n</code></pre>"},{"location":"tutorial/project/#run-unit-tests","title":"Run unit tests","text":"<p>Execute unit tests for a table by sending data to <code>_ut</code> topics and validating results. By default runs insert SQL and foundation steps; use <code>--run-all</code> to also run validation SQL. The test suite report is saved as JSON under the session log directory.</p> Option Default Description <code>table_name</code> (required) Table to run tests for <code>--test-case-name</code> (all) Run only this test case (e.g. <code>_1</code>) <code>--run-all</code> <code>False</code> Also run validation SQL <code>--compute-pool-id</code> config Flink compute pool (env: <code>CPOOL_ID</code>) <code>--post-fix-unit-test</code> config or <code>_ut</code> Suffix for UT topics (e.g. <code>_ut</code>, <code>_foo</code>). Must start with <code>_</code>, then 2\u20133 alphanumeric characters <pre><code># Run all unit tests for the table (inserts and foundations only)\nshift_left table run-unit-tests src_c360_customers\n\n# Run a specific test case and include validation\nshift_left table run-unit-tests src_c360_customers --test-case-name _1 --run-all\n\n# Use a dedicated compute pool and topic suffix\nshift_left table run-unit-tests src_c360_customers --compute-pool-id &lt;pool_id&gt; --post-fix-unit-test _ut\n</code></pre> <p>Related commands:</p> <ul> <li>run-validation-tests (or validate-unit-tests): Run only the validation SQL for the table. Same options as run-unit-tests.</li> <li>delete-unit-tests: Remove Flink statements and Kafka topics created for the table's unit tests. Options: <code>--compute-pool-id</code>, <code>--post-fix-unit-test</code>.</li> </ul> <p>To deploy DDL and DML to Confluent Cloud (create/run Flink statements), see Pipeline Management and Blue/Green Deployment.</p>"},{"location":"tutorial/project/#project-admin-work","title":"Project Admin Work","text":""},{"location":"tutorial/project/#build-table-inventory","title":"Build table inventory","text":"<p>The following command will create a json file under the $PIPELINES folder of all the tables defined in the repository. <pre><code>shift_left table build-inventory\n</code></pre></p>"},{"location":"tutorial/project/#validate-config","title":"Validate config","text":"<p>Check that the config file (from <code>CONFIG_FILE</code>) is valid before running other project or pipeline commands.</p> <pre><code>shift_left project validate-config\n</code></pre>"},{"location":"tutorial/project/#build-table-relationships","title":"Build Table RelationShips","text":"<p>For each table created by Flink statement, it is easy by parsing the SQL statement to know the direct parents using the JOINS, FROM clauses. The following command creates a <code>pipeline_definition.json</code> file for each table that includes the parent list and also assess the complexity of the statement so we could compute the global complexity of a data product.</p> <pre><code>shift_left pipeline build-all-metadata\n</code></pre> <p>When crawling the complete repository the tool updates each table's pipeline_definition.json children list when this table is parent of another. </p> <p>This element is crucial to manage a end-to-end pipeline. The following example comes from this repository with a customer 360 data analytics product</p> <pre><code>{\n   \"table_name\": \"dim_c360_customer_transactions\",\n   \"product_name\": \"c360\",\n   \"type\": \"intermediate\",\n   \"dml_ref\": \"pipelines/intermediates/c360/int_customer_transactions/sql-scripts/dml.int_c360_customer_transactions.sql\",\n   \"ddl_ref\": \"pipelines/intermediates/c360/int_customer_transactions/sql-scripts/ddl.int_c360_customer_transactions.sql\",\n   \"path\": \"pipelines/intermediates/c360/int_customer_transactions\",\n   \"complexity\": {\n      \"number_of_regular_joins\": 0,\n      \"number_of_left_joins\": 0,\n      \"number_of_right_joins\": 0,\n      \"number_of_inner_joins\": 3,\n      \"number_of_outer_joins\": 0,\n      \"complexity_type\": \"Medium\",\n      \"state_form\": \"Stateful\"\n   },\n   \"parents\": [\n      {\n         \"table_name\": \"src_c360_tx_items\",\n         \"product_name\": \"c360\",\n         \"type\": \"source\",\n         \"dml_ref\": \"pipelines/sources/c360/src_tx_items/sql-scripts/dml.src_c360_tx_items.sql\",\n         \"ddl_ref\": \"pipelines/sources/c360/src_tx_items/sql-scripts/ddl.src_c360_tx_items.sql\",\n         \"path\": \"pipelines/sources/c360/src_tx_items\"\n      },\n      {\n         \"table_name\": \"src_c360_transactions\",\n         \"product_name\": \"c360\",\n         \"type\": \"source\",\n         \"dml_ref\": \"pipelines/sources/c360/src_transactions/sql-scripts/dml.src_c360_transactions.sql\",\n         \"ddl_ref\": \"pipelines/sources/c360/src_transactions/sql-scripts/ddl.src_c360_transactions.sql\",\n         \"path\": \"pipelines/sources/c360/src_transactions\"\n      },\n      {\n         \"table_name\": \"src_c360_customers\",\n         \"product_name\": \"c360\",\n         \"type\": \"source\",\n         \"dml_ref\": \"pipelines/sources/c360/src_customers/sql-scripts/dml.src_c360_customers.sql\",\n         \"ddl_ref\": \"pipelines/sources/c360/src_customers/sql-scripts/ddl.src_c360_customers.sql\",\n         \"path\": \"pipelines/sources/c360/src_customers\"\n      },\n      {\n         \"table_name\": \"src_c360_products\",\n         \"product_name\": \"c360\",\n         \"type\": \"source\",\n         \"dml_ref\": \"pipelines/sources/c360/src_products/sql-scripts/dml.src_c360_products.sql\",\n         \"ddl_ref\": \"pipelines/sources/c360/src_products/sql-scripts/ddl.src_c360_products.sql\",\n         \"path\": \"pipelines/sources/c360/src_products\",\n      }\n   ],\n   \"children\": [\n      {\n         \"table_name\": \"c360_fct_customer_profile\",\n         \"product_name\": \"c360\",\n         \"type\": \"fact\",\n         \"dml_ref\": \"pipelines/facts/c360/fct_customer_360_profile/sql-scripts/dml.c360_fct_customer_profile.sql\",\n         \"ddl_ref\": \"pipelines/facts/c360/fct_customer_360_profile/sql-scripts/ddl.c360_fct_customer_profile.sql\",\n         \"path\": \"pipelines/facts/c360/fct_customer_360_profile\",\n      }\n   ]\n}\n</code></pre> <p>Here is a graph view for a view table of a data analytics product:</p> <p>This graph can be built for any table using a command like:</p> <pre><code>shift_left pipeline report &lt;table_name&gt;\n#\nshift_left pipeline report  customer_analysis_c360 --open\n</code></pre>"},{"location":"tutorial/project/#get-table-use-cross-data-product","title":"Get Table Use Cross Data Product","text":"<p>When project grows in number of Flink Statements so table, it will be interesting to get the list of tables that may be used by more than one product. The command writes the list to <code>table_cross_products.txt</code> under <code>~/.shift_left/</code>.</p> <pre><code>shift_left project report-table-cross-products\n</code></pre>"},{"location":"tutorial/project/#list-tables-with-one-child","title":"List tables with one child","text":"<p>Report tables that have exactly one child table. Useful for pipeline simplification or refactoring. The list is written to <code>tables_with_one_child.txt</code> under <code>~/.shift_left/</code>.</p> <pre><code>shift_left project list-tables-with-one-child\n</code></pre> <p>Table with single descendant may be good candidate to become CTEs for the child table.</p>"},{"location":"tutorial/project/#getting-the-list-of-compute-pools","title":"Getting the list of compute pools","text":"<p>Getting current CFU utilization and the list of all compute pools</p> <pre><code>shift_left project list-compute-pools\n</code></pre> <p>The response looks like: <pre><code>ComputePoolList(\n    created_at=datetime.datetime(2026, 2, 2, 10, 52, 10, 327409),\n    pools=[\n        ComputePoolInfo(\n            id='lfcp-....',\n            name='dev-src-sdp-shipments',\n            env_id='env-....',\n            max_cfu=30,\n            region='us-west-2',\n            status_phase='PROVISIONED',\n            current_cfu=1\n        ),\n        ComputePoolInfo(\n            id='lfcp-....',\n            name='data-generation',\n            env_id='env-....',\n            max_cfu=50,\n            region='us-west-2',\n            status_phase='PROVISIONED',\n            current_cfu=0\n        )]\n</code></pre></p>"},{"location":"tutorial/project/#getting-the-list-of-statements-running-in-a-compute-pool","title":"Getting the list of statements running in a compute pool","text":"<p>List Flink statements in a given compute pool. Output includes statement name, status (e.g. RUNNING), compute pool ID, and catalog/database.</p> <pre><code>shift_left project get-statement-list &lt;compute_pool_id&gt;\n</code></pre> <p>Here is an extracted content for one statement:</p> <pre><code>'dev-usw2-c360-dml-src-c360-tx-items': StatementInfo(\n    name='dev-usw2-c360-dml-src-c360-tx-items',\n    status_phase='RUNNING',\n    status_detail='',\n    sql_content='INSERT INTO src_c360_tx_items\\nSELECT \\n    item_id,\\n    transaction_id,\\n    product_id,\\n    quantity,\\n    unit_price,\\n    \nline_total,\\n    discount_applied\\nFROM (\\n    SELECT *,\\n        ROW_NUMBER() OVER (\\n            PARTITION BY item_id \\n            ORDER BY \n`$rowtime` DESC\\n        ) AS row_num\\n    FROM transaction_items_raw\\n)\\nWHERE row_num = 1',\n    compute_pool_id='lfcp-xvrvmz',\n    compute_pool_name=None,\n    created_at=datetime.datetime(2026, 1, 27, 17, 56, 29, 286807, tzinfo=TzInfo(0)),\n    principal='u-xg2ndz',\n    sql_catalog='j9r-env',\n    sql_database='j9r-kafka'\n),\n</code></pre>"},{"location":"tutorial/project/#housekeep-statements","title":"Housekeep statements","text":"<p>Clean up or manage Flink statements in two mutually exclusive ways.</p> <p>Cleanup by filter: Delete statements matching name prefix, status, and age. Defaults: prefix <code>workspace</code>, statuses <code>COMPLETED</code> and <code>FAILED</code>, age <code>0</code> days. The prefix cannot start with reserved words: <code>dev</code>, <code>stage</code>, <code>prod</code>.</p> Option Default Description <code>--starts-with</code> <code>workspace</code> Delete only statements whose name starts with this string <code>--status</code> <code>COMPLETED</code>, <code>FAILED</code> One of: <code>COMPLETED</code>, <code>FAILED</code>, <code>STOPPED</code> <code>--age</code> <code>0</code> Delete only statements at least this many days old <pre><code># Default: delete COMPLETED/FAILED statements named like \"workspace*\"\nshift_left project housekeep-statements\n\n# Delete COMPLETED statements named like \"workspace*\" older than 7 days\nshift_left project housekeep-statements --starts-with workspace --status COMPLETED --age 7\n</code></pre> <p>Here is an example of output: <pre><code>20260202_10:56:43 Statement list has 90 statements\n20260202_10:56:43 Clean statements starting with [ workspace ] in ['COMPLETED'] state, with a age &gt;= [ 0 ]\n20260202_10:56:43 delete workspace-2025-12-20-010600-83d7913f-0db7-4509-be27-9ed218b29687 COMPLETED\n20260202_10:56:43 delete workspace-2025-12-20-010600-a46b44d5-ed97-4b14-9228-f924a7a9c0d4 COMPLETED\n</code></pre></p> <p>Pool-specific actions (with <code>--compute-pool-id</code>): Run an action on statements in a single compute pool. Requires <code>--action</code>. Do not use <code>--starts-with</code>, <code>--status</code>, or <code>--age</code> in this mode.</p> Option Description <code>--compute-pool-id</code> Target compute pool ID <code>--action</code> One of: <code>PAUSE</code>, <code>RESUME</code>, <code>DELETE</code> <code>--statement-name</code> Optional; limit to one statement. Required when <code>--action</code> is <code>RESUME</code> <ul> <li>PAUSE: Pause RUNNING statements (skips non-running).</li> <li>RESUME: Resume STOPPED statements; requires <code>--statement-name</code>.</li> <li>DELETE: Delete all statements in the pool (optionally filtered by <code>--statement-name</code>).</li> </ul> <pre><code># Pause all running statements in a pool\nshift_left project housekeep-statements --compute-pool-id &lt;pool_id&gt; --action PAUSE\n\n# Resume a specific stopped statement\nshift_left project housekeep-statements --compute-pool-id &lt;pool_id&gt; --action RESUME --statement-name &lt;name&gt;\n\n# Delete all statements in a pool\nshift_left project housekeep-statements --compute-pool-id &lt;pool_id&gt; --action DELETE\n</code></pre>"},{"location":"tutorial/project/#list-modified-files","title":"List modified files","text":"<p>List files modified in the current git branch compared to a base branch. By default filters for SQL files and files modified since a given date. Writes affected table names to a file under <code>~/.shift_left/</code> (default: <code>modified_flink_files.txt</code>). Use this to decide which Flink statements to redeploy in a blue-green deployment.</p> Option Default Description <code>branch_name</code> (required) Base branch to compare (e.g. <code>main</code>, <code>origin/main</code>) <code>--project-path</code> <code>.</code> Git repository path <code>--file-filter</code> <code>.sql</code> File extension to include <code>--since</code> <code>2025-12-01</code> Only files modified on or after this date (YYYY-MM-DD) <pre><code># Compare current branch to main, SQL files only\nshift_left project list-modified-files main\n\n# Limit to files changed since a date\nshift_left project list-modified-files origin/main --since 2026-01-01\n</code></pre>"},{"location":"tutorial/project/#orphan-and-unused-tables","title":"Orphan and unused tables","text":"<p>During the life cycle of the project, it may be possible that some tables were created and not deleted. Getting the list of topics in a given Kafka cluster may give a first level of information but this is not enough. The git repository includes an inventory of all the tables and getting the list of tables can be done as seen before.</p> <p>To find Flink SQL tables that are not referenced by any running DML statement, the command reads the table inventory and pipeline metadata (parent/child from <code>pipeline_definition.json</code>), compares them to running Flink statements, and reports tables that appear unused. Optionally it compares Kafka topics to list topics that have no corresponding running statement.</p> <p>Use this to identify tables or topics that can be retired or to spot tables that are unused but still have children (indirect use).</p> Option Default Description <code>inventory_path</code> <code>PIPELINES</code> Pipeline path where tables are defined <code>--include-topics</code> / <code>--no-topics</code> <code>True</code> Include unused Kafka topics in the report <code>--output-file</code> <code>~/.shift_left/unused_tables_&lt;timestamp&gt;.txt</code> Path for the results file <p>Output includes a summary count, a table of unused tables (name, type, product, has children, path), and if requested a list of unused topics. Source tables and tables that have children are called out because they may be used indirectly. Results are also written to the output file.</p> <pre><code># Use PIPELINES; include topics; default output file\nshift_left project assess-unused-tables\n\n# Custom path and output file; skip topic check\nshift_left project assess-unused-tables $PIPELINES --no-topics --output-file ./unused.txt\n</code></pre> <p>Example of report:</p> <p>The full report includes an \"Unused Tables\" section (table name, type, product, has children, path) and, when <code>--include-topics</code> is set, an \"Unused Topics\" section. Below is an example of the topics table:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Topic Name            \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 app_usage_raw         \u2502\n\u2502 append_tx             \u2502\n\u2502 customers             \u2502\n\u2502 customers_dedup       \u2502\n\u2502 customers_faker       \u2502\n\u2502 customers_filteres    \u2502\n\u2502 customers_raw         \u2502\n\u2502 discounts_faker       \u2502\n\u2502 group_hierarchy       \u2502\n\u2502 groups_salted         \u2502\n\u2502 loyalty_program_raw   \u2502\n\u2502 page_views_1m         \u2502\n\u2502 products_raw          \u2502\n\u2502 src_tx_customers      \u2502\n\u2502 src_tx_discounts      \u2502\n\u2502 src_tx_transactions   \u2502\n\u2502 suites_versioned      \u2502\n\u2502 support_ticket_raw    \u2502\n\u2502 test_a                \u2502\n\u2502 transaction_items_raw \u2502\n\u2502 transactions          \u2502\n\u2502 transactions_faker    \u2502\n\u2502 truck_loads           \u2502\n\u2502 tx_raw                \u2502\n\u2502 unique_orders         \u2502\n\u2502 users                 \u2502\n\u2502 users_salted          \u2502\n\u2502 users_transform       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial/project/#drop-all-tables-from-a-list","title":"Drop all tables from a list","text":"<p>As a continuation of assessing orphans, user needs to validate false-positive and update the reported text file accordingly. Once done the following command will call drop table on each of those table</p> <pre><code>shift_left project delete-unused-tables toremove.txt\n</code></pre>"},{"location":"tutorial/setup_lab/","title":"Lab: Setup Shift_left tool","text":"<p>This Lab focuses on how to get started withthe shift_left tool. This tools is not yet a public pipy module so cloning the repository and do some minimum installation are needed.</p> <p>To install the CLI, which is based on Python, use a Python virtual environment like venv and a Python module manafer like <code>pip</code> or uv. All the development is based on <code>uv</code> so we recommend to use it.</p>"},{"location":"tutorial/setup_lab/#1-preparation","title":"1 - Preparation","text":"<p>We assume you have a Confluent Cloud account, an environment sets up, a Kafka Cluster available. shift_left is now available on PyPi.</p> <ol> <li> <p>Clone this repository if you want to run the AI processing to be able to tune the system prompts:    <pre><code>git clone  https://github.com/jbcodeforce/shift_left_utils.git\ncd shift_left_utils\n</code></pre></p> </li> <li> <p>On Windows - enable WSL2. The shift_left tool was developed on Mac and tested on Linux. Windows WSL should work. Powershell will not work as of now (11/2025).</p> </li> <li>All Platforms - install git</li> <li> <p><code>make</code> is used to encapsulate the confluent cli, to make it easier for Data Engineers to deploy Flink statement during development: It is not used by <code>shift_left</code> tool, but <code>shift_left</code> creates the Makefile with the <code>shift_left table init</code> command (see the recipe section). </p> <ul> <li>install make for windows</li> <li>Mac OS: <code>brew install make</code> </li> <li>Linux: <code>sudo apt-get install build-essential</code></li> </ul> </li> <li> <p>All Platforms - install confluent cli. This is not mandatory to run shift_left tool only if you want to use <code>make</code>.</p> </li> <li> <p>Create virtual environment:</p> </li> </ol> uvUsing Python and pip <ul> <li> <p>Once uv installed, verify the version. It should be fine to be greater or equal to 0.9.13 <pre><code>uv self version\n&gt; ...uv 0.9.13\n</code></pre></p> </li> <li> <p>Create a new virtual environment in any folder:  <pre><code># can be in the `shift_left` folder\nuv venv --python 3.12\n</code></pre>  This will create a <code>.venv</code> folder.</p> </li> <li> <p>Activate the environment:     <pre><code>source .venv/bin/activate\n</code></pre></p> </li> <li> <p>Install the cli:     <pre><code>uv pip install shift_left\n</code></pre></p> </li> </ul> <ul> <li>Install python 3.12.xx</li> <li>Create a Python virtual environment:     <pre><code>python -m venv .venv\n</code></pre></li> <li>Activate the environment:     <pre><code>source .venv/bin/activate\n</code></pre></li> <li>Be sure to use the pip install in the virtual environment:     <pre><code>python -m pip --version\n</code></pre></li> <li>Install the <code>shift_left</code> CLI using the command:     <pre><code>pip install shift_left\n</code></pre></li> </ul>"},{"location":"tutorial/setup_lab/#2-a-demonstration-project","title":"2- A Demonstration Project","text":"<ol> <li>[Optional] Clone a git repository with data as a product project done in Flink and in Spark. This project includes different demonstrations, but we will use the Customer c360 analytics project to support the current labs.     <pre><code>cd ..\ngit clone https://github.com/jbcodeforce/flink_project_demos\n</code></pre></li> </ol>"},{"location":"tutorial/setup_lab/#3-get-confluent-cloud-information","title":"3- Get Confluent Cloud Information","text":"<p>The <code>shift_left</code> tool will access Confluent Cloud REST endpoint and will do operations on behalf of your Confluent user. You need to set environment variables and config file. </p> <ol> <li>Get the <code>shift_left</code> config file template to the tutorial folder:     <pre><code># From the `shift_left_utils` folder\ncurl  https://raw.githubusercontent.com/jbcodeforce/shift_left_utils/refs/heads/main/src/shift_left/shift_left/core/templates/config_tmpl.yaml  -o tutorial/setup/config.yaml\n</code></pre></li> </ol>"},{"location":"tutorial/setup_lab/#31-get-environment-id-from-confluent-cloud","title":"3.1 Get Environment id from Confluent Cloud","text":"UIConfluent Cloud CLI <p>Go to Environments from the left menu, select the environment you want to use</p> <p></p> <p>Select the Details tab for the ID, Provider and region </p> <p></p> <p>and update the config.yaml <pre><code> confluent_cloud:\n    environment_id:  env-&lt;TO_FILL&gt;\n</code></pre></p> <p><pre><code>confluent login\nconfluent environment list\n\n     ID     |           Name           | Stream Governance Package |\n------------+--------------------------+----------------------------\n env-nxxxx  | j9r-env                  | ADVANCED      \n</code></pre> and update the config.yaml <pre><code> confluent_cloud:\n    environment_id:  env-&lt;TO_FILL&gt;\n</code></pre></p>"},{"location":"tutorial/setup_lab/#32-get-organization-id","title":"3.2 Get Organization ID","text":"UIConfluent Cloud CLI <p>Go to the contextual menu on the top right side, and then Organization settings, then copy the ID</p> <p></p> <p>and update the config.yaml <pre><code> confluent_cloud:\n    organization_id: &lt;TO_FILL&gt;\n</code></pre></p> <p><pre><code>confluent login\nconfluent organization list\n</code></pre> Copy the ID and update the config.yaml <pre><code> confluent_cloud:\n    organization_id: &lt;TO_FILL&gt;\n</code></pre></p>"},{"location":"tutorial/setup_lab/#33-get-kafka-cluster-information","title":"3.3 Get Kafka Cluster Information","text":"UIConfluent CLI <p>Go to Environments &gt; Clusters &gt; Select on Kafka Cluster &gt; Cluster Settings &gt; General tab</p> <p></p> <p>and update the config.yaml</p> <pre><code>kafka:\n   bootstrap.servers: &lt;To FILL with endpoint&gt;\n   cluster_id: &lt;TO_FILL WITH Cluster ID&gt;\nconfluent_cloud:\n    region:  &lt;TO_FILL with Region&gt;\n    provider:  &lt;TO_FILL with Provider&gt;\nflink:\n    catalog_name: &lt;TO FILL with environment_name&gt;\n    database_name: &lt;TO FILL with cluster_name&gt;\n</code></pre> <p>For the cloud provider and region</p> <p><pre><code>confluent kafka cluster list\n    ID     |   Name     |  Cloud | Region    |\n-----------+------------+--------+-----------+\nlkc-3xxxxx | j9r-kafka  | aws    | us-west-2 |\n</code></pre> and update the config.yaml <pre><code>kafka:\n   bootstrap.servers: &lt;To FILL with endpoint&gt;\n   cluster_id: &lt;TO_FILL WITH ID&gt;\nconfluent_cloud:\n    region:  &lt;TO_FILL with Region&gt;\n    provider:  &lt;TO_FILL with Cloud&gt;\nflink:\n    catalog_name: &lt;TO FILL with environment_name&gt;\n    database_name: &lt;TO FILL with cluster_name&gt;\n</code></pre></p>"},{"location":"tutorial/setup_lab/#4-get-confluent-cloud-keys-and-secrets","title":"4- Get Confluent Cloud Keys and Secrets","text":""},{"location":"tutorial/setup_lab/#41-confluent-cloud-api-keys-and-secrets","title":"4.1 Confluent Cloud API Keys and Secrets","text":"UIConfluent CLI <p>Go to the contextual menu on the top right side, and then <code>Add API Key</code> button:</p> <p></p> <p>Select User or Service Account</p> <p></p> <p>Then resource scope</p> <p></p> <p>Set a name and description and download the created API.</p> <p>List existing keys for your user <pre><code> confluent api-key list | grep username\n</code></pre></p> <p>Create a Cloud api <pre><code>confluent api-key create --resource cloud\n</code></pre></p> <ul> <li>Edit the <code>tutorial/setup/set_env_var</code> file for the following variables:     <pre><code>export SL_CONFLUENT_CLOUD_API_KEY=\nexport SL_CONFLUENT_CLOUD_API_SECRET=\n</code></pre></li> </ul>"},{"location":"tutorial/setup_lab/#42-kafka-cluster-api-keys-and-secrets","title":"4.2 Kafka Cluster API Keys and Secrets","text":"UIConfluent CLI <p>Go to Environments &gt; Cluster &gt; API Keys &gt; Select the Create </p> <p></p> <p>List existing keys for your user</p> <pre><code> confluent api-key list | grep username | grep kafka\n</code></pre> <p>Create a Kafka api for a cluster id:</p> <pre><code>confluent api-key create  --resource lkc-123456\n</code></pre> <ul> <li>Edit the <code>tutorial/setup/set_env_var</code> file for the following variables:     <pre><code>export SL_KAFKA_API_KEY=\nexport SL_KAFKA_API_SECRET=\n</code></pre></li> </ul>"},{"location":"tutorial/setup_lab/#43-kafka-flink-api-keys-and-secrets","title":"4.3 Kafka Flink API Keys and Secrets","text":"UIConfluent CLI <p>Go to Environments &gt; Flink &gt; API Keys tab</p> <p></p> <p>List existing keys for your user <pre><code> confluent api-key list | grep username | grep kafka\n</code></pre></p> <p>Create a Kafka api for a cluster id: <pre><code>confluent api-key create  --resource lkc-123456\n</code></pre></p> <ul> <li>Edit the <code>tutorial/setup/set_env_var</code> file for the following variables:     <pre><code>export SL_FLINK_API_KEY=\nexport SL_FLINK_API_SECRET=\n</code></pre></li> </ul>"},{"location":"tutorial/setup_lab/#5-finalize-environment-variables","title":"5- Finalize environment variables","text":"<p>Shift_left uses the CONFIG_FILE and PIPELINES environment variables all the time. So it is important to set them correctly. The following setting should work for the different labs of this tutorial.</p> <pre><code>export FLINK_PROJECT=$PWD/../flink_project_demos/customer_360/c360_flink_processing\nexport PIPELINES=$FLINK_PROJECT/pipelines\nexport STAGING=$FLINK_PROJECT/staging\nexport SRC_FOLDER=$FLINK_PROJECT/../c360_spark_processing\nexport CONFIG_FILE=$PWD/tutorial/setup/config.yaml\n</code></pre> <ul> <li>Set your terminal shell with those environment variables:     <pre><code>source tutorial/setup/set_env_var\n</code></pre></li> </ul>"},{"location":"tutorial/setup_lab/#6-validate-the-configuration","title":"6- Validate the configuration","text":"<p>The shift_left CLI works with 3 top level commands: <code>project</code>, <code>table</code> and <code>pipeline</code>. With the project the <code>validate_config</code> helps to debug the config.yaml specified by the CONFIG_FILE environment variables.</p> <pre><code>shift_left version\nshift_left project validate-config\n</code></pre> <p>A CONFIG_FILE matches a specific environment, kafka cluster and Flink. So when working with different environments or different Kafka Clusters within an environment, it is recommended to use different config.yaml and environment variables.</p> Security access <p>The config.yaml and environment variable files should ignored in Git. </p>"}]}