# -----------------------------------------------------------------------------
# {{ project_name }} - Confluent Cloud Infrastructure
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Environment (conditional: create new or use existing)
# -----------------------------------------------------------------------------
# Data source for existing environment by ID
data "confluent_environment" "env_by_id" {
  count = var.existing_environment_id != null ? 1 : 0
  id    = var.existing_environment_id
}

# Data source for existing environment by name
data "confluent_environment" "env_by_name" {
  count = var.existing_environment_id == null && var.existing_environment_name != null ? 1 : 0
  display_name = var.existing_environment_name
}

# Create new environment with random ID (only if no ID or name provided)
resource "confluent_environment" "cc_env" {
  count = var.existing_environment_id == null && var.existing_environment_name == null ? 1 : 0

  display_name = "${var.prefix}-environment-${random_id.env_display_id.hex}"

  stream_governance {
    package = "ADVANCED"
  }

  lifecycle {
    prevent_destroy = true
  }
}


# Local values to reference resources (created or existing)
# Priority: existing ID > existing name (data source) > created with name > created with random ID
locals {
  environment_id = var.existing_environment_id != null ? var.existing_environment_id : (
    var.existing_environment_name != null ? data.confluent_environment.env_by_name[0].id : confluent_environment.cc_env[0].id
  )
  environment_name = var.existing_environment_name != null ? var.existing_environment_name : (
    var.existing_environment_id != null ? data.confluent_environment.env_by_id[0].display_name : confluent_environment.cc_env[0].display_name
  )
  environment_resource_name = var.existing_environment_id != null ? data.confluent_environment.env_by_id[0].resource_name : (
    var.existing_environment_name != null ? data.confluent_environment.env_by_name[0].resource_name : confluent_environment.cc_env[0].resource_name
  )
}

# -----------------------------------------------------------------------------
# Kafka Cluster (conditional: create new or use existing)
# -----------------------------------------------------------------------------
resource "confluent_kafka_cluster" "kafka_cluster" {
  count = var.existing_kafka_cluster_id == null ? 1 : 0

  display_name = "${var.prefix}-cluster-${random_id.env_display_id.hex}"
  availability = var.cc_availability
  cloud        = "AWS"
  region       = var.cloud_region

  standard {}

  environment {
    id = local.environment_id
  }

  lifecycle {
    prevent_destroy = false
  }
}

# Data source for existing Kafka cluster
data "confluent_kafka_cluster" "kafka_cluster" {
  count = var.existing_kafka_cluster_id != null ? 1 : 0
  id    = var.existing_kafka_cluster_id

  environment {
    id = local.environment_id
  }
}

# Additional local values
locals {
  kafka_cluster_id = var.existing_kafka_cluster_id != null ? var.existing_kafka_cluster_id : confluent_kafka_cluster.kafka_cluster[0].id
  kafka_cluster_rest_endpoint = var.existing_kafka_cluster_id != null ? data.confluent_kafka_cluster.kafka_cluster[0].rest_endpoint : confluent_kafka_cluster.kafka_cluster[0].rest_endpoint
}

# -----------------------------------------------------------------------------
# Schema Registry (conditional: use existing or auto-provisioned)
# -----------------------------------------------------------------------------
# Data source for existing Schema Registry by ID
data "confluent_schema_registry_cluster" "schema_registry_by_id" {
  count = var.existing_schema_registry_id != null ? 1 : 0
  id    = var.existing_schema_registry_id

  environment {
    id = local.environment_id
  }
}

# Data source for auto-provisioned Schema Registry
data "confluent_schema_registry_cluster" "schema_registry_auto" {
  count = var.existing_schema_registry_id == null ? 1 : 0
  environment {
    id = local.environment_id
  }

  depends_on = [
    confluent_kafka_cluster.kafka_cluster,
    data.confluent_kafka_cluster.kafka_cluster
  ]
}

# Local value to reference Schema Registry (existing or auto-provisioned)
locals {
  schema_registry_id = var.existing_schema_registry_id != null ? var.existing_schema_registry_id : data.confluent_schema_registry_cluster.schema_registry_auto[0].id
  schema_registry_endpoint = var.existing_schema_registry_id != null ? data.confluent_schema_registry_cluster.schema_registry_by_id[0].rest_endpoint : data.confluent_schema_registry_cluster.schema_registry_auto[0].rest_endpoint
  schema_registry = var.existing_schema_registry_id != null ? data.confluent_schema_registry_cluster.schema_registry_by_id[0] : data.confluent_schema_registry_cluster.schema_registry_auto[0]
}



# -----------------------------------------------------------------------------
# Flink Compute Pool (conditional: create new or use existing)
# -----------------------------------------------------------------------------
resource "confluent_flink_compute_pool" "flink_compute_pool" {
  count = var.existing_flink_compute_pool_id == null ? 1 : 0

  display_name = "${var.prefix}-compute-pool-${random_id.env_display_id.hex}"
  cloud        = "AWS"
  region       = var.cloud_region
  max_cfu      = var.flink_max_cfu

  environment {
    id = local.environment_id
  }

  lifecycle {
    prevent_destroy = false
  }
}

# Data source for existing Flink compute pool
data "confluent_flink_compute_pool" "flink_compute_pool" {
  count = var.existing_flink_compute_pool_id != null ? 1 : 0
  id    = var.existing_flink_compute_pool_id

  environment {
    id = local.environment_id
  }
}

# Local value to reference Flink compute pool (created or existing)
locals {
  flink_compute_pool_id = var.existing_flink_compute_pool_id != null ? var.existing_flink_compute_pool_id : confluent_flink_compute_pool.flink_compute_pool[0].id
  flink_compute_pool = var.existing_flink_compute_pool_id != null ? data.confluent_flink_compute_pool.flink_compute_pool[0] : confluent_flink_compute_pool.flink_compute_pool[0]
}

# -----------------------------------------------------------------------------
# Flink Region Data Source
# -----------------------------------------------------------------------------
# Flink API keys are scoped to a Flink region, not a compute pool
data "confluent_flink_region" "flink_region" {
  cloud  = "AWS"
  region = var.cloud_region
}

# -----------------------------------------------------------------------------
# Service Account (conditional: create new or use existing)
# -----------------------------------------------------------------------------
resource "confluent_service_account" "service_account" {
  count = var.existing_service_account_id == null ? 1 : 0

  display_name = "${var.prefix}-service-account-${random_id.env_display_id.hex}"
  description   = "Service account for {{ project_name }} Flink projects"
}


# Data source for existing service account
data "confluent_service_account" "service_account" {
  count = var.existing_service_account_id != null ? 1 : 0
  id    = var.existing_service_account_id
}

# Local value to reference service account (created or existing)
locals {
  service_account_id = var.existing_service_account_id != null ? var.existing_service_account_id : confluent_service_account.service_account[0].id
  service_account = var.existing_service_account_id != null ? data.confluent_service_account.service_account[0] : confluent_service_account.service_account[0]
}

# -----------------------------------------------------------------------------
# Service Account Permissions
# -----------------------------------------------------------------------------
# Grant EnvironmentAdmin role to service account for topic management
resource "confluent_role_binding" "sa_env_admin" {
  principal   = "User:${local.service_account_id}"
  role_name   = "EnvironmentAdmin"
  crn_pattern = local.environment_resource_name

  lifecycle {
    prevent_destroy = false
  }
}

# -----------------------------------------------------------------------------
# API Keys
# -----------------------------------------------------------------------------
# Kafka API Key for service account
resource "confluent_api_key" "sa_kafka_key" {
  display_name = "${var.prefix}-kafka-api-key-${random_id.env_display_id.hex}"
  description  = "Kafka API Key for service account to manage topics"

  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = local.kafka_cluster_id
    api_version = var.existing_kafka_cluster_id != null ? data.confluent_kafka_cluster.kafka_cluster[0].api_version : confluent_kafka_cluster.kafka_cluster[0].api_version
    kind        = var.existing_kafka_cluster_id != null ? data.confluent_kafka_cluster.kafka_cluster[0].kind : confluent_kafka_cluster.kafka_cluster[0].kind

    environment {
      id = local.environment_id
    }
  }

  depends_on = [
    confluent_role_binding.sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}

resource "confluent_api_key" "sa_schema_registry_key" {
  display_name = "${var.prefix}-schema-registry-api-key"
  description  = "Schema Registry API Key that is owned service account"
  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = local.schema_registry_id
    api_version = local.schema_registry.api_version
    kind        = local.schema_registry.kind

    environment {
      id = local.environment_id
    }
  }
  depends_on = [
    confluent_role_binding.sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}

# Flink API Key for service account
resource "confluent_api_key" "sa_flink_key" {
  display_name = "${var.prefix}-flink-api-key-${random_id.env_display_id.hex}"
  description  = "Flink API Key for service account to manage Flink statements"

  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = data.confluent_flink_region.flink_region.id
    api_version = data.confluent_flink_region.flink_region.api_version
    kind        = data.confluent_flink_region.flink_region.kind

    environment {
      id = local.environment_id
    }
  }

  depends_on = [
    confluent_role_binding.sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}

# Tableflow API Key for service account
# Required for enabling Tableflow on topics
resource "confluent_api_key" "sa_tableflow_key" {
  display_name = "${var.prefix}-tableflow-api-key-${random_id.env_display_id.hex}"
  description  = "Tableflow API Key for service account to enable Tableflow on topics"

  owner {
    id          = local.service_account.id
    api_version = local.service_account.api_version
    kind        = local.service_account.kind
  }

  managed_resource {
    id          = "tableflow"
    api_version = "tableflow/v1"
    kind        = "Tableflow"
  }

  depends_on = [
    confluent_role_binding.sa_env_admin
  ]

  lifecycle {
    prevent_destroy = false
  }
}
