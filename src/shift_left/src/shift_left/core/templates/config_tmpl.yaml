kafka:
  bootstrap.servers:  <TO_FILL>
  cluster_id: lkc-jq9kq
  pkafka_cluster: pkc-gwq2v
  api_key: <kafka-api-key>
  api_secret: <kafka-api-key_secret>
  security.protocol: SASL_SSL
  sasl.mechanism: PLAIN
  sasl.username:  <TO_FILL>
  sasl.password:  <TO_FILL>
  session.timeout.ms: 5000
  reject_topics_prefixes: ["clone","dim_","src_"]
  cluster_type: dev
  src_topic_prefix: clone
registry:
  url: <TO_FILL>
  registry_key_name:   <TO_FILL>
  registry_key_secret:  <TO_FILL>
confluent_cloud:
  base_api: api.confluent.cloud/org/v2
  environment_id:  env-<TO_FILL>
  region:  <TO_FILL>
  provider:  <TO_FILL>
  organization_id: <TO_FILL>
  api_key:  <TO_FILL>
  api_secret:  <TO_FILL>
  page_size: 100
  url_scope: private
flink:
  flink_url: <>.confluent.cloud
  api_key:  <TO_FILL>
  api_secret:  <TO_FILL>
  compute_pool_id: lfcp- <TO_FILL>
  catalog_name:  <TO_FILL>
  database_name:  <TO_FILL>
  max_cfu: 10
app:
  delta_max_time_in_min: 15  # this is to apply randomly to the event timestamp to create out of order
  report_output_dir: ./reports
  default_PK: __db
  src_table_name_suffix: ""
  src_table_name_prefix: src_
  logging: INFO
  products: ["p1", "p2", "p3"]
  cache_ttl: 120 # in seconds
  sql_content_modifier: shift_left.core.utils.table_worker.ReplaceEnvInSqlContent
  translator_to_flink_sql_agent: shift_left.core.utils.translator_to_flink_sql.DbtTranslatorToFlinkSqlAgent
  dml_naming_convention_modifier: shift_left.core.utils.naming_convention.DmlNameModifier
  compute_pool_naming_convention_modifier: shift_left.core.utils.naming_convention.ComputePoolNameModifier
  data_limit_column_name_to_select_from: tenant_id
 