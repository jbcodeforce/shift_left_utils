# Pipeline Management

???- info "Version"
    Created Mars 21- 2025 

The goals of this chapter is to present the requirements and design of the pipeline management tools.

## Context

Flink statements are inherently interdependent, consuming and joining tables produced by other statements, forming a complex pipeline. Careful deployment is crucial. The following diagram illustrates this interconnectedness and outlines a pipeline management strategy.

<figure markdown="span">
![](./images/flink_pipeline.png)
 <figcaption>a pipeline of Flink statements</figcaption>
</figure>

*This graph is generated by running a report like: `shift_left pipeline report fct_order --graph`*

## Managing the pipeline

The recipe chapter has how-to descriptions on some the specific commands to use for pipeline management. The following high level concepts are the foundations for this management:

1. The git folder is the source of truth for pipeline definitions. 
1. The table inventory must be run with the simple command:

    ```
    shift_left table build-inventory $PIPELINES
    ```

    and should be run as soon as there is a new commit. The `inventory.json` is persisted in the $PIPELINES folder and committed in git. It will be extensively use by any pipeline commands.

1. Table pipeline definition json files, that include single level of information about the pipeline are built from the sink tables. One by one during development phase or by going into all facts, dimension and views folders.

    ```
    shift_left pipeline build-all-metadata $PIPELINES
    ```

1. Hierarchy view of a pipeline can be used for reporting, or by the developer to understand the complex tree, he/she is using when adding a new table:

    ```
    shift_left pipeline report fct_order --json
    ```

1. But it used to deploy a selected table and its children. Here is an example of tables that will be modified by a stateful statement deployment:

<figure markdown="span">
![](./images/flink_pipeline.drawio.png)
 <figcaption>Intermediate table deployment</figcaption>
</figure>

While a source processing, that most of the time are doing deduplication, which is stateful will impact more elements:

<figure markdown="span">
![](./images/flink_pipeline_src.drawio.png)
 <figcaption>Src table deployment</figcaption>
</figure>



## Requirements


* [x] The expected command to deploy should be simple like:

```sh
shift_left pipeline deploy [OPTIONS] TABLE_NAME INVENTORY_PATH

   --compute-pool-id     TEXT  Flink compute pool ID. If not provided, it will create a pool. [default: None]   
   --dml-only            By default the deployment will do DDL and DML, with this flag it will deploy only DML [default: no-dml-only]                
   --force               The children deletion will be done only if they are stateful. This Flag force to drop table and recreate all (ddl, dml) [default: no-force]
```

* [ ] Deploy dml - ddl: Given the table name, executes the dml and ddl to deploy a pipeline. If the compute pool id is present it will use it. If not it will get the existing pool_id from the table already deployed, if none is defined it will create a new pool and assign the pool_id. A deployment may impact children statement depending of the semantic of the current DDL and the children's one.

* [ ] Support deploying only DML, or both DDL and DML (default)
* [ ] Deploying a DDL, means dropping existing table if exists.
* [ ] Deploying a non existant sink means deploying all its parents if not already deployed, up to the sources. This will be the way to deploy a pipeline. In this case deploy first the sources, ddl and dml, except if already running as it means this table was created by another pipeline.
* [ ] Deploying an existant sink, means drop the table if the force flag is true, and deploy the DML. If forced flag is false, only deploy dml. When DML is stateful deploy DDL and DML (= forced) 

### Questions

The following may be considered:

* does it make sense to have DDL only deployment from a source to sink pipeline?

## Developer's note

The module to support the management of pipeline is `pipeline_mgr.py`
